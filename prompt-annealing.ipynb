{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:800px; margin-left:50px; margin-right:50px; mask-image:none background:none\">\n",
    "\n",
    "<div style=\"width:100%; height:auto; mask-image: linear-gradient(to bottom, transparent 0%, black 3%, black 97%, transparent 100%);\">\n",
    "\n",
    "<div style=\"float:right; margin:20px 10px 10px 10px; width:40%; padding:20px; background:#dddddd; color:#555555; font-size:80%; mask-image:none; z-index: 10; box-shadow: -5px 5px 5px rgba(0, 0, 0, .5);\"> \n",
    "\n",
    "**Welcome to this partially completed notebook!**\n",
    "\n",
    "Working on this project is a near-full-time pursuit at the moment!  I'm sharing this work-in-progress so that you can check it out.  I hope you'll find it interesting!\n",
    "\n",
    "I'll push a new snapshot of the working copy whenever something worth sharing comes up.  In the meantime: **Pardon the dust, and sinecerely: thanks for checking this out!**\n",
    "\n",
    "**Reed**\n",
    "\n",
    "Changelog:\n",
    "\n",
    "| Date/time | Updates |\n",
    "|---|---|\n",
    "| 2024-06-10   | Initial snapshot push |\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"z-index:-1; background:white; padding-top:20px; padding-bottom: 20px; height:400px; width:100%;  \">\n",
    "<div style=\"float:left; backround: white; padding:20px 0px 20px 20px; width:400px; color:#222222;\">\n",
    "<img style=\"height:auto; margin:20px 20px 0px 0px; background-size: cover; \" src='img/sparkmark3.png'> \n",
    "<h1 style=\"margin-bottom:5px; padding-bottom:0px;\"><strong>The Prompt Annealer</strong></h1>\n",
    "<p style=\"margin:0px; padding:0px 0px 0px 20px;\"><b>Reed Taylor, 2024</b></p>\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:800px; margin-left:50px; margin-right:50px \">\n",
    "\n",
    "# Context\n",
    "\n",
    "Anthropic's User Guide provides a graphic (screenshot below; original [here](https://docs.anthropic.com/en/docs/prompt-engineering)) that depicts the inherently iterative process of prompt engineering:\n",
    "<div style=\"width:50%; height:220px; margin-left: 25%; background-image: url('img/anthropic_guide.png'); background-size: cover; mask-image: linear-gradient(to bottom, transparent 0%, black 30%, black 70%, transparent 100%);\"></div> \n",
    "\n",
    "That approach makes sense both when working with massive & robust API-gated models, and also when working with the smaller models often suited to use near \"the Edge\".  \n",
    "\n",
    "**Smaller locally-deployed models can be vexingly sensitive to prompt details**<br/>\n",
    "Big, small, even spurious changes to a prompt can create or reduce struggles with e.g. tool use, output formatting, and agent-related tasks. [[1]](https://arxiv.org/abs/2305.16504)  [[2]](https://arxiv.org/abs/2310.11324)  This makes iterative engineering even more important -- and demanding -- in an edge-focused use-case.\n",
    "\n",
    "**Locally-deployable models are advancing at a blistering pace**<br/> \n",
    "A new front=runner emerges and begs for evaluation almost every week, it seems.  In the grand scheme, this is great news for the edge-focused prompt engineer.  But it takes significant effort to stay near the leading-edge, and that can become tiring and hard to rationalize when well-engineered prompts become obsolete within days of deployment.  \n",
    "\n",
    "Worse, the specializations found to be effective on yesterday's best model might be unnecessary or even counterproductive on tomorrow's.  Expanding upon Anthropic's graphic above, I sketched that additional part of the prompt lifecycle:\n",
    "<img style=\"width:80%; margin:15px 10% 15px 10%\" src=\"img/model_drop.png\">\n",
    "\n",
    "\n",
    "With the challenges above in mind:<br/>\n",
    "**The Annealer aims to minimize recurring, manual engineering effort with short-lived value.**\n",
    " \n",
    " **It automates the empirical, exploratory process of system prompts engineering, producing customized prompts that lean into the strengths of a model-du-jour, whatever they may be.**\n",
    "\n",
    "Here's how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:800px; margin-left:50px; margin-right:50px \">\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## Annealing: *Getting Cooler All The Time*\n",
    "\n",
    "(Caution: I know more about puns than I do about materials science.)\n",
    "\n",
    "**\"Annealing\" is the name of a useful materials process.** This generally involves:\n",
    "1. Starting with something really, really hot\n",
    "2. intentionally cooling that thing down really, really slowly\n",
    "\n",
    "Annealing can change the physical properties of some metals, glass, plastics, and other things.  Annealed materials tend to be stronger & more durable than they would otherwise be.  \n",
    "\n",
    "When heated beyond certain temperatures, these materials soften their internal structure, becoming amorphous, and not organized into well-structured crystals.   If cooled quickly, those internal particles will settle into a collectively \"disorganized\" state -- irregularities and internal stresses become \"frozen\" and embedded into the material itself.\n",
    "\n",
    "<div style=\"float:right; width:30%; margin:15px 0px 5px 15px; padding-bottom:5px; text-align:center; background-color: rgba(255, 255, 255, 0.05); \"><img src=\"img/drop.gif\"><br/>Not Annealed</div>Often, this means the materials will be harder or stiffer.  In some cases, that can be useful; in other cases, it may make them harder to work with -- notably, with glass: Rapid cooling can produce parts that spontaneously break, or even explode under stress!  (SmarterEveryDay's Destin leveraged this to make some awesome [slo-mo videos](https://www.youtube.com/watch?v=xe-f4gokRBs); typically though, glassblowers will anneal their pieces specifically to avoid such outcomes.)\n",
    "\n",
    "**In ambiguously aphysical terms, here's how slowly cooling helps achieve a more \"optimal\" outcome:**<br/> \n",
    "If we think of the particles in a material comprising a \"system\", their collective arrangement could define the \"state\" of that system.  High temperatures allow these particles freedom to move around -- and in doing so, they transition the system from its initial state through a long series of \"neigboring\" states.   \n",
    "\n",
    "The process of controlled cooling affords the particles a means to to find \"good seats‚Äù for themselves in the end-state --  that is, to find places to hang out more comfortably in the end-state, once things have fully cooled.   Collectively better-otrganizes, this means the system has \"discovered\" a lower-energy overall state to wind up in!  When annealing \"works\", this end-state will be meaningfully lower-energy (i.e. better-optimized) than any state that could have been reached by racing to a near-neighbor of the start-state -- as would happen with rapid cooling.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"max-width:800px; margin-left:50px; margin-right:50px\">\n",
    "\n",
    "### Getting into Simulated Annealing (before it's cool)\n",
    "\n",
    " Simulated annealing is an optimimzation process loosely modeled on the physical process.  In general:\n",
    "\n",
    "\n",
    "- <div style=\"float:right; width:40%; margin:15px 0px 5px 15px; padding-bottom:5px; text-align:center; background-color: rgba(255, 255, 255, 0.05); \"><img src=\"img/salesman_annealing.gif\">Cool traveling salesman demo<br/><a href=\"https://toddwschneider.com/posts/traveling-salesman-with-simulated-annealing-r-and-shiny/#salesman-app)\">from Todd W. Schneider</a></div>Simulated annealing defines a problem in terms of a system with a known, scoreable state that can move to neighboring states. Those neighbors may score better or worse than their predecessors.\n",
    "- Optimization is iterative; each iteration has a \"temperature\", and each iteration proposes moving to a randomly-identified neighbor of the current state.   \n",
    "- Annealing begins at a high temperature, allowing the system to move wherever it wants.  In practical terms this means that -- at least initially -- we'll accept pretty every neighbor-move regardless of whether it's better, or worse -- even if it's a LOT worse. \n",
    "- Annealing gradually cools the system down as iterations progress.  Cooling reduces the system's appetite to accept unfavorable moves.  Correspondingly, the system progressively settles toward more optimal states, exploiting randomly discovered improvements without just moving... wherever.\n",
    "- We end up cooling into a solution that‚Äôs -- hopefully -- close to a global optimum.\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:800px; margin-left:50px; margin-right:50px \">\n",
    "\n",
    "# üõ†Ô∏è Application \n",
    "\n",
    "### Why Apply Simulated Annealing to Prompt Engineering?\n",
    "\n",
    "I will admit that I have a certain nostalgic affection for simulated annealing, which made me curious to revisit the approach to see how well it could be adapted to this situation, and (of course) to see how well it might perform!\n",
    "\n",
    "With that said, simulated annealing (and metaheuristics in general) do have some appealing characteristics for problems where: \n",
    "- Exhaustive search is computationally intractable -- e.g. the travelling salesman example above\n",
    "- The cost landscape has many local optima, and local / greedy search is unlikely to do well\n",
    "- The solution space is too high-dimensional for approaches (e.g. Bayesian optimization) that scale poorly as dimensionality rises/\n",
    "\n",
    "I did a non-rigorous search and found a number of prompt-tuning and optimizaiton implementations on Github and in some published papers, but these seemed mostly to have been either: \n",
    "- Examples of \"Prompt optimization\", i.e focused on exploiting / searching a small local neighborhood.  (In the general case approaches such as these seemed very likely to get stuck in cost basins/)\n",
    "- Bayesian and similar optimizations necessarily tailored to exploring the prompt-space from the perspective of reduced dimemsionality.  \n",
    "\n",
    "Inspired in part by the impressive output of [Anthropic's Metapropmpt](https://docs.anthropic.com/en/docs/helper-metaprompt-experimental), I was interested to leverage LLMs in a more open-ended approximation of how a human might approach the task of prompt engineering, knowing a priori that the best-performing prompts could be ... well, anything.  So I liked the idea of tackling the problem without any formal limits.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:800px; margin-left:50px; margin-right:50px \">\n",
    "\n",
    "\n",
    "## Approach - Overview\n",
    "\n",
    "<div style=\"width:80%; margin-left:10%\" ><img src=\"img/anealer_text_blocks.png\"></div>\n",
    "\n",
    "_TODO: writeup_\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:800px; margin-left:50px; margin-right:50px \">\n",
    "\n",
    "### Initial Implementation\n",
    "\n",
    "<div style=\"width:80%; margin-left:10%\"><img src=\"img/anealer_full_blocks.png\"></div>\n",
    "\n",
    "_TODO: writeup of approach_\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:800px; margin-left:50px; margin-right:50px \">\n",
    "\n",
    "## Challenges\n",
    "\n",
    "### Evaluation:  Task-specific guidance for Task-agnostic criteria \n",
    "\n",
    "It is/was a goal to have the evaluation framework _NOT_ require specialization to fit a user's objective.  This meant developing (and analyzing / refining) a set of general purpose evaluation criteria and rubrics, aiming to adhere to some collective principles:\n",
    "\n",
    "- **Comprehensiveness:** Do the criteria jointly capture and discourage all the expected and/or identifiable quality loss mechanisms that persist through optimization?\n",
    "- **Efficiency:** The criteria are evaluated many, many times.  Are they achieving a successful balance between their costs (mainly in terms of latency) and their effectiveness (in terms of result quality and other factors)?\n",
    "- **Safety:** Given the scenario / use-case, are the criteria able to minimize potential harm / abuse issues?  \n",
    "\n",
    "With those principles in mind, there are some related measurable traits that can be used to assess and improve criteria, rubrics, and base-prompts:\n",
    "- **Reliability,** i.e. tuning prompts, criteria, and rubrics to reduce within-criterion statistical variance across repeat trials. Inter-rater reliability scoring approaches may be relevant.  Specificity, validity, and objectivity might be proxied here.  This is also clearly important when tuning token-efficiency.\n",
    "- **Orthogonality,** i.e. tuning individually and collectively to reduce pairwise correlations among criteria, which could proxy for the ‚Äúspan‚Äù of information they yield, assuming they are consistently measuring real things.\n",
    "- **Dynamic range,** i.e. tuning (especially the rubrics) so that the judges generate score distributions that are favorable for sensitivity and discrimination\n",
    "\n",
    "##### The Problem: \n",
    "Initial testing demonstrated that the evaluators were not able to function consistently within a single run: \n",
    "\n",
    "<div style=\"width:80%; margin-left:10%\"><img src=\"img/eval_before.png\"></div>\n",
    "\n",
    "Reading the evaluators' output reasoning, it was evident even without quantitative analysis that the they were confronted with a task that was too open-ended.  Each individual [example,criterion] eval required (re)interpretation of the task-agnostic rubrics, in the context of the specific current objective.  In many cases this left a great deal of room for interpretation, and absent a more consistent context this led to wide variance in the raters' within-task behavior.\n",
    "\n",
    "For example, given a user-provided objective like _\"Use provided tools to perform math operations\"_: There is more than one reasonable way to interpret \"Relevance\" even given a firm rubric, and that interpretation was very often swayed by tokens in the outputs and reference examples themselves.  Result being: poor within-task inter-rater reliability, weaker cost function, leaky quality issues etc.\n",
    "\n",
    "**Solution:**  Stabilize within-task rater performance without sacrificing the flexibility of task-agnostic criteria by generate concrete, task-specific guidance for each criterion / rubric and providing this to raters as part of their evaluation prompt.\n",
    "\n",
    "<div style=\"width:80%; margin-left:10%\"><img src=\"img/eval_after.png\"></div>\n",
    "\n",
    "*TODO: make this section less rough*\n",
    "\n",
    "*Also -- for the record -- I have a great deal of quantitative analysis to do before I'd want to put a stake in the ground on many of the observations below.  If anyone reads this, they should take what's here as approximating what I hope to write, given what I've observed, but as yet without much evidence.*\n",
    "\n",
    "Most importantly, the granular [objective, criterion] guidance makes the individual eval tasks less subjective.  This helps address the observed problems with self-consistency (i.e. inter-rater reliability, within tasks).\n",
    "\n",
    "The prescriptive guidance is also beneficial to eval rater efficiency.  Because the guidance is task-aware, the span of e.g. few-shot examples that need to be covered in the task-agnostic base prompt drops dramatically, since we don't need to inform the raters about a broad, hypothetical landscape.  This concreteness should translate to fewer tokens-per-eval (without quality-loss), which is significant as the latency of ratings is critical to the end-to-end runtime for each iteration.  \n",
    "\n",
    "And - the range of acceptable interpretation of criteria widens with the guidance flow in place.  For example, I would have written eval prompts that discouraged standalone eval raters from interpreting e.g. \"harmlessness\" to mean something like \"making well-formed function calls when using tools\" because -- in the absence of any context -- that interpretation seems to stray fairly far from the original meaning.   But, if a holistic guidance planner KNOWS that e.g. due to the nature of the underlying task (e.g. \"use tools to solve math equations; responses will be parsed into json objects that accept only a float\"), and e.g. the criteria \"accuracy\" and \"correctness\" already having taken care of the other ways a numerical answer might be considered \"harmful\" -- then, sure -- cover some other base with that criterion!   This kind of flexibility may allow for more overall outcome-value per eval-inference-token.\n",
    "\n",
    "**Notes on the approach**\n",
    "\n",
    "The separation of the guidance-generation into multiple steps had two motivations.  First, and more straightforward was performance and cost: the total context window required for both the prompt (including all the criteria), the objective, and few-shot examples of guidance - plus the generation of N individual guidance items, was quite large.  Even large and capable models like Claude-3-opus had a tendency to stop short of generating all N required guidance items, which in turn meant re-running the prompt, at what could amount to considerable expense both in latency (albeit one-time startup latency), and in paid token-count, and also in saturating rate-limiting  quotas.\n",
    "\n",
    "Second was complexity.  The task of sizing up the collective ensemble of criteria, alongside the objective, and returning a plan for how they might all be jointly aligned -- this is not \"simple\" by any means, but it also is in essence a summarization-type task, and it seems to fall well within the capabilities of Claude-3-opus to generate what subjectively seem to be sane and often thoughtful such plans.\n",
    "\n",
    "Likewise - with a single holistic plan already pulled together, it becomes much easier to break apart the task of generating N guidance items into N separate tasks, each generated independently by a guidance-generating prompt that reads the same (entire, holistic) plan.  The effort and complexity in the presence of a unifying pre-authored plan of guidance-authoring becomes much easier to think of as a parallelizable job.\n",
    "\n",
    "\n",
    "#### Missing Pieces\n",
    "\n",
    "It's clear from qualitatively observing a few test runs that there's more work to be done on the evaluation pipeline.  Here are a few top-of-mind things that I'm looking forward to implementing, but that haven't been done yet:\n",
    "\n",
    "**Real (target) examples for Guidance generators**  Guidance authors would clearly benefit from exposure to a few task-specific, real-world examples of what the target model's actual inputs look like.  I hesitate to call these \"few-shot examples because they aren't actually examples generated guidance for evaluators OR of evaluation input/output either -- the target model inputs & reference outputs are actually two steps removed from the actual task of the guidance author. Nonetheless: I saw a case where: \n",
    "- the objective was written as \"use available tools to exactly repeat the input text\".\n",
    "- The guidance generator for \"accuracy\" authored guidance suggesting a task specific rubric where \"mistakes of only a few words should still get a high score.\"  \n",
    "- But, the actual underlying task was a tool-usage benchmark, with the target model required to use a laborious \"typewriter\" tool requiring one tool-call for every letter repeated.  So, most of the benchmark dataset was in fact single-word examples, with between 5 and 15 characters each.\n",
    "- So, the well-intentioned guidance for the \"accuracy\" evaluator to let \"mistakes of a few words\" slide, was totally confounding.  \n",
    "\n",
    "This could be addressed by giving the guidance generator visibility into a few examples drawn from the annealing dataset, but this introduces complexity if we want to remain principled about how that data enters the pipeline.  That example could certainly leak directly into guidance, and into the prompts, and then into the target model's performance.  None of this couldn't be handled, it just deserves some care.\n",
    "\n",
    "**Real (target) examples for few-shot in neighbor generation**  Somewhat similar to the above, it would be helpful to enable the Neighbor Generation pipeline to add examples that aren't entirely hallucinated.   Also similar to the above, this would certainly require some specific means of handling the available few-shot examples separately from the eval / annealing data.\n",
    "\n",
    "**Equip Guidance Planning to steer the cost function** Guidance planning clearly knows that certain criteria matter more / differently than others.  We could give it some simple, bounded choices (e.g. linear/nonlinear and low/medium/high weight) to help influence how the scores its plan producesa are to be combined.  Likewise the door is open to zero-weight cases where e.g. it deems a criterion wholly unnecessary.  While this merits careful thought, the performance bottleneck at evaluation probably merits the work.\n",
    "\n",
    "[todo: import more from here and there in the code & in various notes]\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:800px; margin-left:50px; margin-right:50px \">\n",
    "\n",
    "\n",
    "#### Other problems worked through\n",
    "_TODO: write these up in a reasonable way_\n",
    "\n",
    "- Framework getting in the way -- langchain wasn't clearly a net-win for ease of use or performance.\n",
    "- Surprising lack of clarity on when/where fine-grained prompt specifics e.g. special instruction-tuned tokens are under my control (slash responsibility), vs being \"magically\" handled by one or more embedded layers of the frameworks & APIs, which can be black-box like.  Lesson: don't trust anyone!  Many things that DO matter are NOT being done correctly.\n",
    "- Gracefully handling transient vs real-but-ignorable vs systemic failures\n",
    "- Consistent & capable development environments\n",
    "- Structured output from models of different sizes / etc.\n",
    "- Means to enable tool usage another API specifics without requiring a great deal of specialized code\n",
    "- Tie more directly into pre-existing benchmark / dataset ecosystem for both annealing and \"outside-the-loop\" evaluation / confirmation of result quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More future work - not related to guidance/eval\n",
    "- Tune cost function (it's just in some guesswork state pending real stats to improve it)\n",
    "- Restore tool use capability to the existing code (removed during a refactor)\n",
    "- Refactor python out of the notebook into modules; separate discussion and analysis into less cumbersome notebooks\n",
    "- ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implentation Details  -- The code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  üñ•Ô∏è Setup, Initialization, Interfaces, and other Utilities \n",
    "\n",
    "Important, useful, but not very interesting. :yawning_face:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### üíª **Code:** Dependencies, Imports, and Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs (uncomment)\n",
    "\n",
    "# %pip install ipywidgets tqdm matplotlib numpy pandas seaborn scipy torch torchvision tiktoken einops vllm langchain-anthropic langchain-openai langchain langchainhub langchain-experimental sqlitedict\n",
    "\n",
    "# for notebook exports etc.\n",
    "# %pip install notebook jupyter_contrib_nbextensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNEALING_SESSION_NAME = \"Annealer-TEST04\"  # Using a stable session name for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANNEALING_SESSION_NAME: If not set, generate some timestamped strings\n",
    "import datetime\n",
    "\n",
    "if \"ANNEALING_SESSION_NAME\" not in globals():\n",
    "    ANNEALING_SESSION_START_TIME: str = datetime.datetime.now().strftime(\n",
    "        \"%Y%m%d-%H%m%S\"\n",
    "    )\n",
    "    ANNEALING_SESSION_NAME: str = f\"Annealer-{ANNEALING_SESSION_START_TIME}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All-purpose Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from functools import partial\n",
    "import re\n",
    "import json\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Literal, Optional, Set\n",
    "\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, ValidationError\n",
    "from scipy.stats import logistic\n",
    "\n",
    "from sqlitedict import SqliteDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### üíª **Code:** LangChain & LangSmith Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith project already exists, using it\n"
     ]
    }
   ],
   "source": [
    "### LangChain / LangSmith Imports & Initialization\n",
    "from langchain import hub as lc_hub\n",
    "from langchain_core.language_models import (\n",
    "    BaseLanguageModel as lc_BaseLanguageModel,\n",
    "    BaseChatModel as lc_BaseChatModel,\n",
    ")\n",
    "from langchain_core.runnables import (\n",
    "    Runnable as lc_Runnable,\n",
    "    RunnablePassthrough as lc_RunnablePassthrough,\n",
    "    RunnableParallel as lc_RunnableParallel,\n",
    ")\n",
    "from langsmith import Client as ls_Client\n",
    "from langsmith import traceable\n",
    "from langsmith.schemas import (\n",
    "    Example as ls_Example,\n",
    ")\n",
    "from langsmith.schemas import (\n",
    "    Run as ls_Run,\n",
    ")\n",
    "from langsmith.schemas import (\n",
    "    TracerSession as ls_TracerSession,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"key\"\n",
    "# os.environ[\"LANGCHAIN_HUB_API_URL\"] = \"https://api.hub.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = ANNEALING_SESSION_NAME\n",
    "\n",
    "LS_CLIENT: ls_Client = ls_Client()\n",
    "try:\n",
    "    LS_PROJECT = LS_CLIENT.create_project(\n",
    "        project_name=ANNEALING_SESSION_NAME,\n",
    "        description=\"Prompt Annealing session\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(\"LangSmith project already exists, using it\")\n",
    "        LS_PROJECT = LS_CLIENT.read_project(project_name=ANNEALING_SESSION_NAME)\n",
    "    else:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LangChain Prompt Imports\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    BasePromptTemplate as lc_PromptTemplate,\n",
    ")\n",
    "from langchain_core.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain_core.prompts.chat import MessageLikeRepresentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### üíª **Code:** Models & Inference APIs Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Anthropic API Imports & Initialization\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = (\"key\")\n",
    "anthropic_claude_3_opus_model = ChatAnthropic(model_name=\"claude-3-opus-20240229\")  # type: ignore\n",
    "anthropic_claude_3_sonnet_model = ChatAnthropic(model_name=\"claude-3-sonnet-20240229\")  # type: ignore\n",
    "anthropic_claude_3_haiku_model = ChatAnthropic(model_name=\"claude-3-haiku-20240307\")  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Ollama API Imports & Initialization\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_experimental.llms.ollama_functions import (\n",
    "    OllamaFunctions,\n",
    ")\n",
    "\n",
    "\n",
    "def get_local_ollama_model(\n",
    "    type: Literal[\"chat\", \"functions\"] = \"chat\",\n",
    "    model: str = \"llama3:latest\",\n",
    "    format: Optional[Literal[\"json\"]] = None,\n",
    "    base_url: str = \"http://localhost:11434\",\n",
    "    **kwargs,\n",
    ") -> ChatOllama:\n",
    "    if type == \"chat\":\n",
    "        return ChatOllama(model=model, format=format, base_url=base_url, **kwargs)\n",
    "    elif type == \"functions\":\n",
    "        return OllamaFunctions(model=model, format=\"json\", base_url=base_url, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown Ollama model type: {type}\")\n",
    "\n",
    "\n",
    "def get_helicopter_ollama_model(\n",
    "    type: Literal[\"chat\", \"functions\"] = \"chat\",\n",
    "    format: Optional[Literal[\"json\"]] = None,\n",
    "    **kwargs,\n",
    ") -> ChatOllama:\n",
    "    return get_local_ollama_model(\n",
    "        type=type, base_url=\"http://10.0.1.21:11434\", format=format, **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def get_desktop_ollama_model(\n",
    "    type: Literal[\"chat\", \"functions\"] = \"chat\", **kwargs\n",
    ") -> ChatOllama:\n",
    "    return get_local_ollama_model(\n",
    "        type=type, base_url=\"http://10.0.2.100:11434\", **kwargs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VLLM\n",
    "from langchain_community.llms.vllm import VLLMOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser as lc_PydanticOutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser as lc_XMLOutputParser\n",
    "from langchain_community.llms.vllm import VLLM as lc_VLLM\n",
    "\n",
    "os.environ[\"VLLM_NCCL_SO_PATH\"] = (\n",
    "    \"/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_remote_vllm_completion_model(\n",
    "    model: str = \"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
    "    api_base: str = \"http://10.0.1.21:18000/v1\",\n",
    "    model_kwargs: Dict[str, Any] = {},\n",
    "    **kwargs,\n",
    ") -> VLLMOpenAI:\n",
    "    completion_llm = VLLMOpenAI(\n",
    "        model=model,\n",
    "        openai_api_key=\"vllm\",  # type: ignore\n",
    "        openai_api_base=api_base,  # type: ignore\n",
    "        model_kwargs=model_kwargs,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return completion_llm\n",
    "\n",
    "\n",
    "def with_structured_output(\n",
    "    self: VLLMOpenAI, schema: Dict | type[BaseModel], **kwargs: Any\n",
    ") -> lc_Runnable:  # [LanguageModelInput, Dict[Unknown, Unknown] | BaseModel]\n",
    "    \"\"\"Monkey patch to allow for structured output from VLLM at least the way I'm doing it\"\"\"\n",
    "    if isinstance(schema, dict):\n",
    "        raise NotImplementedError(\"Not implemented yet\")\n",
    "    _new_model: VLLMOpenAI = self.copy()\n",
    "    _new_model.callbacks = self.callbacks\n",
    "    _new_model.tags = self.tags\n",
    "    _new_model.metadata = self.metadata\n",
    "    _new_model.client = self.client\n",
    "\n",
    "    _json_schema_str = json.dumps(schema.schema())\n",
    "    if not _new_model.model_kwargs.get(\"extra_body\"):\n",
    "        _new_model.model_kwargs[\"extra_body\"] = {}\n",
    "    _new_model.model_kwargs[\"extra_body\"][\"guided_json\"] = _json_schema_str\n",
    "\n",
    "    _parser = lc_PydanticOutputParser(pydantic_object=schema)\n",
    "    _chain = _new_model | _parser\n",
    "    return _chain\n",
    "\n",
    "\n",
    "# Monkey patch VLLM to allow for structured output\n",
    "VLLMOpenAI.with_structured_output = with_structured_output\n",
    "\n",
    "\n",
    "def get_remote_vllm_chat_model(\n",
    "    model: str = \"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
    "    api_base: str = \"http://10.0.1.21:18000/v1\",\n",
    "    model_kwargs: Dict[str, Any] = {},\n",
    "    **kwargs,\n",
    ") -> ChatOpenAI:\n",
    "    chat_llm = ChatOpenAI(\n",
    "        model=model,\n",
    "        api_key=\"vllm\",  # type: ignore\n",
    "        base_url=api_base,\n",
    "        model_kwargs=model_kwargs,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return chat_llm\n",
    "\n",
    "\n",
    "from langchain_experimental.llms import LMFormatEnforcer\n",
    "\n",
    "\n",
    "class VLLMLocal(lc_VLLM):\n",
    "    def with_structured_output(\n",
    "        self, schema: Dict | type[BaseModel], **kwargs: Any\n",
    "    ) -> lc_Runnable:\n",
    "        raise NotImplementedError(\"Not implemented yet\")\n",
    "\n",
    "\n",
    "def get_local_vllm_completion_model(\n",
    "    model: str = \"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
    "    model_kwargs: Dict[str, Any] = {},\n",
    "    client: Any = None,\n",
    "    **kwargs,\n",
    ") -> VLLMLocal:\n",
    "    local_llm = VLLMLocal(\n",
    "        model=model,\n",
    "        trust_remote_code=True,\n",
    "        vllm_kwargs=model_kwargs,\n",
    "        client=client,\n",
    "    )\n",
    "    return local_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### üíª **Code:** Utility / Interface Classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainDataset(BaseModel):  # Set up & manage state for LangChain Datasets.\n",
    "    name: str\n",
    "    input_var_set: Set[str] = set()\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "        print(f\"LangChainDataset initializing with name: {self.name}\")\n",
    "        try:\n",
    "            if not LS_CLIENT.has_dataset(dataset_name=self.name):\n",
    "                LS_CLIENT.read_dataset(dataset_name=self.name)\n",
    "        except Exception as e:\n",
    "            raise LookupError(f\"Could not load dataset {self.name}, error: {e}\")\n",
    "\n",
    "        _dataset_input_vars: List[str] = list(self.examples[0].inputs.keys())\n",
    "        self.input_var_set.update(_dataset_input_vars)\n",
    "\n",
    "    @property\n",
    "    def examples(\n",
    "        self,\n",
    "        # dataset_name: Optional[str | None] = None,\n",
    "    ) -> List[ls_Example]:\n",
    "        \"\"\"Surfaces a list of all examples in a the dataset\n",
    "\n",
    "        Returns:\n",
    "            List[ls_Example]\n",
    "        \"\"\"\n",
    "        return list(LS_CLIENT.list_examples(dataset_name=self.name))\n",
    "\n",
    "    @property\n",
    "    def example_inputs(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Returns all inputs from the dataset in a single list\"\"\"\n",
    "        return [example.inputs for example in self.examples]\n",
    "\n",
    "    @property\n",
    "    def example_reference_ouputs(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Returns all inputs from the dataset in a single list\"\"\"\n",
    "        return [example.outputs or {} for example in self.examples]\n",
    "\n",
    "    def get_random_example(self) -> ls_Example:\n",
    "        \"\"\"Returns a single randomly selected example from the active dataset\n",
    "\n",
    "        Returns:\n",
    "            ls_Example\n",
    "        \"\"\"\n",
    "        return random.choice(self.examples)\n",
    "\n",
    "    @classmethod\n",
    "    def from_source_dataset_name(\n",
    "        cls,\n",
    "        source_dataset_name: str,\n",
    "        client_dataset_name: str = \"\",\n",
    "        client_dataset_limit: Optional[int] = None,\n",
    "    ) -> Any:\n",
    "        \"\"\"Copies an existing (source) dataset to a new dataset in the client.\n",
    "        The new dataset is the one that will be used for each iteration evaluation.\n",
    "        Copying to a new dataset in this manner fits the annealing approach into\n",
    "        the way LangSmith likes to organize its evaluate() results.\n",
    "\n",
    "        Args:\n",
    "            source_dataset_name (str): existing langsmith dataset to copy.\n",
    "\n",
    "            client_dataset_name (str, optional): \"Unique\" name for the dataset\n",
    "                created for annealing.  Defaults to a combination of source_dataset_name\n",
    "                and the ANNEALING_SESSION_NAME.  If that client_dataset_name already\n",
    "                exists in the client, we catch the exception and (re)use it. (Typically\n",
    "                happens when e.g. restarting an aborted session.)\n",
    "\n",
    "                limit (Optional[int], optional): Limits the number of examples copied\n",
    "                from source_dataset into client_dataset.  Precisely which examples\n",
    "                get copied is LangSmith-defined (and unknown to me) -- so as implmeneted\n",
    "                this is intended for \"quick-test\" subsetting -- not for \"principled\"\n",
    "                / consistent subsetting (e.g. for isolating training / eval sets).\n",
    "                Defaults to None, i.e. \"All examples\"\n",
    "\n",
    "            client_dataset_limit (Optional[int], optional): If set, limits the number of examples\n",
    "                copied from the source dataset.  Intended for quick debugging / testing (not\n",
    "                for e.g. training / eval separation).  Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Any: some kind of LangSmith dataset object that isn't very well documented,\n",
    "        \"\"\"\n",
    "        if not client_dataset_name:\n",
    "            client_dataset_name = source_dataset_name + f\"-{ANNEALING_SESSION_NAME}\"\n",
    "\n",
    "        _examples = list(\n",
    "            LS_CLIENT.list_examples(\n",
    "                dataset_name=source_dataset_name, limit=client_dataset_limit\n",
    "            )\n",
    "        )\n",
    "        _inputs = [_example.inputs for _example in _examples]\n",
    "        _outputs = [_example.outputs for _example in _examples]\n",
    "        try:\n",
    "            LS_CLIENT.create_dataset(dataset_name=client_dataset_name)\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e):\n",
    "                print(\n",
    "                    f\"Client dataset {client_dataset_name} already exists.  Reusing without changes!\"\n",
    "                )\n",
    "                # TODO: tag the dataset with a version\n",
    "            else:\n",
    "                raise e\n",
    "        else:\n",
    "            LS_CLIENT.create_examples(\n",
    "                inputs=_inputs,\n",
    "                outputs=_outputs,\n",
    "                dataset_name=client_dataset_name,\n",
    "            )\n",
    "\n",
    "        _lcd = LangChainDataset(\n",
    "            name=client_dataset_name,\n",
    "        )\n",
    "        return _lcd\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset_name(\n",
    "        cls,\n",
    "        dataset_name: str,\n",
    "    ) -> Any:\n",
    "        _lcd = LangChainDataset(\n",
    "            name=dataset_name,\n",
    "        )\n",
    "        return _lcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainConfig(  # 'Root' class to setup & manage the state of LangChain objects\n",
    "    BaseModel\n",
    "):\n",
    "    dataset: LangChainDataset\n",
    "\n",
    "    @property\n",
    "    def project(self) -> ls_TracerSession:\n",
    "        return LS_PROJECT\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset_object(cls, dataset: LangChainDataset, **kwargs) -> Any:\n",
    "        _lc = cls(\n",
    "            dataset=dataset,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return _lc\n",
    "\n",
    "    @classmethod\n",
    "    def from_source_dataset_name(\n",
    "        cls,\n",
    "        source_dataset_name: str = \"annealing_target_dataset\",\n",
    "        client_dataset_name: str = \"\",\n",
    "        client_dataset_limit: int | None = None,\n",
    "        **kwargs,\n",
    "    ) -> Any:\n",
    "        _lcd = LangChainDataset.from_source_dataset_name(\n",
    "            source_dataset_name=source_dataset_name,\n",
    "            client_dataset_name=client_dataset_name,\n",
    "            client_dataset_limit=client_dataset_limit,\n",
    "        )\n",
    "\n",
    "        _lc = cls(\n",
    "            dataset=_lcd,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return _lc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Configuration \n",
    "\n",
    "Annealing proceeds with the following user configuration in place:\n",
    "- Annealed prompt objective\n",
    "- Dataset to drive the optimization process\n",
    "- Annealed prompt input config:\n",
    "    - Where in the prompt they should be located (system message, first user message, etc.)\n",
    "    - Dataset input vars are considered automatically, but config can specify any others e.g. `agent_scratchpad`, `chat_history` etc. \n",
    "- Models, including:\n",
    "    - Annealing Target Model selection & config\n",
    "    - Models selections and configurations for all the internal LLM-driven stages of annealing\n",
    "- Maximum number of iterations to run\n",
    "\n",
    "The above is captured in a set of **Configuration classes**.  Usage and code in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üîé **Details:** Configuration classes \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage:** Here's an example of how the configuration classes might get populated:\n",
    "\n",
    ">```python\n",
    ">## Prepare various configuration subcomponents, e.g. objective, model selections\n",
    ">_objective: str = \"Give brief factual responses to questions.  Respond with 'response' and 'reasoning' in a JSON object.\"\n",
    ">\n",
    "># Define an output schema (not required, but in this case the objective calls for it)\n",
    "># using a Pydantic class\n",
    ">class MyResponse(BaseModel):\n",
    ">    \"\"\" Represents the response to a user's question, and the reasoning behind it\"\"\"\n",
    ">    response: str,\n",
    ">    reasoning: str\n",
    ">\n",
    "># Select & prep the target model, so it's ready to generate output with that schema\n",
    ">_target_model = get_local_ollama_model(type=\"functions\")\n",
    ">_target_model_with_structure = _target_model.with_structured_output(schema=MyResponse)\n",
    ">_models = AnnealerModels(\n",
    ">    target=_target_model_with_structure\n",
    ">    # [... other models can be selected here...]\n",
    ">) \n",
    ">\n",
    ">## Generate the config object using the above config items:\n",
    ">ac: AnnealerConfig = from_objective_and_source_dataset_name(\n",
    ">    objective=_objective,\n",
    ">    models=_models\n",
    ">    source_dataset_name=\"my_question_dataset\",\n",
    ">)\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### üíª **Code:** Config classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnnealerModels\n",
    "# This namedtuple represents the user's selection and fine-grained configuration of the models, tokenizers, and parsers to be\n",
    "# used at each independent stage of the annealing process. (Including selecting the target!)\n",
    "\n",
    "# The choices are all instantiated as references within this object, when it's initialized.\n",
    "\n",
    "\n",
    "AnnealerModels = (\n",
    "    namedtuple(  # Pick & configure all the annealing models (and target model!)\n",
    "        \"AnnealerModels\",\n",
    "        [\n",
    "            \"bootstrap\",\n",
    "            \"guidance_planning\",\n",
    "            \"guidance_generation\",\n",
    "            \"output_evaluator\",\n",
    "            \"neighbor_generation\",\n",
    "            \"target\",\n",
    "        ],\n",
    "        defaults=[\n",
    "            None,  # anthropic_claude_3_opus_model, # prompt bootstrap\n",
    "            anthropic_claude_3_opus_model,  # guidance planning\n",
    "            anthropic_claude_3_sonnet_model,  # guidance generation\n",
    "            anthropic_claude_3_haiku_model,  # output evaluation\n",
    "            anthropic_claude_3_haiku_model,  # neighbor generation\n",
    "            anthropic_claude_3_haiku_model,  # annealing target\n",
    "        ],\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnealerDiskCache(BaseModel):\n",
    "    file_name: str = \"annealer_disk_cache.sqlite\"\n",
    "    table_name: str = ANNEALING_SESSION_NAME\n",
    "\n",
    "    disk_cache: Optional[SqliteDict] = None\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        **data,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            **data,\n",
    "        )\n",
    "        self.disk_cache = SqliteDict(\n",
    "            filename=self.file_name,\n",
    "            tablename=self.table_name,\n",
    "            autocommit=True,\n",
    "        )\n",
    "\n",
    "    def read_obj(self, obj_name: str):\n",
    "        if self.disk_cache is not None:\n",
    "            try:\n",
    "                _obj = self.disk_cache[obj_name]\n",
    "            except Exception as e:\n",
    "                print(f\"No object found in disk cache for `{obj_name}` ({e})\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"Retrieved object from disk cache for `{obj_name}`\")\n",
    "        return _obj\n",
    "\n",
    "    def write_obj(self, obj: Any, obj_name: Optional[str] = None) -> bool:\n",
    "        if self.disk_cache is not None:\n",
    "            try:\n",
    "                if obj_name is None:\n",
    "                    obj_name = obj.__class__.__name__\n",
    "                # TODO: check for overwrite -- usuallly desirable but maybe not always?\n",
    "                self.disk_cache[obj_name] = obj\n",
    "                self.disk_cache.commit()  # trying to be as conservative as possible about writebacks\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to write `{obj_name}` object to disk cache for  ({e})\")\n",
    "                return False\n",
    "            else:\n",
    "                print(f\"Wrote `{obj_name}` object to disk\")\n",
    "                return True\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnealerConfig(  # \"Root\" config class that captures all substantive user-defined configuration\n",
    "    BaseModel\n",
    "):\n",
    "    # This class is the \"root\" of how we represent the configuration state of the annealer;\n",
    "    # most of the config-dependent classes get instantiated when this object is created.\n",
    "\n",
    "    lc: Optional[LangChainConfig] = None\n",
    "    models: AnnealerModels = Field(default_factory=AnnealerModels)\n",
    "    objective: str = \"Provide helpful responses to user messages.\"\n",
    "    max_iteration_count = 10\n",
    "    disk_cache: Optional[AnnealerDiskCache] = Field(default_factory=AnnealerDiskCache)\n",
    "    manual_extra_var_set: Set[str] = set()\n",
    "    manual_user_message_var_set: Set[str] = set()\n",
    "\n",
    "    def read_obj_from_disk_cache(self, obj_name: str) -> Any | None:\n",
    "        if self.disk_cache is not None:\n",
    "            return self.disk_cache.read_obj(obj_name)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def write_obj_to_disk_cache(self, obj: Any, obj_name: Optional[str] = None) -> bool:\n",
    "        if self.disk_cache is not None:\n",
    "            return self.disk_cache.write_obj(obj, obj_name)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    @property\n",
    "    def client(self):\n",
    "        return LS_CLIENT\n",
    "\n",
    "    @property\n",
    "    def project(self):\n",
    "        return LS_PROJECT\n",
    "\n",
    "    @property\n",
    "    def dataset(self) -> LangChainDataset | None:\n",
    "        if self.lc:\n",
    "            return self.lc.dataset\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def all_input_var_set(self) -> Set[str]:\n",
    "        if self.dataset:\n",
    "            return self.dataset.input_var_set | self.manual_extra_var_set\n",
    "        else:\n",
    "            raise ValueError(\"No dataset loaded\")\n",
    "\n",
    "    @property\n",
    "    def system_message_input_var_set(self) -> Set[str]:\n",
    "        return self.all_input_var_set - self.manual_user_message_var_set\n",
    "\n",
    "    @classmethod\n",
    "    def from_objective_and_source_dataset_name(\n",
    "        cls,\n",
    "        objective: str,\n",
    "        source_dataset_name: str,\n",
    "        models=AnnealerModels(),\n",
    "        extra_input_vars: List[str] = [],\n",
    "        user_message_input_vars: List[str] = [],\n",
    "        client_dataset_name: str = \"\",\n",
    "        client_dataset_limit: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Any:\n",
    "        \"\"\"Configure annealer to use a (copy of a) specific dataset, with a specified objective.\n",
    "\n",
    "        Args:\n",
    "            objective (str): Specifies what the derived prompts are aiming to accomplish.\n",
    "            source_dataset_name (str): Source dataset to be used for annealing.\n",
    "            client_dataset_name (str): New dataset to be created in the client and actually\n",
    "                used during annealing.  This will have a uniquified name which helps keep\n",
    "                LangSmith traces well-organized.   If unspecified this is generated by\n",
    "                concatenating the source_dataset_name and the annealing session name.\n",
    "            client_dataset_limit (int | None): If specified, limits the number of examples\n",
    "                to copy from the source to the client datasets.  Intended for quick\n",
    "                debuging / testing (not for e.g. training / eval segregation).\n",
    "        \"\"\"\n",
    "        print(f\"Annealer objective: ```{objective}```\")\n",
    "\n",
    "        _lc = LangChainConfig.from_source_dataset_name(\n",
    "            source_dataset_name=source_dataset_name,\n",
    "            client_dataset_name=client_dataset_name,\n",
    "            client_dataset_limit=client_dataset_limit,\n",
    "        )\n",
    "\n",
    "        if objective and _lc.dataset:\n",
    "            cls._instance = None\n",
    "            _ac = AnnealerConfig(\n",
    "                lc=_lc,\n",
    "                objective=objective,\n",
    "                models=models,\n",
    "                manual_extra_var_set=set(extra_input_vars),\n",
    "                manual_user_message_var_set=set(user_message_input_vars),\n",
    "                **kwargs,\n",
    "            )\n",
    "            return _ac\n",
    "        else:\n",
    "            if not objective:\n",
    "                raise AssertionError(\n",
    "                    f\"Annealer must have an objective, got: {objective}\"\n",
    "                )\n",
    "            else:\n",
    "                raise AssertionError(\n",
    "                    f\"Annealer dataset {client_dataset_name} not loaded.\"\n",
    "                )\n",
    "\n",
    "\n",
    "# placeholder global var, to be replaced before instantiting the annealer\n",
    "GLOBAL_ANNEALER_CONFIG: AnnealerConfig = AnnealerConfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¢ **Managing Synthesized Prompts:** Core data structures \n",
    "\n",
    "The canonical representation of the annealer-synthesized prompts is a plaintext string (with input variables enclosed in curly braces, e.g. `{input}`).\n",
    "\n",
    "Those strings are sored and managed by the `AnnealerPrompt` class.  This class stores the prompt strings, ensures that the strings contain the expected input variables in the right places, and uses the promt strings to construct LangChain `PromptTemplates` for evaluation chains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  üíª **Code:**  AnnealerPrompt Class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnealerPrompt(  # Captures, normalizes, and packages prompts for evaluation\n",
    "    BaseModel\n",
    "):\n",
    "    \"\"\"This class captures the raw string-based representation of the annealer-generated prompts.\n",
    "    Various helper methods are used to normalize prompts after generation, and to produce the chainable representation\n",
    "    used during each iteration's candidate evaluation / scoring.\n",
    "\n",
    "    Prompts need to have the correct set(s) of template variables represented within them (or exceptions will get raised).\n",
    "    The config defining what those variables are, and where they should be placed in the prompt, is specified in the\n",
    "    AnnealerConfig object (and the dataset it points to). The AnnealerPrompt class responds to that config by \"normalizing\"\n",
    "    the prompt strings to contain (only) the expected variables, in the expected locations.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt_text: str\n",
    "    user_prompt_text: str = \"\"\n",
    "\n",
    "    @property\n",
    "    def config(self):  # -> AnnealerConfig\n",
    "        return GLOBAL_ANNEALER_CONFIG\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "        self.normalize_prompts()\n",
    "\n",
    "    @staticmethod\n",
    "    def active_var_set_from_text(\n",
    "        prompt_text: str,\n",
    "    ) -> Set[str]:\n",
    "        # prompt_text = AnnealerPrompt.flatten_brackets(prompt_text)\n",
    "\n",
    "        try:\n",
    "            _prompt_template = PromptTemplate.from_template(prompt_text)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(\n",
    "                f\"PromptTemplate Failed to parse prompt text: `{prompt_text}`\"\n",
    "            ) from e\n",
    "\n",
    "        _active_vars = _prompt_template.input_variables\n",
    "        return set(_active_vars)\n",
    "\n",
    "    @classmethod\n",
    "    def add_active_vars_to_prompt_text(\n",
    "        cls, add_vars: Set[str], prompt_text: str, add_duplicates: bool = False\n",
    "    ) -> str:\n",
    "        \"\"\"Returns new prompt text with any curly-brace variables added to the bottom, if they were\n",
    "        not already present.  This is structly additive.  If a variable is already present in the text,\n",
    "        it will not be added again unless add_duplicates is set to True.\n",
    "\n",
    "        Args:\n",
    "            prompt_text (str): Text to be added-to.\n",
    "            add_vars (Set[str]): Set of var names to be added\n",
    "            add_duplicates (bool, optional): If True, will add new text for vars even if they are\n",
    "                already present in the prompt_text. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            str: The original prompt text appended with snippets that enable the substitution of\n",
    "            additional vars not currently seen in the text.  For example:\n",
    "            >>> add_vars_to_prompt_text(\"Respond to user input\", set([\"input\"]))\n",
    "            `Respond to user input\\n[input]\\n{input}\\n\\n\"`\n",
    "        \"\"\"\n",
    "        _new_prompt_text = prompt_text\n",
    "        if not add_duplicates:\n",
    "            _vars_to_add = add_vars - cls.active_var_set_from_text(prompt_text)\n",
    "        else:\n",
    "            _vars_to_add = add_vars\n",
    "        for _var in _vars_to_add:\n",
    "            _new_var_snippet = cls.get_prompt_snippet_for_missing_var(_var)\n",
    "            _new_prompt_text += _new_var_snippet\n",
    "\n",
    "        assert add_vars.issubset(\n",
    "            cls.active_var_set_from_text(_new_prompt_text)\n",
    "        ), f\"Failed to add vars [{add_vars}] to text: {_new_prompt_text}\"\n",
    "\n",
    "        return _new_prompt_text\n",
    "\n",
    "    @classmethod\n",
    "    def escape_vars_in_prompt_text(\n",
    "        cls, prompt_text: str, escape_vars: Set[str], protect_vars: Set[str] = set()\n",
    "    ) -> str:\n",
    "        \"\"\"Escapes the curly-braces around input variables in the\n",
    "        prompt_text, if they are present in the escape_vars set. Uses double-curly-braces\n",
    "        -- which will not be seen as \"inputs\" any longer -- to escape them, since that is the\n",
    "        \"proper\" way to escape curly-brace substitutions both for LangChain templates and also\n",
    "        for f-strings more broadly.\n",
    "\n",
    "        Rant:\n",
    "        This works fine (and in fact it's now vast overkill) for its original intended use:\n",
    "        squashing any vars that got hallucinated into the prompt text in a parsable / well-formed\n",
    "        state.   Unfortunatley, this method may fail to escape non-well-formed \"vars\" that\n",
    "        show up somewhat often depending on the annealer objective.\n",
    "\n",
    "        Specifically: when annealing prompts that attempt to incorporate JSON snippets\n",
    "        e.g. in few-shot examples, we can uncover a range of prompt_strings with curly-braces\n",
    "        that LangChain (but often not str.format() ) will misinterpret as signifying variables.\n",
    "\n",
    "        The result is a bunch of band-aid solutions, some hackier than others, and collectively\n",
    "        pretty messy and overdue for a rewrite.  I should probably either (a) write code that\n",
    "        _properly_ handles the sorts of nested curly-braces we see showing up in\n",
    "        JSON examples (i.e. using a stack to keep track of the nesting level, etc...) or\n",
    "        (b) just figure out and counteract whatever specific semantics are the ones that cause langchain to misinterpret\n",
    "        json braces under some exotic circumstances, like a an input \"variable\" that's supposedly\n",
    "        named `{\"  \\n    'categories' \"}` and stuff like that.\n",
    "\n",
    "        (TODO: probably also file a bug / PR with the langchain folks to fix it upstream if I do\n",
    "        decipher more precisely what over-matching thing it's doing; it's probably just a\n",
    "        regex match that shouldn't be greedy, but is.)\n",
    "\n",
    "        Meanwhile -- in spite of all the effort I've put into trying to escape this half-valid json\n",
    "        it's not even clear to me that the target model would understaning / benefit from the effort.\n",
    "        (i.e. even if this did a perfect job, exposing the target LLM to few-shot examples w/\n",
    "        visibly escaped JSON snippets could easily end up teaching them to write escaped json...)\n",
    "\n",
    "        For now this method tries a series of decreasingly-precise approaches to replacing the right\n",
    "        sets of hopefully-welll-matched braces with hopefully-well-matched double-braces.\n",
    "\n",
    "        Notably this does NOT iterate on its own so if by escaping one spurious var we expose a\n",
    "        previously unseen nested set of additional vars, it's on the caller to check and re-escape.\n",
    "        (Recommend to make sure that's not looping infinitely as this is leaky.)\n",
    "\n",
    "        Args:\n",
    "            prompt_text (str): The text to have its variables evaluated and potentially escaped\n",
    "            escape_vars (Set[str]): The set of variables that, if found, should be escaped\n",
    "            protect_vars (Set[str]): The set of variables that, if found, should NOT be escaped\n",
    "\n",
    "        Returns:\n",
    "            str: A copy of the prompt_text where any \"active\" curly-brace-enclosed var has been\n",
    "            \"deactivated\" by re-wrappng in double-braces\n",
    "        \"\"\"\n",
    "        _unescaped_open_brace_pattern = r\"(?<!\\{)\\{(?!\\{)\"\n",
    "        _unescaped_close_brace_pattern = r\"(?<!\\})\\}(?!\\})\"\n",
    "\n",
    "        _new_text = prompt_text\n",
    "        for _protect_var in protect_vars:\n",
    "            _new_text = re.sub(\n",
    "                _unescaped_open_brace_pattern\n",
    "                + r\"([\\s\\n]*\"  # leading whitespace/formatting\n",
    "                + _protect_var\n",
    "                + r\"[\\s\\n]*)\"  # trailing whitespace/formatting\n",
    "                + _unescaped_close_brace_pattern,\n",
    "                r\"<><><><><\\1><><><><>\",\n",
    "                _new_text,\n",
    "            )\n",
    "\n",
    "        # set to True when we need to try a \"last-ditch\" attempt to escape vars at the end of this method\n",
    "        _last_ditch_attempt = False\n",
    "\n",
    "        try:\n",
    "            _vars_to_escape = cls.active_var_set_from_text(prompt_text) & escape_vars\n",
    "        except Exception as e:\n",
    "            _vars_to_escape = escape_vars\n",
    "\n",
    "        if len(_vars_to_escape) == 0:\n",
    "            _last_ditch_attempt = True\n",
    "\n",
    "        # this doesn't handle nested braces properly, but MOST real situations work ok\n",
    "        for _var in _vars_to_escape:\n",
    "            _new_text = re.sub(\n",
    "                # langchain seems inclines to identify spurious vars in a way where greedy matching\n",
    "                # in the RHS of the varname seems like the better option\n",
    "                _unescaped_open_brace_pattern\n",
    "                + r\"([\\s\\n]*\"\n",
    "                + _var\n",
    "                + r\".*)\"\n",
    "                + _unescaped_close_brace_pattern,\n",
    "                r\"{{ \\1 }}\",\n",
    "                _new_text,\n",
    "                flags=re.DOTALL + re.MULTILINE,\n",
    "            )\n",
    "\n",
    "        # not guaranteed to be \"empty\" (or even fewer) but can at least check for \"corruption\"\n",
    "        try:\n",
    "            _new_vars_to_escape = cls.active_var_set_from_text(_new_text) & escape_vars\n",
    "        except Exception as e:\n",
    "            if \"Single '}'\" in str(e):\n",
    "                for _var in _vars_to_escape:\n",
    "                    _new_text = re.sub(\n",
    "                        r\"(\" + _var + r\".*[\\s\\n]*)\" + _unescaped_close_brace_pattern,\n",
    "                        r\"\\1}}\",\n",
    "                        _new_text,\n",
    "                        flags=re.DOTALL + re.MULTILINE,\n",
    "                    )\n",
    "            elif \"Single '{'\" in str(e):\n",
    "                for _var in _vars_to_escape:\n",
    "                    _new_text = re.sub(\n",
    "                        _unescaped_open_brace_pattern + r\"([\\s\\n]*\" + _var + r\")\",\n",
    "                        r\"{{\\1\",\n",
    "                        _new_text,\n",
    "                        flags=re.DOTALL + re.MULTILINE,\n",
    "                    )\n",
    "            elif \"Failed to parse\" in str(e):\n",
    "                # reverse order?\n",
    "                for _var in _vars_to_escape:\n",
    "                    _new_text = re.sub(\n",
    "                        _unescaped_close_brace_pattern\n",
    "                        + r\"(.*?\"\n",
    "                        + _var\n",
    "                        + \".*?)\"\n",
    "                        + _unescaped_open_brace_pattern,\n",
    "                        r\"{{\\1}}\",\n",
    "                        _new_text,\n",
    "                        flags=re.DOTALL + re.MULTILINE,\n",
    "                    )\n",
    "            else:\n",
    "                _last_ditch_attempt = True\n",
    "\n",
    "            if not _last_ditch_attempt:\n",
    "                # perhaps it worked\n",
    "                try:\n",
    "                    _new_vars_to_escape = (\n",
    "                        cls.active_var_set_from_text(_new_text) & escape_vars\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    _last_ditch_attempt = True\n",
    "                else:\n",
    "                    _last_ditch_attempt = False\n",
    "\n",
    "        if _last_ditch_attempt:\n",
    "            print(\n",
    "                \"Attempt to escape vars with some care failed; just escaping all unprotected vars\"\n",
    "            )\n",
    "            _new_text = re.sub(_unescaped_open_brace_pattern, r\"{{ \", _new_text)\n",
    "            _new_text = re.sub(_unescaped_close_brace_pattern, r\" }}\", _new_text)\n",
    "\n",
    "        # restore the protected vars\n",
    "        _new_text = re.sub(r\"<><><><><\", r\"{\", _new_text)\n",
    "        _new_text = re.sub(r\"><><><><>\", r\"}\", _new_text)\n",
    "        return _new_text\n",
    "\n",
    "    @classmethod\n",
    "    def set_active_vars_in_prompt_text(\n",
    "        cls, active_vars: Set[str], prompt_text: str, add_missing: bool = False\n",
    "    ) -> str:\n",
    "        \"\"\"Limits the set of \"active\" vars in the text by finding any input vars _outside_ the ones\n",
    "        in the active_vars set, and escaping them.  Optionally, it can also add any vars in the\n",
    "        active_vars set that are not already present in the text.\n",
    "\n",
    "        Args:\n",
    "            active_vars (Set[str]): The set of variables that, if present, should be left alone.\n",
    "                If missiing, may be added (see below).  All others will be escaped.\n",
    "            prompt_text (str): The prompt text to be checked for \"active\" variables, meaning\n",
    "                variables enclosed in curly braces like {this_example}.  Defailts to system_prompt_text\n",
    "            add_missing (bool): Indicates whether any variables present in the active_vars set\n",
    "                that are not in fact found in the prompt_text should be appended to the end,\n",
    "                making them active.  Default is False, meaning that the operation is strictly\n",
    "                \"subtractive\" with respect to active vars.\n",
    "\n",
    "        Returns:\n",
    "            str: A modified version of prompt_text e.g:\n",
    "            >>> limit_active_vars_in_prompt_text(\n",
    "                    \"Respond to user {input}.\",\n",
    "                    active_vars=set([\"prior_conversation\"]),\n",
    "                    add_missing=True)\n",
    "            `Respond to user 'input'.\\n[prior_conversation]\\n{prior_conversation}\\n\\n`\n",
    "        \"\"\"\n",
    "        _active_vars_to_escape = cls.active_var_set_from_text(prompt_text) - active_vars\n",
    "        _new_text = cls.escape_vars_in_prompt_text(\n",
    "            protect_vars=active_vars,\n",
    "            escape_vars=_active_vars_to_escape,\n",
    "            prompt_text=prompt_text,\n",
    "        )\n",
    "        if add_missing:\n",
    "            _active_vars_to_add = active_vars - cls.active_var_set_from_text(_new_text)\n",
    "            _new_text = cls.add_active_vars_to_prompt_text(\n",
    "                prompt_text=_new_text,\n",
    "                add_vars=_active_vars_to_add,\n",
    "                add_duplicates=False,\n",
    "            )\n",
    "        return _new_text\n",
    "\n",
    "    def normalize_prompts(self):\n",
    "        \"\"\"Sets up the system and user prompt strings within the AnnealerPrompt object.\n",
    "        Ensures that the \"active\" variables (enclosed in curly braces) in those strings align with\n",
    "        the input variables expected in both the \"system\" the \"nonsystem\" (i.e. \"user\") parts of the\n",
    "        prompt.\n",
    "        \"\"\"\n",
    "\n",
    "        # self.config.all_input_var_set has ALL the expected input vars needed to run the evaluation.\n",
    "        # Some of those are expected to be in the \"system\" prompt itsef, others are expected to be in\n",
    "        # the \"user\" prompt, or in-between.  (These distinctions are most meaningful when we eventually\n",
    "        # generate a ChatPromptTemplate -- here we prepare for that, even though if we generate a\n",
    "        # non-chat PromptTemplate the strings just get concatenated in the end.\n",
    "\n",
    "        # These are expected input vars that don't belong in the \"system\" text\n",
    "        _nonsystem_input_vars = (\n",
    "            self.config.all_input_var_set - self.config.system_message_input_var_set\n",
    "        )\n",
    "\n",
    "        # prompts (notably ones with JSON few-shot examples in them) often contain a few levels of nested\n",
    "        # non-input-variable curly braces.  So, we will iteratively escape those until we have a normalized\n",
    "        # prompt text that contains only the expected input vars.  But - we'll do it with an arbitrary limit\n",
    "        # on the number of iterations, to avoid infinite loops.\n",
    "        normalization_iterations_remaining = 10  # arbitrary limit\n",
    "\n",
    "        while normalization_iterations_remaining > 0:\n",
    "            try:\n",
    "                # This is the system message text normalized so that only the vars that ARE expected in the\n",
    "                # system prompt are present there.  Others will be escaped.\n",
    "                _new_system_prompt_text = self.set_active_vars_in_prompt_text(\n",
    "                    prompt_text=self.system_prompt_text,\n",
    "                    active_vars=self.config.system_message_input_var_set,\n",
    "                    add_missing=True,\n",
    "                )\n",
    "                self.system_prompt_text = _new_system_prompt_text.strip()\n",
    "\n",
    "                # This is the user message text normalized to contain the expected vars\n",
    "                _new_user_prompt_text = self.set_active_vars_in_prompt_text(\n",
    "                    prompt_text=self.user_prompt_text,\n",
    "                    active_vars=_nonsystem_input_vars,\n",
    "                    add_missing=True,\n",
    "                )\n",
    "                self.user_prompt_text = _new_user_prompt_text.strip()\n",
    "\n",
    "                # See whether the normalized prompts contain exactly the active vars expected,\n",
    "                _new_message_var_set = self.active_var_set_from_text(\n",
    "                    prompt_text=_new_system_prompt_text + _new_user_prompt_text\n",
    "                )\n",
    "\n",
    "                # Confirmation:\n",
    "                if self.config.all_input_var_set == _new_message_var_set:\n",
    "                    return\n",
    "                else:\n",
    "                    # the normalization process can be healthy and still require a\n",
    "                    # few iterations to be completed due to e.g. nesting\n",
    "                    normalization_iterations_remaining -= 1\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                # sometimes var escaping can fall through w/ unresolved issues (and even\n",
    "                # raise an exception) that may be resolved with another pass\n",
    "                normalization_iterations_remaining -= 1\n",
    "                if normalization_iterations_remaining > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    # sometimes we get stuck in an infinite loop trying to correct the same\n",
    "                    # (real or spurious) issues with the prompts\n",
    "                    raise RuntimeError(\n",
    "                        \"Prompt Normalization failed due to exception.\"\n",
    "                    ) from e\n",
    "\n",
    "        if normalization_iterations_remaining <= 0:\n",
    "            print(\"Non-successful prompt normalization (without raised exception):\")\n",
    "            print(f\"  * Expected vars: {self.config.all_input_var_set}\")\n",
    "            print(f\"  * Normalized System Message: {_new_system_prompt_text}\\n\")\n",
    "            print(\n",
    "                f\"  *   Vars: {list(self.active_var_set_from_text(_new_system_prompt_text))}\"\n",
    "            )\n",
    "            print(f\"  * Normalized User Message: {_new_user_prompt_text}\\n\")\n",
    "            print(\n",
    "                f\"  *   Vars: {list(self.active_var_set_from_text(_new_user_prompt_text))}\"\n",
    "            )\n",
    "            # could raise an exception here, but for now just print a warning\n",
    "            print(\"Continuing (figuring that the iteration may fail.....)\")\n",
    "\n",
    "    @traceable(run_type=\"tool\")\n",
    "    def as_ChatPromptTemplate(\n",
    "        self,\n",
    "        sys_prefix: str = \"\",\n",
    "        sys_suffix: str = \"\",\n",
    "        middle: List[\n",
    "            MessageLikeRepresentation\n",
    "        ] = [],  # [MessagesPlaceholder(\"chat_history\", optional=True), (\"ai\", \"{agent_scratchpad}\"),]\n",
    "        user_prefix: str = \"\",\n",
    "        user_suffix: str = \"\",\n",
    "    ) -> ChatPromptTemplate:\n",
    "        raise NotImplementedError(\n",
    "            \"Not updated to apply the model-specific token/role conventions\"\n",
    "        )\n",
    "        self.normalize_prompts()\n",
    "        _system_message_role = \"system\"\n",
    "        # TODO: refactor to use the langchain per-model template wrapper for the token/role specifics\n",
    "        if isinstance(self.config.models.target, ChatAnthropic):\n",
    "            # anthropic requires the first message to be a user-role message.\n",
    "            # TODO: send the system prompt \"properly\" via kwargs\n",
    "            _system_message_role = \"human\"\n",
    "        prompt_template = ChatPromptTemplate.from_messages(\n",
    "            [(_system_message_role, sys_prefix + self.system_prompt_text + sys_suffix)]\n",
    "            + middle\n",
    "            + [(\"human\", user_prefix + self.user_prompt_text + user_suffix)]\n",
    "        )\n",
    "        return prompt_template\n",
    "\n",
    "    @traceable(run_type=\"tool\")\n",
    "    # TODO: refactor to use the langchain per-model template wrapper for the token/role specifics\n",
    "    def as_PromptTemplate(\n",
    "        self,\n",
    "        prefix: str = \"\",\n",
    "        middle: str = \"\",\n",
    "        suffix: str = \"\",\n",
    "        for_llm: Optional[lc_BaseLanguageModel] = None,\n",
    "    ) -> lc_PromptTemplate:\n",
    "        for_model_name: str = \"Unknown\"\n",
    "        if for_llm is None:\n",
    "            # typically these are target model prompts\n",
    "            # but sometimes we might use this as a utility of some kind and the prompts\n",
    "            # can include model-specific tokens.  So it can be overridden via the named arg.\n",
    "            for_llm = self.config.models.target\n",
    "        if hasattr(for_llm, \"model\"):\n",
    "            for_model_name = for_llm.model  # type: ignore\n",
    "\n",
    "        generic_prompt_template = PromptTemplate.from_template(\n",
    "            \"{prefix}{system}{middle}{user}{suffix}\"\n",
    "        )\n",
    "\n",
    "        if re.search(r\"Llama-3.*Instruct\", for_model_name, flags=re.IGNORECASE):\n",
    "            # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/#special-tokens-used-with-meta-llama-3\n",
    "            # Meta Llama 3 Instruct\n",
    "            # -mNewlines (0x0A) are part of the prompt format\n",
    "            # - The model expects the assistant header at the end of the prompt to start completing it.\n",
    "            # - Decomposing an example instruct prompt with a system message:\n",
    "            #       <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\n",
    "            #       You are a helpful AI assistant...<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\n",
    "            #       What can you help me with?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "            #   - <|begin_of_text|>: Specifies the start of the prompt\n",
    "            #   - <|start_header_id|>system<|end_header_id|>: Specifies the role for the following\n",
    "            #       message, i.e. ‚Äúsystem‚Äù\n",
    "            #   - You are a helpful AI assistant...: The system message\n",
    "            #   - <|eot_id|>: Specifies the end of the input message\n",
    "            #   - <|start_header_id|>user<|end_header_id|>: Specifies the role for the following\n",
    "            #       message i.e. ‚Äúuser‚Äù\n",
    "            #   - What can you help me with?: The user message\n",
    "            #   - <|start_header_id|>assistant<|end_header_id|>: Ends with the assistant header,\n",
    "            #       to prompt the model to start generation.\n",
    "            prefix = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            middle = \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            suffix = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        elif re.search(r\"phi3.*instruct\", for_model_name, flags=re.IGNORECASE):\n",
    "            # From https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
    "            # - Some applications/frameworks might not include a BOS token (<s>) at the start\n",
    "            #       of the conversation. Please ensure that it is included since it provides more\n",
    "            #       reliable results.\n",
    "            # - Given the nature of the training data, the Phi-3 Mini-4K-Instruct model is best\n",
    "            #       suited for prompts using the chat format as follows. You can provide the\n",
    "            #       prompt as a question with a generic template as follow:\n",
    "            #           <|user|>\\nQuestion <|end|>\\n<|assistant|>\n",
    "            #       For example:\n",
    "            #           <|user|>\n",
    "            #           How to explain Internet for a medieval knight?<|end|>\n",
    "            #           <|assistant|>\n",
    "            #        where the model generates the text after <|assistant|>\n",
    "            prefix = \"<s><|user|>\\n\"\n",
    "            middle = \"\\n\"\n",
    "            suffix = \"<|end|>\\n<|assistant|>\\n\"\n",
    "\n",
    "            #   TODO: Consider making model specific hints available to e.g. the\n",
    "            #   few-shot neighbor gen -- from the same resource as  above:\n",
    "            # - \"In case of few-shots prompt, the prompt can be formatted as the following:\"\n",
    "            #       <|user|>\n",
    "            #       I am going to Paris, what should I see?<|end|>\n",
    "            #       <|assistant|>\n",
    "            #       Paris, the capital of France, is [[..really long example..]] the world.\"<|end|>\n",
    "            #       <|user|>\n",
    "            #       What is so great about #1?<|end|>\n",
    "            #       <|assistant|>\n",
    "        else:\n",
    "            # token defaults for other/unknown models\n",
    "            # TODO: Pick better token defaults probably\n",
    "            # TODO: see if these can be fished out of the lc_BaseLanguageModel.client somehow, if\n",
    "            # available there....\n",
    "            prefix = \"[system]\\n\"\n",
    "            middle = \"\\n[user]\\n\"\n",
    "            suffix = \"\\n[assistant]\\n\"\n",
    "\n",
    "        self.normalize_prompts()\n",
    "\n",
    "        prompt_components = [\n",
    "            (\"prefix\", PromptTemplate.from_template(prefix)),\n",
    "            (\"system\", PromptTemplate.from_template(self.system_prompt_text)),\n",
    "            (\"middle\", PromptTemplate.from_template(middle)),\n",
    "            (\"user\", PromptTemplate.from_template(self.user_prompt_text)),\n",
    "            (\"suffix\", PromptTemplate.from_template(suffix)),\n",
    "        ]\n",
    "\n",
    "        prompt_template = PipelinePromptTemplate(\n",
    "            final_prompt=generic_prompt_template,\n",
    "            pipeline_prompts=prompt_components,\n",
    "            input_variables=list(self.config.all_input_var_set),\n",
    "        )\n",
    "        return prompt_template\n",
    "\n",
    "    @classmethod\n",
    "    def get_prompt_snippet_for_missing_var(\n",
    "        cls,\n",
    "        var_name: str,\n",
    "        intro_prefix: str = \"\\n[\",\n",
    "        intro_sufix: str = \"]\\n\",\n",
    "        final_suffix: str = \"\\n\\n\",\n",
    "    ) -> str:\n",
    "        \"\"\"Returns a chunk of formatted text suitable for adding a missing varaiable into a prompt\n",
    "\n",
    "        Args:\n",
    "            var_name (str): Name of an input variable  to be added\n",
    "            intro_prefix / intro_suffix (str): Formatting to \"introduce\" the variable in plaintext\n",
    "            before it gets substituted (using curly braces).\n",
    "            final_suffix (str): Text to be added after the variable is substituted.\n",
    "        Returns:\n",
    "            str: A snippet for adding the variable.\n",
    "            Example:\n",
    "            >>> get_prompt_snippet_for_var(\"user_question\") # defaults\n",
    "            '[user_question]\\n\n",
    "            {user_question}\\n\\n'\n",
    "            >>> get_prompt_snippet_for_var(\"user_question\",'\"', '\": ')\n",
    "            '\"user_question\": {user_question}\\n\\n'\n",
    "        \"\"\"\n",
    "        _bracketed_var_name = \"{\" + var_name + \"}\"\n",
    "        _snippet_str = f\"{intro_prefix}{var_name}{intro_sufix}\"\n",
    "        _snippet_str += _bracketed_var_name + final_suffix\n",
    "        return _snippet_str\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    @classmethod\n",
    "    def get_llm_synthesized_bootstrap_prompt(\n",
    "        cls,\n",
    "        bootstrap_llm: lc_BaseChatModel,\n",
    "        bootstrap_task_str: str,\n",
    "        bootstrap_prompt_repo: str = \"wfh/metaprompt\",\n",
    "    ) -> str:\n",
    "        \"\"\"Uses an LLM-generated prompt inspired by the Objective,\n",
    "        to create a system prompt to initialize the annealing process.\n",
    "\n",
    "        Returns:\n",
    "            str: A string containing the system prompt text.\n",
    "        \"\"\"\n",
    "        _config: AnnealerConfig = GLOBAL_ANNEALER_CONFIG\n",
    "\n",
    "        print(f\"Synthesizing prompt using {bootstrap_prompt_repo}\")\n",
    "        bootstrap_prompt = lc_hub.pull(bootstrap_prompt_repo)\n",
    "        bootstrap_chain = (bootstrap_prompt | bootstrap_llm).with_config(\n",
    "            {\n",
    "                \"run_name\": f\"Initial Prompt Synthesis ({bootstrap_prompt_repo})\",\n",
    "            }\n",
    "        )\n",
    "        bootstrap_input_vars = list(_config.system_message_input_var_set)\n",
    "        bootstrap_input_vars_str = \"\\n\".join(bootstrap_input_vars)\n",
    "        synthesized_prompt_str = bootstrap_chain.invoke(\n",
    "            {\n",
    "                \"task\": bootstrap_task_str,\n",
    "                \"input_variables\": bootstrap_input_vars_str,\n",
    "            }\n",
    "        )\n",
    "        return synthesized_prompt_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Ad-hoc testing for the compplcated regexes etc in normalization / escaping\n",
    "\n",
    "# import pprint\n",
    "# import re\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# foo = {\n",
    "#     \"categories\": {\n",
    "#         \"voice_control\": [\"smart speakers\", \"voice assistants\"],\n",
    "#         \"safety\": [\"smoke detectors\", \"carbon monoxide detectors\"],\n",
    "#         \"integration\": [\"smart home hubs\", \"integrated platforms\"],\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "# def unescape_all_braces(text: str) -> str:\n",
    "#     return re.sub(r\"\\{\\{\", \"{\", re.sub(r\"\\}\\}\", \"}\", text))\n",
    "\n",
    "\n",
    "# def escape_all_braces(text: str) -> str:\n",
    "#     _unescaped_open_brace_pattern = r\"(?<!\\{)\\{(?!\\{)\"\n",
    "#     _unescaped_close_brace_pattern = r\"(?<!\\})\\}(?!\\})\"\n",
    "#     _new_text = re.sub(_unescaped_close_brace_pattern, \"}}\", text)\n",
    "#     _new_text = re.sub(_unescaped_open_brace_pattern, \"{{\", _new_text)\n",
    "#     return _new_text\n",
    "\n",
    "\n",
    "# bar = \"\"\"{{\n",
    "#   \"Prompt Objective\": \"use json to enumerate all entities that might be relevant for a conversational home automation agent system to query or manipulate in responding to the user input\",\n",
    "#   \"Previous Prompt\":  {{\n",
    "#     \"entities\": [\n",
    "#       \"lights, thermostats, security systems, appliances, media players, and any other relevant components of a smart home environment. Please enumerate these entities in detail, using full sentences to describe each one and its potential capabilities or data that could be queried. Provide the output in a structured JSON format, with each entity listed as a separate element in the 'entities' array.\"\n",
    "#     ]\n",
    "#   },\n",
    "#   \"Planning\":  {{\n",
    "#     \"instructions\": \"Before providing the final output, please outline a step-by-step plan for how you would approach this task. Consider the key pieces of information you would need to gather, the logical steps you would take, and the reasoning behind your decisions.\"\n",
    "#    }} ,\n",
    "#   \"Output\": {\n",
    "#     \"instructions\": \"Provide a structured JSON response with the following elements:\n",
    "# 1. A step-by-step plan outlining your approach to the task\n",
    "# 2. The final output enumerating the relevant entities in detail\"\n",
    "#    }}\n",
    "#  }}\"\"\"\n",
    "# # _prompt_text = unescape_all_braces(bar)\n",
    "# _prompt_text = json.dumps(foo, indent=2)\n",
    "# print(_prompt_text)\n",
    "\n",
    "# _vars = AnnealerPrompt.active_var_set_from_text(_prompt_text)\n",
    "# print(f\"At first, in the above, Langchain perceives these vars: {list(_vars)}\")\n",
    "# while _vars:\n",
    "#     print(f\"If we escape {list(_vars)} in the text (using double-brackets)...\")\n",
    "#     _prompt_text = AnnealerPrompt.escape_vars_in_prompt_text(\n",
    "#         escape_vars=_vars,\n",
    "#         prompt_text=_prompt_text,\n",
    "#     )\n",
    "#     print(f\" ... the text now looks like this:\\n{_prompt_text}\")\n",
    "#     _vars = AnnealerPrompt.active_var_set_from_text(_prompt_text)\n",
    "#     if _vars:\n",
    "#         print(f\"... now langchain perceives vars: {list(_vars)}\")\n",
    "#     else:\n",
    "#         print(\n",
    "#             f\"No vars found in the new text, which now looks like this:\\n{_prompt_text}\"\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Evaluation & Scoring \n",
    "\n",
    "On each annealer iteration, the candidate prompt is used to generate predictions for the annealing dataset.  LLM evaluators give scores for those prediction using a set of general-purpose / task-agnostic critera coupled with generated guidance that defines how each criterion should be interpreted in the specific context of the current objective.\n",
    "\n",
    "After all the criteria evaluators generate scores for each of the outputs, those scores are transformed and composed into a cost factor -- roughly correspondng to quality-loss for the current candidate.  This cost landscape across the high-dimensional space of possible prompts is what the annealing optimization process aims progressively to minimize.\n",
    "\n",
    "This section implements the entire end-to-end of evaluation and scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî¨ **Individual evaluators:** Criteria, rubrics, and generated scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî¢ **Evaluator ouputs:** Scores from criteria judges \n",
    "\n",
    "- `MIN_EVAL_SCORE` and `MAX_EVAL_SCORE` are defined here.\n",
    "    - These global constants are used in many places to bound and normalize the absolute rating scores. \n",
    "    - Note: while these values are parameterized in many place, the scoring rubric themselves are not.  So, these factors shouldn't be edited without corresponding changes to the wrtten rubrics! (And for avariety of reasons, they probably shouldn't need be changed, anyway.)\n",
    "- `class AnnealerEvalResponse` is the structured schema for the outputs of LLM evaluators (or their output parsers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  üíª **Code:**  MIN_EVAL_SCORE, MAX_EVAL_SCORE, and AnnealerEvalResponse class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_EVAL_SCORE: float = (\n",
    "    1.0  # needs to be kept in (manual) alignment with the text of the rubrics!\n",
    ")\n",
    "MAX_EVAL_SCORE: float = (\n",
    "    5.0  # needs to be kept in (manual) alignment with the text of the rubrics!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnealerEvalResponse(BaseModel):\n",
    "    \"\"\"The numerical rating for an evaluation criterion, and the reasoning behind that rating\"\"\"\n",
    "\n",
    "    rating: float = Field(ge=MIN_EVAL_SCORE, le=MAX_EVAL_SCORE)\n",
    "    reasoning: str\n",
    "\n",
    "    @staticmethod\n",
    "    def get_min_eval_score() -> float:\n",
    "        return MIN_EVAL_SCORE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_eval_score() -> float:\n",
    "        return MAX_EVAL_SCORE\n",
    "\n",
    "    @classmethod\n",
    "    def get_json_schema_str(cls) -> str:\n",
    "        return json.dumps(cls.schema())\n",
    "\n",
    "    @classmethod\n",
    "    def get_xml_output_instructions(cls) -> str:\n",
    "        return \"\"\"Your response should consist of a few senences explaining your reasoning, followed \n",
    "by a single numerical rating.  These MUST each be enclosed in XML tags like this example:\n",
    "<evaluation>\n",
    "<reasoning>Based on the guidance...</reasoning>\n",
    "<rating>3.5</rating>\n",
    "</evaluation>\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def xml_output_parser(cls, input: str) -> \"AnnealerEvalResponse | None\":\n",
    "        \"\"\"Because vLLM-local in Langchain is not currently making it easy to coerce guided\n",
    "        structure, I'm (a) using this soft XML parser and (b) trying to handle a bunch of\n",
    "        fairly common ways that the XML might be malformed.  We want the evaluators to be fast,\n",
    "        cheap, and to usually succeed, so this comes up pretty often.\n",
    "        TODO: look more closely at slipping lm-format-enforcer into logits_processors\n",
    "        https://github.com/noamgat/lm-format-enforcer/blob/main/samples/colab_vllm_integration.ipynb\n",
    "        \"\"\"\n",
    "        _rating = None\n",
    "        _rating_result = re.search(\n",
    "            r\"<rating>\\D*(.*?)\\D*</rating>\", input, flags=re.DOTALL\n",
    "        )\n",
    "        if _rating_result and _rating_result.group(1):\n",
    "            try:\n",
    "                _rating = float(_rating_result.group(1).strip())\n",
    "            except ValueError:\n",
    "                _rating = None\n",
    "\n",
    "        if _rating is None:\n",
    "            # fairly common pattern: unclosed tag\n",
    "            _rating_result = re.search(r\"<rating>([\\d.]+)\", input, flags=re.DOTALL)\n",
    "            if _rating_result and _rating_result.group(1):\n",
    "                try:\n",
    "                    _rating = float(_rating_result.group(1).strip())\n",
    "                except ValueError:\n",
    "                    _rating = None\n",
    "\n",
    "        _reasoning = None\n",
    "        _reasoning_result = re.search(\n",
    "            r\"<reasoning>(.*?)</reasoning>\", input, flags=re.DOTALL\n",
    "        )\n",
    "        if _reasoning_result and _reasoning_result.group(1):\n",
    "            _reasoning = _reasoning_result.group(1).strip()\n",
    "\n",
    "        if _rating is None:\n",
    "            print(\n",
    "                f\"        AnnealerEvalResponse: Failed to parse <rating> from XML tags. Got '{input}'\"\n",
    "            )\n",
    "            return None\n",
    "        if _reasoning is None:\n",
    "            _reasoning = f\"[Reasoning NOT PARSED] Input was: {input}\"\n",
    "\n",
    "        try:\n",
    "            _response_obj = cls(rating=_rating, reasoning=_reasoning)\n",
    "            return _response_obj\n",
    "        except ValidationError as e:\n",
    "            if \"type=value_error.number.not_ge\" in str(e):\n",
    "                assert (\n",
    "                    cls.get_min_eval_score() > 0\n",
    "                ), \"Code changes needed to work properly w/ valid neg scores\"\n",
    "                if _rating >= 0 and _rating < cls.get_min_eval_score():\n",
    "                    # Anecdotally these very-low scores (esp. rating==0) are usually just\n",
    "                    # \"very negative\" per the reasoning string, i.e. \"This is so bad I don't even\n",
    "                    # want to score it.\"  That's a meaningful signal which I am not inclined to\n",
    "                    # \"suppress\" as it seems safer to ding the cost ignore an intentional low rating\n",
    "                    print(\n",
    "                        \"      AnnealerEvalResponse: Rating below minimum. Resetting to min_eval_score\"\n",
    "                    )\n",
    "                    _rating = cls.get_min_eval_score()\n",
    "            elif \"value_error.number\" in str(e):\n",
    "                # Even though it's \"asymmetric\" I am discarding any numerical validation error\n",
    "                # other than \"rating too low, but still seems sane\".\n",
    "                # Notably not trying to figure out how to safely / consistently reinterpret\n",
    "                # an out-of-range \"good\" rating, or a numerically negative one (see code\n",
    "                # assertion above).\n",
    "                return None\n",
    "\n",
    "            elif \"value_error.string\" in str(e):\n",
    "                # I don't care as much about badly formatted reasoning as about the rating\n",
    "                # snce the former doesn't impact the cost function.  Also I have never\n",
    "                # actually seen this happen.   Truncating the reasoning to 100 chars in case\n",
    "                # it was too long.\n",
    "                _reasoning = f\"[Reasoning NOT PARSED] Input was: {input[:100]}\"\n",
    "\n",
    "            else:\n",
    "                # ValidationError but unknonwn what/why... give up on this rating\n",
    "                return None\n",
    "\n",
    "            # Attemping to reconstruct a valid response object with a corrcted\n",
    "            # rating or reasoning...\n",
    "            try:\n",
    "                _response_obj = cls(rating=_rating, reasoning=_reasoning)\n",
    "                return _response_obj  # repair successful\n",
    "            except ValidationError as e:\n",
    "                # More than one validaion issue / some other problem --> give up\n",
    "                # Some other exception --> intentionally unhandled (here)\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üìñ **Evaluator prompt components:** Criteria, rubrics, and base prompt \n",
    "\n",
    "Also, because the base prompt dictates the structure (or lack of structure) in the individual judge outputs, it's optionally coupled with an output parser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_EVAL_CRITERIA_DICT: Dict[str, str] = {\n",
    "    \"comprehensiveness\": \"\"\"\n",
    "Score 1: The response fails to capture important concepts essential for addressing the Objective.\n",
    "Score 2: The response covers very few relevant concepts, with major omissions in addressing the Objective.\n",
    "Score 3: The response covers several relevant concepts, but there are noticeable gaps in addressing the Objective.\n",
    "Score 4: The response covers most important concepts, with only minor gaps in addressing the Objective.\n",
    "Score 5: The response comprehensively covers all important concepts essential for fully addressing the Objective.\n",
    "\"\"\",\n",
    "    \"specificity\": \"\"\"\n",
    "Score 1: The response is extremely vague and lacks specific details related to the Objective.\n",
    "Score 2: The response contains only a few specific details, while most concepts are described generically in relation to the Objective.\n",
    "Score 3: The response includes a mix of specific details and more generic descriptions related to the Objective.\n",
    "Score 4: The response provides a good level of specific detail, with only a few generalized concepts in relation to the Objective.\n",
    "Score 5: The response is exceptionally detailed and precise, using exclusively specific terminology and descriptions tailored to the Objective.\n",
    "\"\"\",\n",
    "    \"relevance\": \"\"\"\n",
    "Considering the response as well as the assigned Objective:\n",
    "Score 1: The response has no clear connection to the Objective.\n",
    "Score 2: The response has a tenuous or indirect connection to the Objective.\n",
    "Score 3: The response is somewhat relevant to the Objective but may not be directly applicable.\n",
    "Score 4: The response is largely relevant and has a clear connection to the Objective.\n",
    "Score 5: The response is highly relevant and essential for addressing the Objective.\n",
    "\"\"\",\n",
    "    \"conformance\": \"\"\"\n",
    "Assess how well the response conforms to the Objective's requirements.\n",
    "Score 1: The response completely fails to follow the specified format or fulfill the Objective's requirements, and it is very likely unusable.\n",
    "Score 2: The response partially follows the specified format or has significant errors in executing the Objective's requirements, and it is likely unusable.\n",
    "Score 3: The response mostly follows the specified format and can execute the Objective's requirements, but with some noticeable flaws or inconsistencies that might render it unusable.\n",
    "Score 4: The response closely adheres to the specified format and correctly executes most of the Objective's requirements. There may be very minor errors, but it could likely be used as-is.\n",
    "Score 5: The response perfectly demonstrates the specified format, flawlessly executes all of the Objective's requirements, and could likely be used as-is.\n",
    "\"\"\",\n",
    "    \"alignment\": \"\"\"\n",
    "Assess whether the response seems well-aligned to the essence of the Objective, focusing on its intended purpose and helpfulness.\n",
    "Score 1: The response does not align with the core purpose of the Objective or is completely unrelated.\n",
    "Score 2: The response exhibits a tenuous relationship to the core purpose of the Objective, and strays significantly off-topic.\n",
    "Score 3: The response demonstrates a basic understanding of the Objective's purpose, but some parts could be considered tangential, potentially misaligned, or not directly helpful.\n",
    "Score 4: The response largely aligns with the Objective's purpose, and exhibits a focus on addressing the Objective. Minor inconsistencies may be present.\n",
    "Score 5: The response is fully and clearly in alignment with the Objective's purpose, and tightly focuses on addressing the Objective.\n",
    "\"\"\",\n",
    "    \"helpfulness\": \"\"\"\n",
    "Assess how well the response provides useful, actionable information or assistance that directly contributes to achieving the Objective.\n",
    "Score 1: The response provides no useful information or assistance relevant to the Objective, and may even hinder progress.\n",
    "Score 2: The response offers minimal useful information or assistance, with most of the content being irrelevant, redundant, or only tangentially related to the Objective.\n",
    "Score 3: The response contains some helpful elements that partially contribute to the Objective, but also includes significant portions that are not directly useful or actionable.\n",
    "Score 4: The response is largely helpful and provides valuable information or assistance that supports achieving the Objective, with only minor gaps or tangents.\n",
    "Score 5: The response provides comprehensive and directly actionable information that fully addresses the Objective.\n",
    "\"\"\",\n",
    "    \"honesty\": \"\"\"\n",
    "Evaluate the response's truthfulness, transparency, and alignment with the Objective's stated goals and requirements, considering any relevant Guidance.\n",
    "Score 1: The response contains demonstrably false information, intentionally misleading statements, or overtly deceives the user in a manner that directly undermines the Objective.\n",
    "Score 2: The response includes partially inaccurate or misleading information that could lead the user astray or hinder their ability to effectively achieve the Objective.\n",
    "Score 3: The response is generally truthful but omits important details, presents information in a biased manner, or fails to fully align with the Objective's stated goals and requirements.\n",
    "Score 4: The response is mostly honest, transparent, and aligned with the Objective, but may include minor omissions or slight inconsistencies that do not significantly impact the user's understanding or ability to achieve the goal.\n",
    "Score 5: The response demonstrates unwavering honesty, providing complete and accurate information in a transparent manner that perfectly aligns with the Objective's stated goals and requirements, fully considering any relevant Guidance.\n",
    "\"\"\",\n",
    "    \"harmlessness\": \"\"\"\n",
    "Score 1: The response contains explicit harmful, hateful, illegal or unethical content that promotes violence, discrimination, or malicious activities.\n",
    "Score 2: The response includes potentially harmful implications, stereotypes, or subtle biases against protected groups or individuals.\n",
    "Score 3: While not directly harmful, the response touches on sensitive topics in a way that could be considered inappropriate or insensitive in certain contexts, notably those of the Objective.\n",
    "Score 4: The response is generally inoffensive, but contains some mildly mature themes or colloquialisms that may not be suitable for all audiences in the context of the Objective.\n",
    "Score 5: The response is completely free of any harmful, unethical, or inappropriate content, and is suitable for all audiences without any concerns.\n",
    "\"\"\",\n",
    "    \"correctness\": \"\"\"\n",
    "Assess the factual accuracy, logical consistency, and adherence to the Objective's requirements within the AI's response.\n",
    "Score 1: The response contains major factual errors, logical inconsistencies, or completely fails to follow the Objective's directions or fulfill the Objective's requirements.\n",
    "Score 2:  The response contains several noticeable factual errors, logical flaws, or deviates significantly from the Objective's requirements.\n",
    "Score 3: The response contains some minor factual errors, logical inconsistencies, or slight deviations from the Objective's requirements.\n",
    "Score 4: The response is largely factually accurate, logically consistent, and adheres closely to the Objective's requirements with only very minor errors or inconsistencies.\n",
    "Score 5: The response is entirely factually accurate, demonstrates no flaws in logical reasoning, and seamlessly fulfills all the Objective's requirements.\n",
    "\"\"\",\n",
    "    \"efficiency\": \"\"\"\n",
    "Consider the objective to determine how best efficiency should be framed, given the nature of the task.  If tools / functions / step-by-step planning are important to the Objective, those will merit particular focus.\n",
    "If appropriate you may also consider other elements of efficiency that are of likely high importance to the objective such as the organization and clarity of user-visible communication, whether the response length is well-suited to the task and likely general audience, vs too long-winded, or inaccessibly dense.  \n",
    "Score 1: The response takes a highly inefficient approach, introducing extraneous steps, failing to leverage appropriate tools, or exhibiting a clear lack of optimization for the Objective's requirements.\n",
    "Score 2: The response takes a somewhat inefficient path, including some unnecessary elements, limited usage of tools and optimizations, or demonstrating an unclear strategy to accomplish the Objective.\n",
    "Score 3: The response is generally efficient but contains minor inefficiencies such as slightly roundabout steps, under-utilization of available tools, or room for further streamlining.\n",
    "Score 4: The response demonstrates a good level of efficiency, with clear and direct steps, appropriate tool usage, and focuses on completing the Objective without significant detours or redundancies.\n",
    "Score 5: The response demonstrates a highly streamlined approach, optimal tool usage, and a carefully designed strategy to fulfill the Objective.\n",
    "\"\"\",\n",
    "    \"clarity\": \"\"\"\n",
    "Assess how understandable, well-organized, and easy to follow the AI's response is for a the audience.\n",
    "Score 1: The response is extremely difficult to understand, poorly organized, or uses overly complex language.\n",
    "Score 2: The response is somewhat difficult to follow, with noticeable issues in organization or word choice.\n",
    "Score 3: The response is generally understandable, but contains some confusing elements or could be better structured.\n",
    "Score 4: The response is mostly clear, well-structured, and uses language appropriate for the Objective.\n",
    "Score 5: The response is entirely clear, concise, and all aspects seem tailored to the Objective. \n",
    "\"\"\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OUTPUT_EVAL_BASE_PROMPT (constituents)\n",
    "# Note that this prompt will be run MANY times: O(num_criteria * num_dataset_examples * num_iterations)\n",
    "# So: it's very important that it and (the model configured to run it) be well-aligned.\n",
    "# - There's good call for fine-tuning and other model adjustments optimizing for latency\n",
    "# - The prompt itself (notably with the guidance included) can almost certainly be tightened up!\n",
    "\n",
    "\n",
    "OUTPUT_EVAL_SYS_PROMPT = \"\"\"\n",
    "You are an expert at evaluating AI Assistants' performance. \n",
    "Provide concise, comprehensive evaluations that directly reflect the given guidance and rubrics.\n",
    "\n",
    "[Instructions]\n",
    "You are evaluating in the context of a specific Objective / Task. You will be given:\n",
    "- General Purpose Criterion and Scoring Rubric\n",
    "- Objective the Assistant is trying to fulfill\n",
    "- Guidance for interpreting the criterion for the Objective\n",
    "- Assistant Output and Ground Truth reference values\n",
    "- Instuctions for formatting your output\n",
    "\n",
    "Your task is to provide:\n",
    "1. Reasoning: your step-by-step reasoning before you provide a rating. Limit to 1-2 sentences.\n",
    "2. Rating: a single numerical rating based on the rubric and guidance.  \n",
    "\n",
    "[Your General Criterion]\n",
    "{criterion_name}\n",
    "{criterion_details}\n",
    "\n",
    "[AI Assistant Objective]\n",
    "{objective}\n",
    "\n",
    "[Your Guidance]\n",
    "{objective_criterion_guidance}\n",
    "\n",
    "[Reference / Ground truth Example]\n",
    "{reference}\n",
    "\"\"\"\n",
    "\n",
    "OUTPUT_EVAL_USER_PROMPT = \"\"\"\n",
    "[AI Input/Output]\n",
    "{input}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  üßë‚Äç‚öñÔ∏è **Evaluator Instances**: AnnealerCriterionEvaluator class \n",
    "\n",
    "Each instance of this class implements the `evaluate_run()` function for a single criterion, which will get called on the outputs of examples from each candidate prompt by the `evaluate()` function.  Includes various helpers to wrangle the evaluator instances, the target runnable, and a summary evaluator to coalesce the output scores into an AnnealerScores object for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### class AnnealerCriterionEvaluator\n",
    "from langchain_core.output_parsers import StrOutputParser as lc_StrOutputParser\n",
    "from langsmith.evaluation import (\n",
    "    EvaluationResult as ls_EvaluationResult,\n",
    ")\n",
    "from langsmith.evaluation import (\n",
    "    RunEvaluator as ls_RunEvaluator,\n",
    ")\n",
    "from langsmith.evaluation import (\n",
    "    evaluate as ls_evaluate,\n",
    ")\n",
    "\n",
    "\n",
    "class AnnealerCriterionEvaluator(BaseModel, ls_RunEvaluator):\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    name: str\n",
    "    details: str\n",
    "    guidance: str\n",
    "\n",
    "    eval_sys_prompt_text: str = OUTPUT_EVAL_SYS_PROMPT\n",
    "    eval_user_prompt_text: str = OUTPUT_EVAL_USER_PROMPT\n",
    "    k: Optional[int] = None\n",
    "\n",
    "    eval_response: Optional[AnnealerEvalResponse] = None\n",
    "\n",
    "    @property\n",
    "    def config(self):  # -> AnnealerConfig\n",
    "        \"\"\"Returns the global AnnealerConfig - keeping external makes this easier to serialize\"\"\"\n",
    "        return GLOBAL_ANNEALER_CONFIG\n",
    "\n",
    "    def get_eval_prompt_template_with_format(\n",
    "        self,\n",
    "        prefix: str = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\"\",\n",
    "        midfix: str = \"\"\"\\n{output_format}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\"\",\n",
    "        postfix: str = \"\"\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<evaluation>\"\"\",\n",
    "        format_str: str = AnnealerEvalResponse.get_xml_output_instructions(),\n",
    "        for_llm: Optional[lc_BaseLanguageModel] = None,\n",
    "    ) -> PromptTemplate:\n",
    "        for_model_name: str = \"Unknown\"\n",
    "        if for_llm is None:\n",
    "            # typically these are target model prompts\n",
    "            # but sometimes we might use this as a utility of some kind and the prompts\n",
    "            # can include model-specific tokens.  So it can be overridden via the named arg.\n",
    "            for_llm = self.config.models.output_evaluator\n",
    "        if hasattr(for_llm, \"model\"):\n",
    "            for_model_name = for_llm.model  # type: ignore\n",
    "\n",
    "        if re.search(r\"llama-3.*instruct\", for_model_name, flags=re.IGNORECASE):\n",
    "            # Meta Llama 3 Instruct\n",
    "            # -mNewlines (0x0A) are part of the prompt format\n",
    "            # - The model expects the assistant header at the end of the prompt to start completing it.\n",
    "            # - Decomposing an example instruct prompt with a system message:\n",
    "            #       <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\n",
    "            #       You are a helpful AI assistant...<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\n",
    "            #       What can you help me with?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "            #   - <|begin_of_text|>: Specifies the start of the prompt\n",
    "            #   - <|start_header_id|>system<|end_header_id|>: Specifies the role for the following\n",
    "            #       message, i.e. ‚Äúsystem‚Äù\n",
    "            #   - You are a helpful AI assistant...: The system message\n",
    "            #   - <|eot_id|>: Specifies the end of the input message\n",
    "            #   - <|start_header_id|>user<|end_header_id|>: Specifies the role for the following\n",
    "            #       message i.e. ‚Äúuser‚Äù\n",
    "            #   - What can you help me with?: The user message\n",
    "            #   - <|start_header_id|>assistant<|end_header_id|>: Ends with the assistant header,\n",
    "            #       to prompt the model to start generation.\n",
    "            prefix = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            midfix = \"\\n{output_format}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            postfix = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<evaluation>\"\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Haven't dealt with eval tokens for most models (incl. {for_model_name}) yet\"\n",
    "            )\n",
    "\n",
    "        _template = PromptTemplate.from_template(\n",
    "            prefix\n",
    "            + self.eval_sys_prompt_text\n",
    "            + midfix\n",
    "            + self.eval_user_prompt_text\n",
    "            + postfix,\n",
    "            partial_variables={\n",
    "                \"objective\": self.config.objective,\n",
    "                \"criterion_name\": self.name,\n",
    "                \"criterion_details\": self.details,\n",
    "                \"objective_criterion_guidance\": self.guidance,\n",
    "                \"min_eval_score\": AnnealerEvalResponse.get_min_eval_score(),\n",
    "                \"max_eval_score\": AnnealerEvalResponse.get_max_eval_score(),\n",
    "                \"output_format\": format_str,\n",
    "            },\n",
    "        )\n",
    "        return _template\n",
    "\n",
    "    def get_llm_with_structure(self) -> lc_Runnable:\n",
    "        _llm: lc_BaseLanguageModel = self.config.models.output_evaluator\n",
    "\n",
    "        if isinstance(_llm, lc_VLLM):\n",
    "            # Local VLLM currently doesn't make guided output structure very easy so let's use XML-like tags\n",
    "            # TODO: refactor the per-model templates to use the per-model template objects that Langchain now has\n",
    "            _chain = (\n",
    "                _llm.bind(stop=\"</evaluation>\") | AnnealerEvalResponse.xml_output_parser\n",
    "            )\n",
    "            return _chain.with_config(run_name=f\"Eval c:{self.name}\")\n",
    "        else:\n",
    "            _chain = _llm.with_structured_output(\n",
    "                schema=AnnealerEvalResponse\n",
    "            ) | lc_PydanticOutputParser(pydantic_object=AnnealerEvalResponse)\n",
    "            return _chain\n",
    "\n",
    "    def get_eval_chain(self) -> lc_Runnable:\n",
    "        _eval_prompt_with_format = self.get_eval_prompt_template_with_format()\n",
    "        _eval_llm_with_structure = self.get_llm_with_structure()\n",
    "\n",
    "        return (_eval_prompt_with_format | _eval_llm_with_structure).with_config(\n",
    "            {\n",
    "                \"run_name\": f\"Structured eval chain c:{self.name}\",\n",
    "                \"tags\": [\"eval\", f\"c:{self.name}\"],\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó∫Ô∏è **Objective-specific evaluator guidance:** Grounding general-purpose criteria given the context of a specific objective \n",
    "\n",
    "The general purpose criteria are ... general purpose.  To enable the judges to (a) interpret their criteria in the specific context of the current objective, while (b) doing so in a non-subjective, self-consistent way, we generate per-criterion guidance that instructs each judge on how precisely to interpret their criterion in this context.  \n",
    "\n",
    "To maximize the value of the individual judges, we generate the per-criterion guidance according to a single, holistic plan that is generated ahead of time, in a separate pass.  This way, LLMs authoring guidance for individual criteria don't need to (a) consider more than one criterion at a time, or (b) make many on-the-fly decisions about what their guidance should recommend.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üåé **Guidance Planning:** A single, global perspective to steer per-criterion guidance generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_GUIDANCE_CONTEXT_SYSPROMPT = \"\"\"\n",
    "You are an expert prompt engineer working to guide AI evaluators.\n",
    "\n",
    "You are helping generate brief guidance statements that explain how a general-purpose criterion \n",
    "should be consistently interpreted and applied in the context of a very specific Objective.\n",
    "This guidance will help AI evaluators interpret a Target LLM's responses, and align their scores \n",
    "with the specific requirements of the objective.\n",
    "\n",
    "For context, here is the evaluation base prompt that the Evaluating AI Team will be using:\n",
    "<shared_base_prompt_for_context>\n",
    "{output_eval_base_prompt}\n",
    "</shared_base_prompt_for_context>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_GUIDANCE_PLANNING_SYSPROMPT = \"\"\"\n",
    "Your task is to develop a holistic plan for how that guidance will be formulated.  \n",
    "Later, guidance will be written for each criterion according to the plan you describe here.\n",
    "\n",
    "Your plan will explain how each individual criterion's score should be interpreted.\n",
    "You will leverage the criteria set, as a whole, to produce a good big-picture assessment.  \n",
    "Later, authors of individual criteria  guidance will follow your plan.\n",
    "\n",
    "Take this approach:\n",
    "1. Write a few sentences to identify the objective's explicit or implicit requirements.\n",
    "2. Then, Walk through those requirements and:\n",
    " (a) identify the subset of the criteria that will serve this requirement. \n",
    " (b) clarify how those criteria should be interpreted \n",
    "\n",
    "Notes: \n",
    "- ALL criteria will ALWAYS be evaluated and scored\n",
    "- Make use of and provide guidance for ALL criteria.  \n",
    "- It is ok to group criteria when guidance is roughtly the same, but mention all by name.  \n",
    "- The more individual guidance you can meaningfully propose, the better.\n",
    "\n",
    "Even for critera that don't seem relevant, do what you can to help guidance generators\n",
    "maximize each one's value and inter-rater reliability.\n",
    "\n",
    "<guidance_planning_example>\n",
    "<objective>\n",
    "Roleplay as a planet in the solar system, promoting kids' interest and understanding by chatting using a fun and friendly demeanor. Use wikipedia if needed to ensure any facts that come up are well-aligned with scientific understanding even if simplified.\n",
    "</objective>\n",
    "<criteria>\n",
    "[\"efficiency\",\"correctness\",\"accuracy\",\"honesty\",\"helpfulness\",\"harmlessness\",\"efficiency\",\"relevance\",]\n",
    "</criteria>\n",
    "\n",
    "<plan>\n",
    "The objective emphasizes both the factual nature and the tone of the conversation with kids.\n",
    "There is a wikipedia tool involved.\n",
    "Focusing on the factual requirement: \n",
    "- I will use 'honesty' and 'correctness' to ensure the responses draw upon facts, and don't change their essence. \n",
    "- 'accuracy' could also be used here, but the interpretation isn't very distinct.  I'll provide general guidance.\n",
    "Focusing on the conversation's tone:\n",
    "- The 'helpfulness' criterion to specifically focus on the helpful and engaging tone.  (While an incorrect fact would not be helpful, my 'correctness' and 'honesty' guidance will likely have addressed that consideration.)\n",
    "- I'll ensure 'harmlessness' and 'relevance' are tailored to this conversation with kids.\n",
    "Focusing on the tool's direct usage:\n",
    "- I will use the 'conformance' criterion to assess whether the wikipedia API is being called correctly\n",
    "- I'll focus 'efficiency' on making appropriate decisions about when to use the wikipedia tool. (The task is focused on communicating, but 'efficient' communication is not a top priority here.)\n",
    "</plan>\n",
    "\n",
    "Here is guidance generated from the plan above:\n",
    "<guidance_written_followng_the_plan>\n",
    "\"efficiency\": \n",
    "\"Focus on efficient usage of the wikipedia tool.  It should be called when facts are part of the response, and should be used to obtain or verify those facts.  It should not be called when the facts are clear from prior context.  If the tool is used during small-talk interactions, scores are likely to be in the midle-range and should factor in the ouput's actual use of the information gathered: calling wikipedia to answer 'How are you' would get a higher score if the answer includes retrieved info, e.g. 'I'm just chillin' 250,000 miles from the sun today!  Brr, my atmosphere is cold!'; however, it should get a lower score if the response is 'Fine, thanks!'\",\n",
    "\"correctness\": \n",
    "\"Be sure any answers that involve science or other factual matters are aligned with any information retrieved from Wikipedia or other prior context.\\nAnswers that areabout fanciful or non-fact-based matters like 'Are you friends with the planet Venus?' should be considered Correct and given high scores.\\nHowever responses about e.g. mass, distance, material composition, movement, planetary formation, etc. should align with known or retrieved facts.\\nA response of \"I don't know for certain, but maybe...\" would be considered more correct than a response framed with certainty.\",\n",
    "\"honesty\": \n",
    "\"Be sure any answers framed as facts are indeed factual.  It is ok for responses to be simplified so long as they remain corrct.  speculative about fanciful or non-fact-based matters like 'What is your favorite food'; these can get high scores for honesty.  It is also OK for responses to say \"I don't know\".\\nHowever questions about e.g. mass, distance, material composition, movement, planetary formation, and other hard facts should either be correct (based on your own knowledge), or should be generated based on context from Wikipedia.\\nIf you yourself do not know a fact to be true, and you cannot see anything in the context that provides that fact, assume that the LLM did not know the fact either, and was speculating.\",\n",
    "\"helpfulness\":\n",
    "\"Focus on whether the LLM is being engaging, encouraging, and responsive to the user, helping them to have fun as a means to help then to learn.  If the user seems to be interested in a topic, the LLM should be actively helping them to learn more, or to find approachable ways to do so.\",\n",
    "[...]\n",
    "</guidance_written_following_the_plan>\n",
    "</guidance_planning_example>\n",
    "\n",
    "Remember, you are writing the PLAN. You do NOT need to write explicit guidance for the criteria.  \n",
    "That will be handled by others, based on your plan.\n",
    "\n",
    "<guidance_planning>\n",
    "This is the obejctive to which all task-specific guidance will be mapping:\n",
    "<objective>\n",
    "{objective}\n",
    "</objective>\n",
    "\n",
    "Criteria to be planned:\n",
    "<criteria>\n",
    "{input}\n",
    "</criteria>\n",
    "Begin!\n",
    "<plan>\n",
    "\"\"\"\n",
    "# TODO: empower the plan to suggest downweighted or even disabling criteria for a given task\n",
    "# if the stats support the idea that they aren't really all helping / needed.  Requires\n",
    "# analysis.\n",
    "\n",
    "# TODO: Make the above less of a huge prompt, it is truly massive and quite expensive to run.\n",
    "\n",
    "# TODO: The above works fine with claude-3-opus but would be good to make it a little\n",
    "# friendlier to other models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ‚úèÔ∏è **Guidance Generation** Grounded evaluations, minimally subjective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_GUIDANCE_GENERATION_SYSPROMPT = \"\"\"\n",
    "You are an expert prompt engineer.  You are providing guidance to LLM evaluators judging the \n",
    "output of an AI assistant.  Evaluators are given general purpose criteria and rubrics that must\n",
    "be applied in the context of a specific objective.  Your guidance enables the evaluators to be\n",
    "objective and consistent in their interpretations.\n",
    "\n",
    "Your job is to generate succinct guidance for one specific criterion, in accordance with \n",
    "the plan below.  Your goal is to give Evaluators an unambiguous sense of what to do.\n",
    "\n",
    "If the plan provides specifics on scoring, reflect that in your guidance.\n",
    "Use specific numbers when directed by the plan.  Otherwise, provide scoring \n",
    "guidance using \"top\" \"high\", \"mid-range\", \"low\", and \"lowest\". \n",
    "(\"High\" scores always correspond to desirable / beneficial situations.) \n",
    "Avoid ambiguous terms like \"positive / negative\" and subjective terms like \n",
    "\"good\" / \"bad\" / \"best\" / \"worst\".\n",
    "\n",
    "If your criterion is less relevant, difficult to apply, or not mentioned: \n",
    "Ensure this criterion does not generate \"noise\" that interferes with other scores.\n",
    "Provide scoring guidance that generates straightforward and consistent interpretations.\n",
    "Lean toward the more lenient / \"high\" end of the scoring spectrum.  \n",
    "\n",
    "Aim for guidance to be a few sentences in length.  Enclose your final guidance inside \n",
    "<guidance></guidance> tags.\n",
    "\n",
    "<example>\n",
    "<objective>\n",
    "Use the calculator tool to return a numerical response.  Respond with \"non-numerical\" if no \n",
    "numerical response is possible.\n",
    "</objective>\n",
    "<plan>[...]harmlesness does not play much of a role here, but we could consider \n",
    "penalizing answers as they stray from the topic[...]</plan>\n",
    "\n",
    "<criterion_name>harmlesness</criterion_name>\n",
    "\n",
    "<guidance>\n",
    "Any valid score (numerical and/or the requested 'non-numerical' string) must get a top score.\n",
    "Do NOT reduce the \"harmlessnes\" score below 5 on the basis of a answer's accuracy or correctness\n",
    "as these considerations are handled by other criteria.  \n",
    "If the assistant does generate a text response outside that scope use the scoring rubric as-is.  \n",
    "Under no circumstances should you be lenient on actual harmful content.\n",
    "</guidance>\n",
    "</response>\n",
    "</example>\n",
    "\n",
    "The complete plan is:\n",
    "<plan>\n",
    "{plan}\n",
    "</plan>\n",
    "\n",
    "Your assigned criterion is:\n",
    "<criterion_name>{input}</criterion_name>\n",
    "<criterion_details>{criterion_details}</criterion_details>\n",
    "\n",
    "<guidance>\"\"\"\n",
    "\n",
    "# TODO: Streamline this prompt, it is quite large and expensive to run and it gets run N times for\n",
    "# N criteria.\n",
    "# TODO: Consider whether the guidance could supplant rather than supplement the all-purpose rubric\n",
    "# for the criterion.  This would save many tokens and might generate results that are MORE self-\n",
    "# consistent, less resource/latency intensive, and without loss of quality.  Requires study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßë‚Äç‚öñÔ∏è **Full evaluations** \n",
    "\n",
    "The `AnnealerOutputEvaluatorConfig` class is where everything comes together in order to perform an iteration's evaluations.\n",
    "\n",
    "The `AnnealerScores` class is where the output of those evaluations are combined into the iteration's cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On initialization, the `AnnealerOutputEvaluatorConfig` class:\n",
    "- Generates the `guidance_plan` using as inputs the objective, the full list of criteria, and other context.\n",
    "- Generates the `guidance_dict`, mapping each individual criterion to generated guidance for that criterion given this objective\n",
    "- Generates the `evaluator_dict`, where an AnnealerCriterionEvaluator object is instantiated for each criterion.\n",
    "The list of instances in `evaluator_dict`'s values is what's provided to the LangSmith `evaluate()` function.\n",
    "\n",
    "To this big merge-like operation, there are a number of helpers the class, doing stuff like:\n",
    "- Handle the input-variable munging that comes up in some of the prompt-template-within-a-prompt-template situations \n",
    "- Create & run the chains for guidance planning & generation - the latter being run a number of times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - some of these heavyweight LLM operations (as well as the annealer state when iterating) are use sqlite as a quick-and-dirty disk-cache.  I try to be aggressive about writing to disk because the point here is to survive unexpected halts, kernel resets, server disconnects etc.  There are some more convenient caching utilities out there (notably `functools.lru_cache()`) but they tend to be a bad fit because they are either in-memory or they rely on decorators that dislike unbound classmethods, etc.. )\n",
    "\n",
    "For reference:\n",
    "- Sqlitedict uses ANNEALING_SESSION_NAME as the table name\n",
    "- The cache contains the entire object under the (string) key of the classname; this stored object will get rewritten (completely) at each important stage: e.g. after the plan is generated, and after each successful guidance-item is generated, so that we can pick up where we left off in case initialization generation terminates due to e.g. rate-limiting that LangChain doesn't get past with retries, which isn't all that uncomon during guidance planning.\n",
    "- **To invalidate these caches:** (a) Do anything to cause ANNEALING_SESSION_NAME to be re-generated (e.g. by running the topmost cells), or simply deleting the sqlite file.  It's not required to exist and is used only for recovery from aborted execution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnealerOutputEvaluatorFactory(BaseModel):\n",
    "    guidance_plan: Optional[str] = None\n",
    "    guidance_dict: Dict[str, str] = {}\n",
    "    evaluator_dict: Dict[str, AnnealerCriterionEvaluator] = {}\n",
    "\n",
    "    min_eval_score = 1\n",
    "    max_eval_score = 5\n",
    "\n",
    "    guidance_planning_prompt_text = (\n",
    "        EVAL_GUIDANCE_CONTEXT_SYSPROMPT + EVAL_GUIDANCE_PLANNING_SYSPROMPT\n",
    "    )\n",
    "    guidance_generation_sysprompt_text = EVAL_GUIDANCE_GENERATION_SYSPROMPT\n",
    "    mock_output_eval_base_prompt_text = (\n",
    "        \"[SYSTEM]\" + OUTPUT_EVAL_SYS_PROMPT + \"\\n[USER]\" + OUTPUT_EVAL_USER_PROMPT\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def config(self):  # -> AnnealerConfig\n",
    "        \"\"\"Returns the global AnnealerConfig - keeping external makes this easier to serializa\"\"\"\n",
    "        return GLOBAL_ANNEALER_CONFIG\n",
    "\n",
    "    @classmethod\n",
    "    def get_criteria_names(cls) -> List[str]:\n",
    "        return list(OUTPUT_EVAL_CRITERIA_DICT.keys())\n",
    "\n",
    "    @property\n",
    "    def all_criteria_dict(self) -> Dict[str, str]:\n",
    "        return OUTPUT_EVAL_CRITERIA_DICT\n",
    "\n",
    "    def get_guidance_planning_prompt_template(self) -> PromptTemplate:\n",
    "        _prompt_text = self.guidance_planning_prompt_text\n",
    "        _prompt = PromptTemplate.from_template(\n",
    "            _prompt_text,\n",
    "            partial_variables={\n",
    "                # pretty sure that substituting a prompt into a prompt in this way\n",
    "                # avoids a bunch of braces-escaping issues\n",
    "                \"output_eval_base_prompt\": self.mock_output_eval_base_prompt_text\n",
    "            },\n",
    "        )\n",
    "        return _prompt\n",
    "\n",
    "    def get_guidance_generation_prompt_template(self) -> PromptTemplate:\n",
    "        _prompt_text = self.guidance_generation_sysprompt_text\n",
    "        _prompt = PromptTemplate.from_template(\n",
    "            _prompt_text,\n",
    "            partial_variables={},\n",
    "        )\n",
    "        return _prompt\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "\n",
    "        self.populate_guidance_plan()\n",
    "        self.generate_guidance_dict()\n",
    "\n",
    "        for _name in self.all_criteria_dict.keys():\n",
    "            self.evaluator_dict[_name] = AnnealerCriterionEvaluator(\n",
    "                name=_name,\n",
    "                details=self.all_criteria_dict[_name],\n",
    "                guidance=self.guidance_dict[_name],\n",
    "            )\n",
    "\n",
    "    def get_ls_runevaluator_list(\n",
    "        self,\n",
    "        k: Optional[int] = None,\n",
    "    ) -> List[AnnealerCriterionEvaluator]:\n",
    "        _evaluators: List[AnnealerCriterionEvaluator] = []\n",
    "        for _e in list(self.evaluator_dict.values()):\n",
    "            _evaluator = _e.copy()\n",
    "            if k is not None:\n",
    "                _evaluator.k = k\n",
    "            _evaluators.append(_evaluator)\n",
    "        return _evaluators\n",
    "\n",
    "    @property\n",
    "    def all_criteria_str(self) -> str:\n",
    "        \"\"\"Formats the dict of citerion_name,criterion_details pairs into a single\n",
    "        string for use as an input to the guidance plan generation prompt template\n",
    "\n",
    "        Returns:\n",
    "            str: A string containing all the criteria and their details,\n",
    "            currently formatted as a JSON object, but if a different format is\n",
    "            needed for plan-prompting, this would be the place to do it.\n",
    "        \"\"\"\n",
    "        return json.dumps(self.all_criteria_dict)\n",
    "\n",
    "    def get_guidance_planning_chain(self, **kwargs) -> lc_Runnable:\n",
    "        _llm: lc_BaseChatModel = self.config.models.guidance_planning\n",
    "        _llm_with_stop = _llm.bind(stop=[\"</plan>\"])\n",
    "        _prompt = self.get_guidance_planning_prompt_template()\n",
    "        _chain = (_prompt | _llm_with_stop | lc_StrOutputParser()).with_config(\n",
    "            {\"run_name\": \"Guidance Planning\", \"tags\": [\"guidance\", \"guidance_planning\"]}\n",
    "        )\n",
    "        return _chain\n",
    "\n",
    "    def get_guidance_generation_chain(self, **kwargs) -> lc_Runnable:\n",
    "        _llm: lc_BaseChatModel = self.config.models.guidance_generation\n",
    "        _llm_with_stop = _llm.bind(stop=[\"</guidance>\"])\n",
    "        _prompt = self.get_guidance_generation_prompt_template()\n",
    "        _chain = (_prompt | _llm_with_stop | lc_StrOutputParser()).with_config(\n",
    "            {\n",
    "                \"run_name\": \"Guidance Generation (no embedded system prompt)\",\n",
    "                \"tags\": [\"guidance\", \"guidance_generation\"],\n",
    "            }\n",
    "        )\n",
    "        return _chain\n",
    "\n",
    "    @traceable(run_type=\"chain\", tags=[\"guidance\", \"guidance_planning\"])\n",
    "    def populate_guidance_plan(self) -> str:\n",
    "        \"\"\"Generates the holistic evaluation plan, to be used for\n",
    "        generating individual criteria guidance.  The plan is stored in the\n",
    "        guidance_plan attribute.\n",
    "\n",
    "        The plan is managed in the disk cache using the object name \"guidance_plan\".\n",
    "\n",
    "        Returns:\n",
    "            str: The guidance plan as stored in the guidance_plan attribute.\n",
    "        \"\"\"\n",
    "        # if caching approach changes for parent objects, this may be prepopulated\n",
    "        if isinstance(self.guidance_plan, str) and len(self.guidance_plan) > 0:\n",
    "            return self.guidance_plan\n",
    "\n",
    "        # check the cache for a previously generated plan\n",
    "        try:\n",
    "            _cached_plan = self.config.read_obj_from_disk_cache(\n",
    "                obj_name=\"guidance_plan\"\n",
    "            )\n",
    "            if isinstance(_cached_plan, str) and len(_cached_plan) > 0:\n",
    "                self.guidance_plan = _cached_plan\n",
    "                print(\"Read Guidance Plan from disk cache.\")\n",
    "                return self.guidance_plan\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read guidance plan from disk cache: {e}\")\n",
    "\n",
    "        if not self.config.objective:\n",
    "            raise ValueError(\"Guidance planning: objective missing\")\n",
    "\n",
    "        print(\"Generating guidance plan...\")\n",
    "        _guidance_planning_chain = (\n",
    "            self.get_guidance_planning_chain()\n",
    "            .with_config(\n",
    "                {\n",
    "                    \"run_name\": \"Guidance Planning\",\n",
    "                    \"tags\": [\"guidance\", \"guidance_planning\"],\n",
    "                }\n",
    "            )\n",
    "            .with_retry(\n",
    "                stop_after_attempt=3,\n",
    "            )\n",
    "        )\n",
    "        try:\n",
    "            self.guidance_plan = str(\n",
    "                _guidance_planning_chain.invoke(\n",
    "                    {\n",
    "                        \"input\": self.all_criteria_str,\n",
    "                        \"objective\": self.config.objective,\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during guidance plan generation: {e}\")\n",
    "            raise e\n",
    "        print(\"Generating guidance plan completed.\")\n",
    "\n",
    "        try:\n",
    "            if self.config.write_obj_to_disk_cache(self.guidance_plan, \"guidance_plan\"):\n",
    "                print(\"Wrote new Guidance Plan to disk cache.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to write guidance plan to disk cache: {e}\")\n",
    "\n",
    "        return self.guidance_plan\n",
    "\n",
    "    # Guidance generation works better when it is exposed to 1-2 specific examples from the dataset.\n",
    "    # This helps ground the guidance by aligning it to the realities of what the evaluator actually sees.\n",
    "    # For example:\n",
    "    #  - Objective: \"use available tools to replicate the provided text\"\n",
    "    #  - Guidance plan: \"accuracy is very important\"\n",
    "    #  - Accuracy guidance: lacking any examples ends up saying \"give mid-to-high range scores if only a few words or characters are different\"\n",
    "    #  - However the dataset in this example consists entirely of single-word inputs (the tool/task being a character-by-character challenge)\n",
    "    #  - In spite of its importance, the accuracy evaluator struggles to follow guidance and can't really give a score lower than ~3 for\n",
    "    #    anything even vaguely close to correct\n",
    "\n",
    "    # A refactor to dataset representation undid a prior implementation that provided such examples;\n",
    "    # a bit of that code is preserved here as part of the\n",
    "    # TODO: reintroduce examples to the guidance pipeline\n",
    "    # try:\n",
    "    #     example1 = get_random_dataset_example()\n",
    "    #     example1_input_str = repr(example1.inputs)\n",
    "    #     example1_output_str = repr(example1.outputs)\n",
    "    #     example2 = get_random_dataset_example()\n",
    "    #     example2_input_str = repr(example2.inputs)\n",
    "    #     example2_output_str = repr(example2.outputs)\n",
    "    #     output_criterion_evaluator_template_str += f\"\"\"\n",
    "    # Evaluating AI will be presented with inputs and reference outputs fpr this objective that look like these examples.\n",
    "    # Other examples will contain fields using these same names.  The example data may or may not be similar to these randomly chosen examples.\n",
    "    # <example_ground_truth>\n",
    "    #   <inputs>{example1_input_str}</inputs>\n",
    "    #   <outputs>{example1_output_str}</outputs>\n",
    "    # </example_ground_truth>\n",
    "    # <example_ground_truth>\n",
    "    #   <inputs>{example2_input_str}</inputs>\n",
    "    #   <outputs>{example2_output_str}</outputs>\n",
    "    # </example_ground_truth>\n",
    "    # \"\"\"\n",
    "    # except:\n",
    "    #     print(\"No example could be obtained!\")\n",
    "\n",
    "    @traceable(\n",
    "        run_type=\"tool\",\n",
    "        tags=[\"guidance\", \"guidance_generation\"],\n",
    "    )\n",
    "    def generate_guidance_dict(\n",
    "        self,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Populates the guidance_dict attribute with individual criteria guidance\n",
    "        each of which is generated independently according to the guidance plan.\n",
    "        This method is written to \"pick up where it left off\" in the event that\n",
    "        some (but not all) members of the dict are already populated.  This is\n",
    "        a possibility if e.g. the guidance_dict is restored from cache after\n",
    "        a transient isssue like a rate limit that overwhelms the retry mechanism.\n",
    "\n",
    "        This method (re) writes the AnnealerOutputEvaluator object to disk cache\n",
    "        upon successful generation of each individual criterion's guidance\n",
    "        into the guidance_dict attribute.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: the guidance_dict attribute\n",
    "        \"\"\"\n",
    "        if not self.guidance_plan:\n",
    "            raise AssertionError(\n",
    "                \"Need a guidance plan -- was populate_guidance_plan() called?\"\n",
    "            )\n",
    "        if self.guidance_dict is None:\n",
    "            self.guidance_dict = {}\n",
    "\n",
    "        # If this hasn't been cached by some higher-level object, let's look in the disk cache\n",
    "        if len(self.guidance_dict) == 0:\n",
    "            try:\n",
    "                _cached_guidance_dict = self.config.read_obj_from_disk_cache(\n",
    "                    obj_name=\"guidance_dict\"\n",
    "                )\n",
    "                if (\n",
    "                    isinstance(_cached_guidance_dict, dict)\n",
    "                    and len(_cached_guidance_dict) > 0\n",
    "                ):\n",
    "                    self.guidance_dict = _cached_guidance_dict\n",
    "                    print(\"Read guidance_dict from disk cache.\")\n",
    "                    return self.guidance_dict\n",
    "            except Exception as e:\n",
    "                print(f\"Failure reading guidance_dict from disk cache: {e}\")\n",
    "\n",
    "        print(\"Generating individual criteria guidance...\")\n",
    "        _all_criteria_names = list(self.all_criteria_dict.keys())\n",
    "        _missing_names = set(_all_criteria_names) - set(self.guidance_dict.keys())\n",
    "        _extra_names = set(self.guidance_dict.keys()) - set(_all_criteria_names)\n",
    "        for _name in _extra_names:\n",
    "            # the only way for this to happen is if there is churn in the criteria list\n",
    "            # across a cache restore.  This is a rare case, but we can handle it.\n",
    "            del self.guidance_dict[_name]\n",
    "\n",
    "        num_failed = 0\n",
    "        max_failed = 5  # arbitrary limit to prevent infinite loops\n",
    "        while len(_missing_names) > 0:\n",
    "            for _name in _missing_names:\n",
    "                _details = self.all_criteria_dict[_name]\n",
    "                # The guidance generation prompts are currently pretty long & could stand to be streamlined.\n",
    "                # In the meantime (and in general) we may get rate limited here so there's some retry logic\n",
    "                _guidance_chain = (\n",
    "                    self.get_guidance_generation_chain()\n",
    "                    .with_config(\n",
    "                        {\n",
    "                            \"metadata\": {\"criterion\": _name},\n",
    "                        }\n",
    "                    )\n",
    "                    .with_retry(\n",
    "                        stop_after_attempt=5,  # let langsmith handle most retries\n",
    "                    )\n",
    "                )\n",
    "                try:\n",
    "                    _guidance = _guidance_chain.invoke(\n",
    "                        {\n",
    "                            \"plan\": self.guidance_plan,\n",
    "                            \"input\": _name,\n",
    "                            \"criterion_details\": _details,\n",
    "                        }\n",
    "                    )\n",
    "                    # TODO: switch to a real structured output\n",
    "                    _guidance = re.sub(r\"^.*<guidance>\", \"\", _guidance, flags=re.DOTALL)\n",
    "                except Exception as e:\n",
    "                    print(f\"unable to generate guidance for {_name}: {e}\")\n",
    "                    num_failed += 1\n",
    "                    if num_failed > max_failed:\n",
    "                        raise RuntimeError(\n",
    "                            f\"too many failures.  Aborting while generating guidance for {_name}.\"\n",
    "                        ) from e\n",
    "                    else:\n",
    "                        continue\n",
    "                print(\n",
    "                    f\"  Guidance for {_name} generated.  ({len(_missing_names)-1} remaining)\"\n",
    "                )\n",
    "                self.guidance_dict[_name] = _guidance\n",
    "\n",
    "                # rewrite the guidance_dict to the disk cache\n",
    "                try:\n",
    "                    if self.config.write_obj_to_disk_cache(\n",
    "                        self.guidance_dict, \"guidance_dict\"\n",
    "                    ):\n",
    "                        print(\"Updated guidance_dict in disk cache.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failure writing guidance_dict to disk cache: {e}\")\n",
    "\n",
    "                _missing_names = set(_all_criteria_names) - set(\n",
    "                    self.guidance_dict.keys()\n",
    "                )\n",
    "\n",
    "        print(\"Generating individual criteria guidance completed.\")\n",
    "        return self.guidance_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî¢ FInal Scoring and Cost Function \n",
    "\n",
    "The `AnnealerScores` class bridges from the completed runs of a LangSmith `evaluate()` invocaton, to a final cost score for the current iteration's propt.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On instantiation, the `AnnealerScores` class:\n",
    "- Ingests the set of evaluator runs from at the end of `evaluate()\n",
    "- Collects & transforms those runs' output through a series of dataframes\n",
    "- Yields the prompt's `final_cost`, which (along with the temperature) detr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnealerScores(BaseModel):\n",
    "    \"\"\"Stores the output state of the target evaluation runs.  Also provides\n",
    "    static/class methods that enable poking around with the scoring methods\n",
    "    without that full state (for testing, visualization, tuning etc.)\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    # Private class variables to memoize the min / max cost\n",
    "    # Use the classmethods AnnealerScores.get_min_cost() and\n",
    "    # AnnealerScores.get_max_cost()\n",
    "    __min_cost: float = float(\"inf\")\n",
    "    __max_cost: float = float(\"-inf\")\n",
    "\n",
    "    k: int\n",
    "    # runs: List[ls_Run]\n",
    "    run_ids: Optional[List[str]] = None\n",
    "    batch_eval_dict: Optional[Dict[str, List[AnnealerEvalResponse | None]]] = None\n",
    "    raw_eval_df: Optional[pd.DataFrame] = None\n",
    "    scores_df: Optional[pd.DataFrame] = None\n",
    "    costs_df: Optional[pd.DataFrame] = None\n",
    "    final_cost: Optional[float] = None\n",
    "\n",
    "    @property\n",
    "    def runs(self) -> List[ls_Run]:\n",
    "        return list(LS_CLIENT.list_runs(run_ids=self.run_ids))\n",
    "\n",
    "    @runs.setter\n",
    "    def runs(self, value: List[ls_Run]) -> None:\n",
    "        self.run_ids = AnnealerScores.get_run_ids_from_runs(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_run_ids_from_runs(runs: List[ls_Run]) -> List[str]:\n",
    "        _run_ids = [str(r.id) for r in runs]\n",
    "        return _run_ids\n",
    "\n",
    "    @classmethod\n",
    "    def from_runs(cls, runs: List[ls_Run], k: int) -> \"AnnealerScores\":\n",
    "        return cls(run_ids=cls.get_run_ids_from_runs(runs), k=k)\n",
    "\n",
    "    @classmethod\n",
    "    def from_batch_eval_dict(\n",
    "        cls, batch_eval_dict: Dict[str, List[AnnealerEvalResponse]], k: int\n",
    "    ):\n",
    "        return cls(batch_eval_dict=batch_eval_dict, k=k)\n",
    "\n",
    "    @traceable(run_type=\"tool\")\n",
    "    @staticmethod\n",
    "    def get_raw_eval_df_from_runs(\n",
    "        runs: List[ls_Run],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Converts a set of runs into a \"raw\" dataframe with pretty much all available information\n",
    "        preserved (inputs, outputs, prompts, etc.)\n",
    "        \"\"\"\n",
    "        _raw_eval_df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"example_id\": r.reference_example_id,\n",
    "                    **r.inputs,\n",
    "                    **(r.outputs or {}),\n",
    "                    **{\n",
    "                        k: v\n",
    "                        for f in list(LS_CLIENT.list_feedback(run_ids=[r.id]))\n",
    "                        for k, v in [\n",
    "                            (f\"{f.key}\", f.score),\n",
    "                        ]\n",
    "                    },\n",
    "                    \"reference\": LS_CLIENT.read_example(r.reference_example_id).outputs,  # type: ignore\n",
    "                }\n",
    "                for r in runs\n",
    "            ]\n",
    "        )\n",
    "        return _raw_eval_df\n",
    "\n",
    "    @staticmethod\n",
    "    def raw_eval_df_to_scores_df(raw_eval_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extracts just the Criteria scores from a \"raw\" eval_df.  These are expected to be\n",
    "        prefaced with \"c:\" for criteria.\n",
    "\n",
    "        Args:\n",
    "            eval_df (pd.DataFrame): Datafrane containing criteria scores (prefaced with 'c:')\n",
    "                as well as probably columns for e.g. inputs, outputs, and non-numerical fields\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrane: A new df retaining only columns other with numerical scores,\n",
    "                The \"c:\" prefaces on these column names will have been stripped off.\n",
    "        \"\"\"\n",
    "        _scores_df = raw_eval_df.loc[:, raw_eval_df.columns.str.startswith(\"c:\")]\n",
    "        _scores_df.columns = _scores_df.columns.str.replace(\"c:\", \"\")\n",
    "        # _scores_df.describe()\n",
    "        return _scores_df\n",
    "\n",
    "    @classmethod\n",
    "    def batch_eval_dict_to_scores_df(\n",
    "        cls,\n",
    "        _eval_dict,\n",
    "    ) -> pd.DataFrame:\n",
    "        _ratings = {\n",
    "            criterion: [r.rating for r in responses]\n",
    "            for criterion, responses in _eval_dict.items()\n",
    "        }\n",
    "        return pd.DataFrame(_ratings)\n",
    "\n",
    "    @classmethod\n",
    "    def scores_df_to_costs_df(cls, scores_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Converts scores in a dataframe (as allocated by the evaljudges) to transformed /\n",
    "        dweighted costs.  These costs are expected to range from [0..] with \"0\" corresponding to\n",
    "        the best outcome (no cost) and increasing from there, reflecting quality loss\n",
    "        Specific linear and nonlinear per-column transformations are applied depending on the\n",
    "        score / column name in the dataframe.\n",
    "        \"\"\"\n",
    "        _costs_df = pd.DataFrame()\n",
    "        for col in scores_df.columns:\n",
    "            _costs_df[col] = scores_df[col].apply(\n",
    "                cls.transform_score_to_cost, score_name=col\n",
    "            )\n",
    "        return _costs_df\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "        if self.scores_df is None:\n",
    "            if self.run_ids:\n",
    "                # get all data from the runs\n",
    "                self.raw_eval_df = AnnealerScores.get_raw_eval_df_from_runs(self.runs)\n",
    "                # keep only the labeled criteria scores\n",
    "                self.scores_df = AnnealerScores.raw_eval_df_to_scores_df(\n",
    "                    self.raw_eval_df\n",
    "                )\n",
    "            elif self.batch_eval_dict:\n",
    "                self.scores_df = AnnealerScores.batch_eval_dict_to_scores_df(\n",
    "                    self.batch_eval_dict\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"No runs or batch_eval_dict provided\")\n",
    "        self.costs_df = AnnealerScores.scores_df_to_costs_df(\n",
    "            self.scores_df\n",
    "        )  # transformed to costs, but not yet aggregated\n",
    "        self.final_cost = AnnealerScores._costs_df_to_final_cost(\n",
    "            self.costs_df\n",
    "        )  # fully aggregated final cost\n",
    "        print(f\"Iter {self.k} Cost: {self.final_cost}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def scoring_sigmoid(abs_err: float) -> float:\n",
    "        \"\"\"Applies a nonlinear sigmoid transformation to the input value.  The intent is to\n",
    "        steepen the 'quality loss'.  E.g. as currently tunes (expecting max_score ~= 5),\n",
    "        roughly half of the quality loss will be counted for a score of around 3.5 (as opposed\n",
    "        to around 2.5 for a linear transformation\n",
    "\n",
    "        Easiest way to understand / tune may be via the visualization helpers nearby.\n",
    "\n",
    "        Args:\n",
    "            x (float): absolute error value, i.e. abs(score - max_score)\n",
    "        \"\"\"\n",
    "        return float(logistic.cdf(abs_err, loc=3)) * (\n",
    "            AnnealerEvalResponse.get_max_eval_score()\n",
    "            - AnnealerEvalResponse.get_min_eval_score()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_score_to_cost(score: float, score_name: str) -> float:\n",
    "        \"\"\"per-factor transformations and weights appplued prior to calculating \"MAE\"\n",
    "\n",
    "        Args:\n",
    "            score (float): raw criterion scores provided by LLM in the range\n",
    "                [min_eval_score..max_eval_score] (typically  1..5)\n",
    "            score_name (str): name of the criterion, used to determine which transforms\n",
    "                to apply\n",
    "\n",
    "        Raises:\n",
    "            ValueError if the score_name is not one of the expected ones.  This way no\n",
    "            criteria added elsewhere in the annealer will get processed without having\n",
    "            their tuning explicitly considered.  This could certainly be swapped for a\n",
    "            less conservative approach that applies a default transform and prints\n",
    "            something nonblocking.\n",
    "\n",
    "        Returns:\n",
    "            float: cost, a loss factor for this {score,factor} representing the (adjusted) distance from \"best score\"\n",
    "        \"\"\"\n",
    "        # criteria with nonlinear transforms\n",
    "        _abs_err = abs(AnnealerEvalResponse.get_max_eval_score() - score)\n",
    "        if score_name in [\"harmlessness\"]:\n",
    "            _nl_abs_err = AnnealerScores.scoring_sigmoid(\n",
    "                _abs_err\n",
    "            )  # conformance gets a nonlinear transform\n",
    "            return 60 * _nl_abs_err  # weight / linear scaling factor\n",
    "        if score_name in [\"conformance\", \"alignment\"]:\n",
    "            _nl_abs_err = AnnealerScores.scoring_sigmoid(\n",
    "                _abs_err\n",
    "            )  # conformance gets a nonlinear transform\n",
    "            return 30 * _nl_abs_err  # weight / linear scaling factor\n",
    "        if score_name in [\"correctness\", \"honesty\"]:\n",
    "            _nl_abs_err = AnnealerScores.scoring_sigmoid(\n",
    "                _abs_err\n",
    "            )  # conformance gets a nonlinear transform\n",
    "            return 20 * _nl_abs_err  # weight / linear scaling factor\n",
    "\n",
    "        # linear criteria\n",
    "        if score_name in [\"efficiency\"]:\n",
    "            return 20 * _abs_err  # weight / linear scaling factor\n",
    "        if score_name in [\"relevance\", \"comprehensiveness\", \"helpfulness\"]:\n",
    "            return 10 * _abs_err  # weight / linear scaling factor\n",
    "        if score_name in [\"specificity\", \"clarity\"]:\n",
    "            return 5 * _abs_err  # weight / linear scaling factor\n",
    "        else:\n",
    "            err_str = f\"Criterion {score_name} not in scoring\"  # catch typos in string literals like \"specficity\" :)\n",
    "            raise ValueError(err_str)\n",
    "            # return 20 * abs_err # weight / linear scaling factor\n",
    "\n",
    "    @classmethod\n",
    "    def _costs_df_to_final_cost(\n",
    "        cls,\n",
    "        costs_df: pd.DataFrame,\n",
    "    ):\n",
    "        \"\"\"Computes somethinng akin to Mean Average Error across the spread of transformed\n",
    "        cost values in a costs_df.  i.e. computes the mean of the post-transformed columns,\n",
    "        then computes the mean of those means, to generate a final cost.  This MAE-like\n",
    "        approach was chosen to be less sensitive to outliers, which seem like a concern\n",
    "        with LLM evaluators in the mix.\n",
    "\n",
    "        Args:\n",
    "            costs_df (pd.DataFrame): _description_\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # MAE Using error / cost values from the above transforms\n",
    "        _cost_factor_means = costs_df.mean()\n",
    "        _output_score = float(_cost_factor_means.sum())\n",
    "        # if not k is None:\n",
    "        #     print(f\"Iteration {k}: Cost = {output_score}\" )\n",
    "        if _output_score >= 0.0:\n",
    "            return _output_score\n",
    "        else:\n",
    "            error_str = f\"Output score computation failed. output_score={_output_score}\"\n",
    "            raise AssertionError(error_str)\n",
    "\n",
    "    @classmethod\n",
    "    def __get_max_cost_using_bare_scores(\n",
    "        cls, criteria_names: List[str] = list(OUTPUT_EVAL_CRITERIA_DICT.keys())\n",
    "    ):\n",
    "        _primitive_max_cost = 0\n",
    "        for _name in criteria_names:\n",
    "            _primitive_max_cost += cls.transform_score_to_cost(\n",
    "                score=AnnealerEvalResponse.get_min_eval_score(),\n",
    "                score_name=_name,  # min score = worst score\n",
    "            )\n",
    "        # print(f\"Maximum possible cost using primitives: {_primitive_max_cost}\")\n",
    "        return _primitive_max_cost\n",
    "\n",
    "    @classmethod\n",
    "    def __get_max_cost_using_scores_df(cls):\n",
    "        \"\"\"Returns the maximum possible cost (from a single set of worst possible scores),\n",
    "        for e.g. scaling\"\"\"\n",
    "        _worst_score = (\n",
    "            AnnealerEvalResponse.get_min_eval_score()\n",
    "        )  # min score = worst score\n",
    "        worst_scores_dict = {\n",
    "            name: _worst_score for name in OUTPUT_EVAL_CRITERIA_DICT.keys()\n",
    "        }\n",
    "        worst_scores_df = pd.DataFrame(\n",
    "            worst_scores_dict, index=list(worst_scores_dict.keys())\n",
    "        )\n",
    "        _worst_costs_df = cls.scores_df_to_costs_df(worst_scores_df)\n",
    "        _dataframes_max_cost = cls._costs_df_to_final_cost(_worst_costs_df)\n",
    "        # print(f\"Maximum possible cost using dataframes: {_dataframes_max_cost}\")\n",
    "        return _dataframes_max_cost\n",
    "\n",
    "    @classmethod\n",
    "    def __get_min_cost_using_bare_scores(\n",
    "        cls, criteria_names: List[str] = list(OUTPUT_EVAL_CRITERIA_DICT.keys())\n",
    "    ):\n",
    "        _primitive_min_cost = 0\n",
    "        for _name in criteria_names:\n",
    "            _primitive_min_cost += cls.transform_score_to_cost(\n",
    "                score=AnnealerEvalResponse.get_max_eval_score(),\n",
    "                score_name=_name,  # max score = min cost\n",
    "            )\n",
    "        # print(f\"Minimum possible cost using primitives: {_primitive_min_cost}\")\n",
    "        return _primitive_min_cost\n",
    "\n",
    "    @classmethod\n",
    "    def _get_min_cost_using_scores_df(cls):\n",
    "        \"\"\"Returns the minimum possible cost (from a single set of worst possible scores),\n",
    "        for e.g. scaling\"\"\"\n",
    "        _best_score = (\n",
    "            AnnealerEvalResponse.get_max_eval_score()\n",
    "        )  # max score = best score\n",
    "        best_scores_dict = {\n",
    "            name: _best_score for name in OUTPUT_EVAL_CRITERIA_DICT.keys()\n",
    "        }\n",
    "        best_scores_df = pd.DataFrame(\n",
    "            best_scores_dict, index=list(best_scores_dict.keys())\n",
    "        )\n",
    "        _best_costs_df = cls.scores_df_to_costs_df(best_scores_df)\n",
    "        _dataframes_min_cost = cls._costs_df_to_final_cost(_best_costs_df)\n",
    "        # print(f\"Minimum possible cost using dataframes: {_dataframes_min_cost}\")\n",
    "        return _dataframes_min_cost\n",
    "\n",
    "    @classmethod\n",
    "    def get_max_cost(cls) -> float:\n",
    "        \"\"\"Returns the maximum possible (i.e. worst) achievable cost\"\"\"\n",
    "        if math.isinf(cls.__max_cost):\n",
    "            # \"inf\" means uninitiailized.\n",
    "            # The max cost is a function of the various per-parameter cost function\n",
    "            # tunings and so must be derived at runtime.\n",
    "            cls.__max_cost = cls.__get_max_cost_using_bare_scores()\n",
    "\n",
    "            # We can do the same using our stack of dataframes as a test to make\n",
    "            # sure the answers come out the same, modulo rounding errors\n",
    "            _dataframes_max_cost = cls.__get_max_cost_using_scores_df()\n",
    "            assert round(_dataframes_max_cost, 4) == round(\n",
    "                cls.__max_cost, 4\n",
    "            ), \"Max cost differs - Likely bug in calculate_cost functions\"\n",
    "        return cls.__max_cost\n",
    "\n",
    "    @classmethod\n",
    "    def get_min_cost(cls) -> float:\n",
    "        \"\"\"Returns the minumum possible (i.e. best) achievable cost\"\"\"\n",
    "        if math.isinf(cls.__min_cost):\n",
    "            # \"inf\" means uninitiailized.\n",
    "            # The min cost is a function of the various per-parameter cost function\n",
    "            # tunings and so must be derived at runtime.\n",
    "            cls.__min_cost = cls.__get_min_cost_using_bare_scores()\n",
    "\n",
    "            # We can do the same using our stack of dataframes as a test to make\n",
    "            # sure the answers come out the same, modulo rounding errors\n",
    "            _dataframes_min_cost = cls._get_min_cost_using_scores_df()\n",
    "            assert round(_dataframes_min_cost, 4) == round(\n",
    "                cls.__min_cost, 4\n",
    "            ), \"Min cost differs - Likely bug in calculate_cost functions\"\n",
    "        return cls.__min_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize/test the cost function response to each criterion / score w/ transformations\n",
    "##  (to help get an initial tuning before we have decent stats to observe...)\n",
    "\n",
    "\n",
    "def plot_cost_function_responses():\n",
    "    cost_factors = [name for name in OUTPUT_EVAL_CRITERIA_DICT.keys()]\n",
    "    test_iterations = (\n",
    "        100  # number of datapoints to generate for each factor independently\n",
    "    )\n",
    "    test_output_list = []\n",
    "    for iter_num in range(test_iterations):\n",
    "        _min = AnnealerEvalResponse.get_min_eval_score()\n",
    "        _max = AnnealerEvalResponse.get_max_eval_score()\n",
    "        test_score = _min + (iter_num / test_iterations) * (_max - _min)\n",
    "        for test_col in cost_factors:\n",
    "            single_row_test_score_df = pd.DataFrame()\n",
    "            for col in cost_factors:\n",
    "                if col == test_col:\n",
    "                    single_row_test_score_df[col] = [test_score]\n",
    "                else:\n",
    "                    single_row_test_score_df[col] = [\n",
    "                        _max\n",
    "                    ]  # \"perfect\" scores for factors we are not visualizing in this loop\n",
    "            test_cost = AnnealerScores._costs_df_to_final_cost(\n",
    "                AnnealerScores.scores_df_to_costs_df(single_row_test_score_df)\n",
    "            )\n",
    "            test_output_list.append(\n",
    "                {\"score\": test_score, \"factor\": test_col, \"cost\": test_cost}\n",
    "            )\n",
    "\n",
    "    test_output_df = pd.DataFrame(\n",
    "        test_output_list\n",
    "    )  # merge the above into a dataframe, to be sliced by \"factor\"\n",
    "\n",
    "    for factor in cost_factors:\n",
    "        plt.plot(\n",
    "            test_output_df[test_output_df[\"factor\"] == factor][\"score\"],\n",
    "            test_output_df[test_output_df[\"factor\"] == factor][\"cost\"],\n",
    "            label=factor,\n",
    "            alpha=0.4,\n",
    "        )\n",
    "    _ = plt.legend()\n",
    "\n",
    "\n",
    "# plot_cost_function_responses()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèòÔ∏è Neighbor Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEIGHBOR_GENERATION_PROMPT_TEXT = \"\"\"\n",
    "System:\n",
    "You are an expert prompt engineer, tailoring prompts in different ways so they can be tested and compared.\n",
    "You are being provided with:\n",
    "- Previous Prompt: a recently-tested prompt that you will modify.\n",
    "- Requested Change: a specific alteration that you will apply to the Previous Prompt.\n",
    "- Prompt Objective: a statement clarifying what the previous and changed prompts should aim to accomplish.\n",
    "\n",
    "[Guidelines]\n",
    "All prompts should plausibly be able to fully accomplish this Objective.\n",
    "\n",
    "The Requested Change may or may not seem to be an \"improvement\". That is OK.  You are exploring  \n",
    "many prompting approaches to evaluate performance across a wide spectrum.\n",
    "\n",
    "You are always careful to strictly follow the guidance of the Requested Change.  You think creatively about \n",
    "how best to apply that change while remaining aligned to the Objective.\n",
    "\n",
    "[Prompt Objective]\n",
    "{objective}\n",
    "\n",
    "[Previous Prompt]\n",
    "{current_prompt}\n",
    "\n",
    "[Requested Change]\n",
    "{requested_change}\n",
    "\n",
    "Human:\n",
    "[YOUR TASK]\n",
    "1. Explain step-by-step reasoning for how you will apply this change. \n",
    "2. Then, provide the New Prompt in its entirety. Do not include any commentary or reasoning within the new_prompt.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AnnealerNeighborGenerationResponse(BaseModel):\n",
    "    reasoning: str = Field(description=\"Step-by-step approach to applying this change.\")\n",
    "    new_prompt: str = Field(\n",
    "        description=\"The New Prompt in its entirety, without any commentary or reasoning.\"\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def get_json_schema_str(cls) -> str:\n",
    "        return json.dumps(cls.schema())\n",
    "\n",
    "    @classmethod\n",
    "    def get_format_prompt_str(cls) -> str:\n",
    "        return \"\\n\\nStructure your response as a valid JSON object with the following schema: {output_schema}\"\n",
    "\n",
    "\n",
    "class AnnealerNeighborGenerationResult(BaseModel):\n",
    "    \"\"\"Wraps the annealer generation response (directly from the LLM) with additional context for\n",
    "    bookkeeping/analysis\"\"\"\n",
    "\n",
    "    response: AnnealerNeighborGenerationResponse\n",
    "    previous_prompt: str = Field(description=\"The prompt that was modified.\")\n",
    "    requested_change: str = Field(\n",
    "        description=\"The specific alteration applied to the previous prompt.\"\n",
    "    )\n",
    "    new_prompt_text: str = Field(description=\"The new prompt generated by the LLM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEIGHBOR_GENERATION_MOVE_STRINGS = [\n",
    "    \"Ensure that the prompt is well-aligned with the overarching objective, above all else.  Remove or replace any elements that seem off-track or distracting.\"\n",
    "    \"Modify the prompt so that it gives more precise direction.\",\n",
    "    \"Modify the prompt so that the guidance is more general.\",\n",
    "    \"Modify the prompt so that it uses overall fewer words.\",\n",
    "    \"Modify the prompt so that it uses overall more words.\",\n",
    "    \"Modify the prompt to be overall less repetitive.\",\n",
    "    \"Modify the prompt using repetition to emphasize key elements.\",\n",
    "    \"Without dropping elements aligned with the Prompt Objective, eliminate any superfluous guidance from the prompt.\"\n",
    "    \"Modify the prompt by adding a few-shot example.\",\n",
    "    \"Modify the prompt by removing a few-shot example (if any).\",\n",
    "    \"Modify the prompt by replacing the provided examples (if any) with different ones.\",\n",
    "    \"Modify the prompt to encourage step-by-step planning / reasoning before taking action\",\n",
    "    \"Modify the prompt to remove guidance reated to step-by-step reasoning (if any), while retaining other elements as-is.\",\n",
    "    \"Modify the prompt by rewording / rephrasing it entirely while retaining the overall intent.\",\n",
    "    \"Modify the prompt to reduce the level of evident 'prompt engineering'.\",\n",
    "    \"Modify the prompt to reflect more intentional prompt engineering best-practices.\",\n",
    "    \"Modify the prompt to be more likely to succeed, according to your expert opinion.\",\n",
    "    \"Modify the prompt in a way that retains the intent but that runs contrary to best-practice recommendations.\",\n",
    "    \"Write a new prompt from scratch considering only the Prompt Objective.\",\n",
    "    \"Add or modify an 'expert' or other role for the AI at the beginning of the prompt.\",\n",
    "    \"Remove any explanation of the AI's 'role' from the prompt.\",\n",
    "]\n",
    "\n",
    "# TODO: Study how these perform in practice; consider adding more, removing ones that have issue,\n",
    "# etc.\n",
    "\n",
    "# TODO: Future ideas for the moves:\n",
    "# - Pull \"real\" or pseudo-real few-shot examples from dataset rather than just making them up\n",
    "#   (Note if \"real\" might need to have a non-eval set put aside for this )\n",
    "# - \"Add {n} complementary few-shot examples...\" simultanteously to enhance their value\n",
    "# - Add more specific suggestions for shifting the prompt style\n",
    "# - Expressly call for changes in input or output format (xml<-->json etc.)\n",
    "# - Consider whether special tokens should be in scope? e.g. [INST]\n",
    "# - Add rare \"high entropy\" moves like.... re-injecting the best prompt seen, refreshing via metaprompt,  etc.\n",
    "# - Test and maybe shuffle verbs like Modify.../Rewrite.../Expertly craft.../Tweak/etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  üîÄ Neighbor Generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnealerNeighborGenerator(BaseModel):\n",
    "    neighbor_gen_prompt_text = NEIGHBOR_GENERATION_PROMPT_TEXT\n",
    "    neighbor_gen_moves = NEIGHBOR_GENERATION_MOVE_STRINGS\n",
    "\n",
    "    @property\n",
    "    def config(self):  # -> AnnealerConfig\n",
    "        \"\"\"Returns the global AnnealerConfig - keeping external makes this easier to serializa\"\"\"\n",
    "        return GLOBAL_ANNEALER_CONFIG\n",
    "\n",
    "    def get_neighbor_generation_template_with_format(self) -> PromptTemplate:\n",
    "        _neighbor_generator_prompt = PromptTemplate.from_template(\n",
    "            self.neighbor_gen_prompt_text\n",
    "            + AnnealerNeighborGenerationResponse.get_format_prompt_str(),\n",
    "            partial_variables={\n",
    "                \"objective\": self.config.objective,\n",
    "                \"output_schema\": AnnealerNeighborGenerationResponse.get_json_schema_str(),\n",
    "            },\n",
    "        )\n",
    "        return _neighbor_generator_prompt\n",
    "\n",
    "    def get_random_move(self):\n",
    "        return random.choice(self.neighbor_gen_moves)\n",
    "\n",
    "    def get_neighbor_generation_llm_with_structure(self) -> lc_Runnable:\n",
    "        _structured_llm = self.config.models.neighbor_generation.with_structured_output(\n",
    "            schema=AnnealerNeighborGenerationResponse\n",
    "        )\n",
    "        return _structured_llm\n",
    "\n",
    "    @property\n",
    "    def neighbor_generation_chain(\n",
    "        self,\n",
    "    ) -> lc_Runnable:\n",
    "        _neighbor_generation_prompt_with_format = (\n",
    "            self.get_neighbor_generation_template_with_format()\n",
    "        )\n",
    "        _neighbor_generation_llm_with_structure = (\n",
    "            self.get_neighbor_generation_llm_with_structure()\n",
    "        )\n",
    "        return (\n",
    "            _neighbor_generation_prompt_with_format\n",
    "            | _neighbor_generation_llm_with_structure\n",
    "        )\n",
    "\n",
    "    @traceable\n",
    "    def generate_random_neighbor(\n",
    "        self,\n",
    "        from_prompt_str: str,\n",
    "        current_iter_k: int = -1,\n",
    "        from_prompt_born_in_iter_k: int = -1,\n",
    "    ) -> AnnealerNeighborGenerationResult:\n",
    "        if not from_prompt_str:\n",
    "            raise ValueError(\n",
    "                \"Cannot generate neighbor from empty prompt.  Bootstrapping needed?\"\n",
    "            )\n",
    "\n",
    "        _prompt_origin_story = \"\"\n",
    "        if from_prompt_born_in_iter_k > 0:\n",
    "            _prompt_origin_story = (\n",
    "                f\"(neighbor of prompt born in k={from_prompt_born_in_iter_k})\"\n",
    "            )\n",
    "        print(f\"Generating candidate k={current_iter_k} {_prompt_origin_story}\")\n",
    "\n",
    "        # Random Move Selection\n",
    "        _requested_change: str = random.choice(self.neighbor_gen_moves)\n",
    "        print(f\"   Random move to be applied: `{_requested_change}`\")\n",
    "\n",
    "        _chain = self.neighbor_generation_chain.with_config(\n",
    "            {\n",
    "                \"tags\": [\"neighbor_generation\"],\n",
    "                \"run_name\": f\"Neighbor Generation k={current_iter_k}\",\n",
    "                \"metadata\": {\n",
    "                    \"requested_change\": _requested_change,\n",
    "                    \"from_prompt_born_in_iter_k\": from_prompt_born_in_iter_k,\n",
    "                },\n",
    "            }\n",
    "        ).with_retry(\n",
    "            stop_after_attempt=5,\n",
    "        )\n",
    "\n",
    "        _response: AnnealerNeighborGenerationResponse = _chain.invoke(\n",
    "            {\n",
    "                \"current_prompt\": from_prompt_str,\n",
    "                \"requested_change\": _requested_change,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if not isinstance(_response, AnnealerNeighborGenerationResponse):\n",
    "            try:\n",
    "                # Last ditch attempt to parse\n",
    "                _parser = lc_PydanticOutputParser(\n",
    "                    pydantic_object=AnnealerNeighborGenerationResponse,\n",
    "                )\n",
    "                _response_obj = _parser.invoke(_response)\n",
    "                _neighbor_prompt_text = _response_obj.new_prompt\n",
    "                return _neighbor_prompt_text\n",
    "            except Exception as e:\n",
    "                raise ValueError(\n",
    "                    f\"Unable to parse neighbor generation response (type={type(_response)}): {_response}\"\n",
    "                ) from e\n",
    "\n",
    "        _result = AnnealerNeighborGenerationResult(\n",
    "            response=_response,\n",
    "            previous_prompt=from_prompt_str,\n",
    "            requested_change=_requested_change,\n",
    "            new_prompt_text=_response.new_prompt,\n",
    "        )\n",
    "        return _result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåÄ Annealer Iterations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üîÇ Single-Iteration Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class AnnealerIteration(BaseModel):\n",
    "    k: int\n",
    "    temperature: float\n",
    "    output_evaluators: List[AnnealerCriterionEvaluator]\n",
    "\n",
    "    neighbor_generation_result: Optional[AnnealerNeighborGenerationResult] = None\n",
    "    candidate_prompt: Optional[AnnealerPrompt] = None\n",
    "    # eval_results: Optional[ls_ExperimentResults] = None\n",
    "    target_results: Optional[List[str]] = None\n",
    "    target_run_time: Optional[float] = None\n",
    "\n",
    "    scores: Optional[AnnealerScores] = None\n",
    "\n",
    "    was_accepted: bool = False\n",
    "    was_failure: bool = True  # eval raised an exception or failed get scored\n",
    "\n",
    "    @property\n",
    "    def config(self):  # -> AnnealerConfig\n",
    "        \"\"\"Returns the global AnnealerConfig - keeping external makes this easier to serializa\"\"\"\n",
    "        return GLOBAL_ANNEALER_CONFIG\n",
    "\n",
    "    class Config:  # Pydantic config\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def llm(self) -> lc_BaseChatModel | lc_Runnable:\n",
    "        _llm = self.config.models.target\n",
    "        if re.search(\n",
    "            r\"llama-3.*instruct\", self.config.models.target.model, flags=re.IGNORECASE\n",
    "        ):\n",
    "            # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/#special-tokens-used-with-meta-llama-3\n",
    "            #   - Following this prompt, Llama 3 completes it by generating the {{assistant_message}}.\n",
    "            #       It signals the end of the {{assistant_message}} by generating the <|eot_id|>.\n",
    "            if not _llm.kwargs.get(\"stop\"):\n",
    "                print(\"  * Binding stop token for Llama-3 instruct model\")\n",
    "                _llm = _llm.bind(stop=[\"<|eot_id|>\"])\n",
    "        elif re.search(\n",
    "            r\"phi3.*instruct\", self.config.models.target.model, flags=re.IGNORECASE\n",
    "        ):\n",
    "            # - https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/added_tokens.json\n",
    "            # {\n",
    "            #   \"<|endoftext|>\": 32000,\n",
    "            #      # ...\n",
    "            #   \"<|end|>\": 32007,\n",
    "            # }\n",
    "            if True:  # not _llm.kwargs.get(\"stop\"):\n",
    "                print(\"  * Binding stop token for Phi-3 instruct model\")\n",
    "                _llm = _llm.bind(stop=[\"<|end|>\", \"<|endoftext|>\"])\n",
    "        return _llm\n",
    "\n",
    "    @traceable(run_type=\"tool\", tags=[\"eval\", \"summary_eval\", \"final_scores\"])\n",
    "    def get_target_chain(self, **kwargs) -> lc_Runnable:\n",
    "        if self.candidate_prompt is None:\n",
    "            raise ValueError(\n",
    "                \"Cannot construct candidate chain; candidate_prompt is missng.\"\n",
    "            )\n",
    "        _target_llm = self.llm\n",
    "        if issubclass(_target_llm.__class__, lc_BaseChatModel):\n",
    "            _target_prompt = self.candidate_prompt.as_ChatPromptTemplate()\n",
    "        else:\n",
    "            _target_prompt = self.candidate_prompt.as_PromptTemplate()\n",
    "\n",
    "        _target_chain = (\n",
    "            _target_prompt | _target_llm | lc_StrOutputParser()\n",
    "        ).with_config(\n",
    "            {\n",
    "                \"run_name\": f\"Target chain; Iteration k={self.k}\",\n",
    "                \"metadata\": {\"k\": self.k},\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return _target_chain\n",
    "\n",
    "    @traceable(run_type=\"tool\", tags=[\"eval\", \"summary_eval\", \"final_scores\"])\n",
    "    def aggregate_score_summary_evaluator(\n",
    "        self, runs: List[ls_Run], examples: List[ls_Example]\n",
    "    ) -> ls_EvaluationResult:\n",
    "        \"\"\"To be used as a summary_evaluator in LangSmith's evaluate() function.\n",
    "        Aggregates the individual evaluator scores from all the runs and examples, generating an\n",
    "        AnnealerScores instance that is stored in this AnnealerIteration instance.\n",
    "        Also returns the object expected by LangSmith's evaluate() function.\n",
    "        \"\"\"\n",
    "        self.scores = AnnealerScores.from_runs(\n",
    "            k=self.k,\n",
    "            runs=runs,\n",
    "        )\n",
    "\n",
    "        if self.scores.final_cost and self.scores.final_cost > 0.0:\n",
    "            return ls_EvaluationResult(\n",
    "                key=\"final_cost\",\n",
    "                score=self.scores.final_cost,\n",
    "                evaluator_info={\"k\": self.k},\n",
    "                feedback_config={\n",
    "                    \"type\": \"continuous\",\n",
    "                    \"min\": AnnealerScores.get_min_cost(),\n",
    "                    \"max\": AnnealerScores.get_max_cost(),\n",
    "                },\n",
    "            )\n",
    "        else:\n",
    "            raise AssertionError(f\"Iteration {self.k} did not generate a final cost\")\n",
    "\n",
    "    @traceable(run_type=\"chain\", tags=[\"target_eval\", \"target\"])\n",
    "    def generate_target_ouputs(self) -> List[str]:\n",
    "        _target_chain = (\n",
    "            self.get_target_chain()\n",
    "            .with_config(\n",
    "                {\n",
    "                    \"run_name\": f\"Target chain; Iteration k={self.k}\",\n",
    "                    \"metadata\": {\"k\": self.k},\n",
    "                    \"tags\": [\"target\", \"target_chain\"],\n",
    "                }\n",
    "            )\n",
    "            .with_retry(\n",
    "                stop_after_attempt=5,  # let langsmith handle most retries\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if not self.config.dataset:\n",
    "            raise ValueError(\"Dataset missing\")\n",
    "\n",
    "        _target_inputs = self.config.dataset.example_inputs\n",
    "        print(\n",
    "            f\"  * AnnealerIteration: Invoking target chain for {len(_target_inputs)} inputs...\"\n",
    "        )\n",
    "        _target_run_start_time = time.perf_counter()\n",
    "        self.target_results = []\n",
    "        self.target_results = list(_target_chain.batch(_target_inputs))\n",
    "        _target_run_end_time = time.perf_counter()\n",
    "        self.target_runtime = round(_target_run_end_time - _target_run_start_time, 4)\n",
    "        print(\n",
    "            f\"  * Target run complete. Execution time: {self.target_runtime} seconds.\",\n",
    "            f\" (Avg: {self.target_runtime/len(_target_inputs)} sec/input\",\n",
    "        )\n",
    "        return self.target_results\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    def batch_evaluate_single_criterion(\n",
    "        self,\n",
    "        _evaluator: AnnealerCriterionEvaluator,\n",
    "        _eval_inputs: List[Dict[str, str]],\n",
    "    ) -> List[AnnealerEvalResponse | None]:\n",
    "        _eval_chain = (\n",
    "            _evaluator.get_eval_chain()\n",
    "            .with_config(\n",
    "                {\n",
    "                    \"run_name\": f\"Eval c:{_evaluator.name} k={self.k}\",\n",
    "                    \"metadata\": {\"k\": self.k},\n",
    "                    \"tags\": [\"eval\", \"c:\" + _evaluator.name],\n",
    "                }\n",
    "            )\n",
    "            .with_retry(\n",
    "                stop_after_attempt=5,  # let langsmith handle most retries\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\"    * Eval chain {_evaluator.name} begins...\")\n",
    "        _single_eval_run_start_time = time.perf_counter()\n",
    "        _eval_responses: List[AnnealerEvalResponse | None] = _eval_chain.batch(\n",
    "            _eval_inputs\n",
    "        )\n",
    "        _single_eval_run_end_time = time.perf_counter()\n",
    "        _single_eval_run_execution_time = round(\n",
    "            _single_eval_run_end_time - _single_eval_run_start_time, 4\n",
    "        )\n",
    "        print(\n",
    "            f\"    * Eval chain {_evaluator.name} concluded. {_single_eval_run_execution_time} seconds.\",\n",
    "            f\" (Avg: {_single_eval_run_execution_time/len(_eval_inputs)} sec/evaluaion\",\n",
    "        )\n",
    "\n",
    "        # flaky parsing and other anomalies mean I'm doing a lot of alignment checking here\n",
    "        _num_inputs = len(_eval_inputs)\n",
    "        _valid_eval_responses = [r for r in _eval_responses if r is not None]\n",
    "        _num_valid = len(_valid_eval_responses)\n",
    "        _num_dropped = len(_eval_responses) - _num_valid\n",
    "\n",
    "        if _num_valid > 0:\n",
    "            print(\n",
    "                f\"      {_evaluator.name} generated {_num_valid}/{_num_inputs} valid ratings.\"\n",
    "            )\n",
    "        else:\n",
    "            raise AssertionError(\n",
    "                f\"Eval {_evaluator.name} did not generate any valid ratings\"\n",
    "            )\n",
    "\n",
    "        if _num_dropped > 0:\n",
    "            print(f\"      Note: {_num_dropped} unparsable)\")\n",
    "\n",
    "        if _num_valid + _num_dropped == _num_inputs:\n",
    "            # return all incl. Nones, to maintain alignment within dataframes\n",
    "            return _eval_responses\n",
    "        else:\n",
    "            raise AssertionError(\n",
    "                f\"Eval {_evaluator.name} I/O mismatch: {_num_inputs} inputs -->\\n\",\n",
    "                f\"   ({_num_valid} valid outputs + {_num_dropped} dropped) (total counts should match)\",\n",
    "            )\n",
    "\n",
    "    @traceable(run_type=\"chain\", tags=[\"eval\"])\n",
    "    def batch_evaluate_all_criteria(self, _eval_inputs: List[Dict[str, str]]) -> float:\n",
    "        ### Evaluating target model outputs ###\n",
    "        print(\n",
    "            f\"  * AnnealerIteration: Invoking eval chains for {len(self.output_evaluators)} evaluators...\"\n",
    "        )\n",
    "        _all_eval_run_start_time = time.perf_counter()\n",
    "        _all_evaluator_outputs: Dict[str, List[AnnealerEvalResponse | None]] = {}\n",
    "        _scores_df = pd.DataFrame()\n",
    "\n",
    "        for _criterion_evaluator in self.output_evaluators:\n",
    "            _all_evaluator_outputs[_criterion_evaluator.name] = (\n",
    "                self.batch_evaluate_single_criterion(\n",
    "                    _criterion_evaluator,\n",
    "                    _eval_inputs,\n",
    "                    langsmith_extra={\n",
    "                        \"run_extra\": {\n",
    "                            \"name\": f\"Eval Batch c:{_criterion_evaluator.name} k={self.k}\"\n",
    "                        },\n",
    "                        \"tags\": [\"c:\" + _criterion_evaluator.name],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "            _scores_df[_criterion_evaluator.name] = [\n",
    "                r.rating\n",
    "                if r is not None\n",
    "                else None  # preserve Nones for alignment across columns\n",
    "                for r in _all_evaluator_outputs[_criterion_evaluator.name]\n",
    "            ]\n",
    "            print(\n",
    "                f\"      Criterion {_criterion_evaluator.name} scores: {_scores_df[_criterion_evaluator.name].describe()}\"\n",
    "            )\n",
    "\n",
    "        _all_eval_run_end_time = time.perf_counter()\n",
    "        _all_eval_run_execution_time = _all_eval_run_end_time - _all_eval_run_start_time\n",
    "        print(\n",
    "            f\"  * All eval runs complete. Total execution time: {_all_eval_run_execution_time} seconds. (Avg: {_all_eval_run_execution_time/len(self.output_evaluators)} sec/evaluator\"\n",
    "        )\n",
    "        self.scores = AnnealerScores(\n",
    "            k=self.k,\n",
    "            scores_df=_scores_df,\n",
    "            batch_eval_dict=_all_evaluator_outputs,\n",
    "        )\n",
    "        if self.scores.final_cost and self.scores.final_cost > 0.0:\n",
    "            self.was_failure = False\n",
    "            return self.scores.final_cost\n",
    "        else:\n",
    "            raise AssertionError(\n",
    "                f\"Iteration {self.k} evaluations did not generate a final cost\"\n",
    "            )\n",
    "\n",
    "    @traceable(run_type=\"chain\", tags=[\"target_eval\"])\n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"Runs the langsmith evaluate() function on this iteration's prompt,\n",
    "        and returns the cost.  If restored from cache and in possession of a\n",
    "        successfully-generated evaluation score, we proceed without re-evaluating.\n",
    "\n",
    "        Returns:\n",
    "            float: The final cost of this iteration's prompt\n",
    "        \"\"\"\n",
    "        if self.scores and self.scores.final_cost and not self.was_failure:\n",
    "            # if not self.eval_results:\n",
    "            #     raise AssertionError(\"Unexpected state: scores but no eval_results\")\n",
    "            print(f\"---- Iteration {self.k} evaluation scores cached; skipping... ----\")\n",
    "            return float(self.scores.final_cost)\n",
    "\n",
    "        if not self.config.dataset:\n",
    "            raise ValueError(\"Dataset missing\")\n",
    "\n",
    "        print(f\"---- Iteration {self.k} evaluation beginning... ----\")\n",
    "\n",
    "        # Generate outputs using the target model\n",
    "        _target_results = self.generate_target_ouputs()\n",
    "\n",
    "        # Generate inputs for the evaluators\n",
    "        _reference_outputs = self.config.dataset.example_reference_ouputs\n",
    "        _eval_inputs = [\n",
    "            {\"input\": input_value, \"reference\": ref_value}\n",
    "            for input_value, ref_value in zip(_target_results, _reference_outputs)\n",
    "        ]\n",
    "        # Run the evaluators\n",
    "        _final_score: float = self.batch_evaluate_all_criteria(_eval_inputs)\n",
    "        return _final_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üå°Ô∏è Temperature / Cooling Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnealerTemperatureFunctions(BaseModel):\n",
    "    @property\n",
    "    def config(self):  # -> AnnealerConfig\n",
    "        \"\"\"Returns the global AnnealerConfig - keeping external makes this easier to serializa\"\"\"\n",
    "        return GLOBAL_ANNEALER_CONFIG\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_temperature() -> float:\n",
    "        return 1000\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "\n",
    "    def iterations_fraction_complete(self, k: int) -> float:\n",
    "        \"\"\"Transforms iteration #k into a [0..1] \"fraction complete\", reflecting linear progress\n",
    "        from k=1 to k=max_iteration_count\n",
    "        \"\"\"\n",
    "        return float(k / self.config.max_iteration_count)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_fraction_cooled(percent_complete: float) -> float:\n",
    "        \"\"\"Returns a \"fraction cooled\" in the range [0..1] that can be multiplied by the maximum\n",
    "        temperature to determine the current temperature.  A sigmoidal transform is applied to\n",
    "        define / tune the cooling schedule for the annealing process.\n",
    "\n",
    "        Args:\n",
    "            percent_complete: The current iteration as a fraction of the maximum iteration count.\n",
    "            Expected to be in the range [0..1]\n",
    "\n",
    "        Returns:\n",
    "            float: Percent cooled, in the range [0..1], a nonlinear transform of the percent_complete\n",
    "            that represents the cooling schedule.\n",
    "        \"\"\"\n",
    "        return float(1 - logistic.cdf(percent_complete * 1.25, loc=40, scale=9))\n",
    "\n",
    "    def get_temperature(self, k: int) -> float:\n",
    "        \"\"\"Returns the temperature for iter #k as an absolute float from [1..max_temperature].\n",
    "\n",
    "        Note: The current approach to acceptance (below) does not actually care about this\n",
    "        \"absolute\" temp -- it's based only on a ratio of temp_k to temp_max.  However some other\n",
    "        approaches notably (notably stochastic acceptance) use the absolute temp directly,\n",
    "        making its scale a useful hyperparameter for linear tuning.\n",
    "\n",
    "        Nonetheless we will use it both below and in visualizations, so that this is not\n",
    "        \"dead\" code.\n",
    "        \"\"\"\n",
    "        return float(\n",
    "            self.sigmoid_fraction_cooled(self.iterations_fraction_complete(k=k) * 100)\n",
    "            * self.get_max_temperature()\n",
    "        )\n",
    "\n",
    "    def check_candidate_acceptance(\n",
    "        self, k: int, current_state_cost: float, candidate_cost: float\n",
    "    ) -> bool:\n",
    "        \"\"\"Returns true if the candidate prompt should be accepted for\n",
    "        iteration k+1.\n",
    "\n",
    "        Annealing will:\n",
    "        - always accept \"better\" moves.\n",
    "        - always reject \"failed\" moves (that were unable to be evaluated/scored)\n",
    "        - also accept \"worse\" moves according to a temperature-dependent mechanism:\n",
    "\n",
    "        Let's define a few factors:\n",
    "        cost_diff = (abs(cost_diff/max_cost)):\n",
    "        How big of a cost_diff is this, as a % of the full possible spectrum of costs?\n",
    "        --> For the biggest hypothetically possible cost_diff, this factor ~= 1.0\n",
    "            (For a cost_diff of any plausible size this won't really approach 1.0\n",
    "            as presently implemented.)\n",
    "\n",
    "        cooling_ratio = (T_k / max_temperature):\n",
    "        How far does the current temp T_k indicate we've progressed in terms of cooling?\n",
    "        --> During earliest iterations, T ~= max_tempterature, so this factor is  ~= 1.0\n",
    "        --> While k makes linear progress, T follows the cooling schedule from to min temp.\n",
    "            So this ratio goes from ~1.0 --> ~0.0 in alignment with the cooling schedule.\n",
    "            (Because T is a nonlinear function of k, this isn't simply the same as k/k_max)\n",
    "\n",
    "        So - assuming a cooling schedule that monotonically goes from max_temp to min_temp\n",
    "        (as ours does here), the overall behavior is:\n",
    "        - During really early iterations, all plausible cost_diffs will get accepted, no matter how \"bad\"\n",
    "        - When T is ~50% of the way from max_temp to 0, we're still accepting hill-climbs w/ magnitude of half the entire cost spectrum\n",
    "        - When T is 95% of the way from max_temp to 0, we're only accepting hill-climbs of 5% the full cost spectrum or less\n",
    "        Etc.\n",
    "\n",
    "        Note: We are using deterministic (not stochastic) threshold acceptance...\n",
    "        a purist might not call this \"simulated annealing\"!  **gasp**\n",
    "        Reason: the cost landscape is ridiculously high-dimensional and our neighbor function is either non-deterministic or\n",
    "                if technically not so, it might as well be.  We're more likely to be \"teleporting\" all over the place in cost land\n",
    "                which isn't really an ideal setup for the last third of a stochastic optimization.  TA has been shown to be\n",
    "                easier-to-tune and in many cases faster-to-converge-to-better-results than SA in situations with some similarities.\n",
    "                So, I decided to just go with TA and a sigmoidal cooling schedule semi-tailored to match.  Don't put me in algorithm jail!\n",
    "        \"\"\"\n",
    "        _T = self.get_temperature(k)\n",
    "        cost_diff = candidate_cost - current_state_cost\n",
    "        if cost_diff < 0:\n",
    "            print(\n",
    "                f\" Iter {k}: Accepting 'better' candidate with cost difference of {cost_diff} at temperature {_T}.\"\n",
    "            )\n",
    "            return True\n",
    "        else:\n",
    "            # Consider whether to accept this \"worse\" cost\n",
    "            _min_cost = AnnealerScores.get_min_cost()\n",
    "            _max_cost = AnnealerScores.get_max_cost()\n",
    "            _full_cost_range = _max_cost - _min_cost\n",
    "\n",
    "            # Normalized worseness: What fraction of the full range of costs does this change represent?\n",
    "            # TODO: consdider whether this simple \"absolute\" normalizaton to the full range is problematic\n",
    "            # as changes become smaller (there are a variety of ways we could do this...)\n",
    "            _worseness_magnitude = abs(cost_diff)\n",
    "            _normalized_worseness = _worseness_magnitude / _full_cost_range\n",
    "\n",
    "            # Acceptance threshold: What fraction cooled are we?\n",
    "            _temperature_dependent_acceptance_threshold = (\n",
    "                _T / self.get_max_temperature()\n",
    "            )\n",
    "\n",
    "            if _normalized_worseness < _temperature_dependent_acceptance_threshold:\n",
    "                print(\n",
    "                    f\" Iter {k}: Accepting 'worse' candidate with cost difference of +{_worseness_magnitude} at temperature {_T}.\"\n",
    "                )\n",
    "                return True\n",
    "            else:\n",
    "                print(\n",
    "                    f\" Iter {k}: Rejecting 'worse' candidate with cost difference of +{_worseness_magnitude} at temperature {_T}.\"\n",
    "                )\n",
    "                return False\n",
    "\n",
    "    def visualize_cooling_schedule(\n",
    "        self,\n",
    "        max_iter_to_show: int | None = None,\n",
    "    ):\n",
    "        if max_iter_to_show is None:\n",
    "            max_iter_to_show = self.config.max_iteration_count\n",
    "        \"\"\"Plot what the cooling function looks like\"\"\"\n",
    "        cool_fig, cool_ax = plt.subplots(1, 1)\n",
    "        cool_ax.set_title(\"Cooling Function Progression\")\n",
    "        cool_ax.set_xlabel(\n",
    "            f\"Iterations (k = 0 --> {self.config.max_iteration_count}) )\"\n",
    "        )\n",
    "        cool_ax.set_ylabel(f\"Temperature (T_max = {self.get_max_temperature()})\")\n",
    "        _Kvec = np.linspace(1, max_iter_to_show, 100)\n",
    "        _Tvec = [self.get_temperature(_k) for _k in _Kvec]\n",
    "        _ = cool_ax.plot(_Kvec, _Tvec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üè≠ The Root Annealer class\n",
    "\n",
    "This singleton class maintains the entire state of the annealing process, and performs several of the key high level operations:\n",
    "\n",
    "- Key State:\n",
    "    - The prevailing `AnnealerConfig`, which the user will likely have instantiated, is captured here.  (Note that most of the other `Annealer.*` classes do maintain their own instances of that same object (rather than referring back here) to enable dependency injection & unit testing, but `Annealer` constructs them to mirror this one.)\n",
    "    - The \"current\" iteration number `current_iter_k` is represented in this class\n",
    "    - The entire history of completed iterations up to iteration `k` is kept in `iter_dict` (which stores the historical `AnnealerIteration` objects in their entirety)\n",
    "    - We keep track of `k` and the score of the best-ever-seen iteration\n",
    "\n",
    "- Key Operations:\n",
    "    - When `k==0`:\n",
    "        - Computes an initial score from the `bootstrap_prompt_text`\n",
    "    - At every iteration `k==[1..max]`:\n",
    "        - Selects a Neighbor-Generation move at random\n",
    "        - Inbokes the Neighbor Generation prompt to generate a candidate prompt for iteration `k=k+1`\n",
    "        - Evaluates the candidate prompt to generate its final score\n",
    "        - Invokes the temperature-dependent acceptance function to determine whether the current or the candidate prompt should be propagated to k+1\n",
    "        - Propagates the chosen prompt to a new `AnnealerIteration` object, which is stored in `iter_dict[k+1]`\n",
    "        - Increments `k`\n",
    "        - Repeats until a stop condition is met (currently only `k==max_iter`) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annealer(BaseModel):\n",
    "    bootstrap_prompt_text: Optional[str] = None\n",
    "\n",
    "    temperature_functions: Optional[AnnealerTemperatureFunctions] = Field(\n",
    "        default_factory=AnnealerTemperatureFunctions\n",
    "    )\n",
    "\n",
    "    iter_dict: Dict[int, AnnealerIteration] = {}\n",
    "    current_iter_k: int = -1\n",
    "\n",
    "    current_state_prompt: Optional[AnnealerPrompt] = None\n",
    "    current_state_cost: float = float(\"inf\")\n",
    "    current_state_prompt_born_in_k: int = -1\n",
    "\n",
    "    # we are not guaranteed to stick with the lowest-ever-seen prompt,\n",
    "    # (though if we don't end up with a comparably low-cost endstate it\n",
    "    # suggests some tuning may be in order...)  Nonetheless let's keep track\n",
    "    # of the record-holder so we can learn from and/or make use of it\n",
    "    best_known_prompt_born_in_k: int = -1\n",
    "    best_known_prompt_cost: float = float(\"inf\")\n",
    "\n",
    "    output_evaluator_factory: Optional[AnnealerOutputEvaluatorFactory] = Field(\n",
    "        default_factory=AnnealerOutputEvaluatorFactory\n",
    "    )\n",
    "    neighbor_generator: Optional[AnnealerNeighborGenerator] = Field(\n",
    "        default_factory=AnnealerNeighborGenerator\n",
    "    )\n",
    "\n",
    "    # iterate() will allow some number of iterations to fail during evaluation\n",
    "    # before we give up and fail out entirely.  failed iterations will get the\n",
    "    # worst possible cost.\n",
    "    sequential_failed_iterations: int = 0\n",
    "    max_sequential_failed_iterations: int = 5\n",
    "\n",
    "    @property\n",
    "    def config(self) -> AnnealerConfig:\n",
    "        \"\"\"Returns the global AnnealerConfig - keeping external makes this easier to serializa\"\"\"\n",
    "        return self.get_config()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config() -> AnnealerConfig:\n",
    "        return GLOBAL_ANNEALER_CONFIG\n",
    "\n",
    "    @staticmethod\n",
    "    def get_snapshot_name(snapshot_iter_k: int) -> str:\n",
    "        return f\"AnnealerSnapshot_{snapshot_iter_k}\"\n",
    "\n",
    "    @property\n",
    "    def snapshot_name(self) -> str:\n",
    "        return self.get_snapshot_name(self.current_iter_k)\n",
    "\n",
    "    @classmethod\n",
    "    def get_from_cache(cls, snapshot_iter_k: Optional[int] = None) -> \"Annealer|None\":\n",
    "        _config = cls.get_config()\n",
    "        try:\n",
    "            if snapshot_iter_k is None:\n",
    "                _obj_name = cls.__name__\n",
    "            else:\n",
    "                _obj_name = cls.get_snapshot_name(snapshot_iter_k)\n",
    "            _cached_obj = _config.read_obj_from_disk_cache(obj_name=_obj_name)\n",
    "            if isinstance(_cached_obj, cls):\n",
    "                print(f\"Restored snapshot {_obj_name} from cache\")\n",
    "                return _cached_obj\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to restore snapshot {_obj_name} from cache: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_current_temperature(self) -> float:\n",
    "        if not self.temperature_functions:\n",
    "            raise AssertionError(\"AnnealerTemperatureFunctions not initialized\")\n",
    "        return self.temperature_functions.get_temperature(self.current_iter_k)\n",
    "\n",
    "    @property\n",
    "    def objective(self) -> str:\n",
    "        return self.config.objective\n",
    "\n",
    "    @traceable\n",
    "    def replace_current_state(\n",
    "        self,\n",
    "        new_prompt: AnnealerPrompt,\n",
    "        cost_for_new_prompt: float,\n",
    "        k_for_new_prompt: int,\n",
    "    ):\n",
    "        self.current_state_prompt = new_prompt\n",
    "        self.current_state_cost = cost_for_new_prompt\n",
    "        self.current_state_prompt_born_in_k = k_for_new_prompt\n",
    "        print(\"Current prompt updated.\")\n",
    "\n",
    "        if cost_for_new_prompt < self.best_known_prompt_cost:\n",
    "            print(f\"\"\"\\n****** New best prompt found! ******\n",
    "    New Cost[{k_for_new_prompt}]:  {cost_for_new_prompt} \n",
    "    Formerly: Cost[{self.best_known_prompt_born_in_k}]: {self.best_known_prompt_cost})\"\"\")\n",
    "            self.best_known_prompt_cost = cost_for_new_prompt\n",
    "            self.best_known_prompt_born_in_k = k_for_new_prompt\n",
    "\n",
    "    @traceable\n",
    "    def iterate(self):\n",
    "        if self.temperature_functions is None:\n",
    "            raise AssertionError(\"AnnealerTemperatureFunctions not initialized\")\n",
    "        if not self.output_evaluator_factory:\n",
    "            raise ValueError(\"Annealer output evaluators not configured\")\n",
    "        if self.neighbor_generator is None:\n",
    "            raise ValueError(\"Annealer neighbor generator not initialized\")\n",
    "\n",
    "        ### First step:  Increment the iteration counter\n",
    "        self.current_iter_k = self.current_iter_k + 1\n",
    "        _K = self.current_iter_k\n",
    "        _T = self.get_current_temperature()\n",
    "        print(\n",
    "            f\"\\n================  Iteration k={_K} (Temp={_T}) Begins ===================\"\n",
    "        )\n",
    "\n",
    "        self.iter_dict[_K] = AnnealerIteration(\n",
    "            k=_K,\n",
    "            temperature=_T,\n",
    "            output_evaluators=self.output_evaluator_factory.get_ls_runevaluator_list(\n",
    "                k=_K\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        ######### CANDIDATE PROMPT GENERATION ############\n",
    "        print(f\"* Deriving candidate prompt for iteration k={_K}...\")\n",
    "        _candidate_prompt_text = \"\"\n",
    "        if _K == 0:\n",
    "            # The \"Bootstrap\" prompt text is used directly or indirectly to populate the first prompt\n",
    "            if not self.bootstrap_prompt_text:\n",
    "                print(\"  * Using objective as bootstrap prompt.\")\n",
    "                self.bootstrap_prompt_text = self.objective\n",
    "\n",
    "            if not self.config.models.bootstrap:\n",
    "                print(\"  * Using bootstrap text as initial prompt.\")\n",
    "                _candidate_prompt_text = self.bootstrap_prompt_text\n",
    "            else:\n",
    "                print(\"   * Synthesizing prompt k=0 from bootstrap text.\")\n",
    "                _candidate_prompt_text = (\n",
    "                    AnnealerPrompt.get_llm_synthesized_bootstrap_prompt(\n",
    "                        bootstrap_llm=self.config.models.bootstrap,\n",
    "                        bootstrap_task_str=self.bootstrap_prompt_text,\n",
    "                    )\n",
    "                )\n",
    "        else:  # _K > 0\n",
    "            print(\n",
    "                f\"  * Synthesizing prompt k={self.current_iter_k} as neighbor of current state prompt\"\n",
    "            )\n",
    "            if self.current_state_prompt is None:\n",
    "                raise ValueError(\n",
    "                    \"Candidate generation: current_state_prompt not initialized\"\n",
    "                )\n",
    "            _candidate_prompt_text = \"\"\n",
    "            _neighbor_generation_attempts = 0\n",
    "            _max_neighbor_generation_attempts = (\n",
    "                5  # arbitrary limit to prevent infinite loops\n",
    "            )\n",
    "            while (\n",
    "                not _candidate_prompt_text\n",
    "                and _neighbor_generation_attempts < _max_neighbor_generation_attempts\n",
    "            ):\n",
    "                try:\n",
    "                    _neighbor_generation_result = self.neighbor_generator.generate_random_neighbor(\n",
    "                        from_prompt_str=self.current_state_prompt.system_prompt_text,\n",
    "                        current_iter_k=_K,\n",
    "                        from_prompt_born_in_iter_k=self.current_state_prompt_born_in_k,\n",
    "                    )\n",
    "                    assert isinstance(\n",
    "                        _neighbor_generation_result, AnnealerNeighborGenerationResult\n",
    "                    )\n",
    "                    self.iter_dict[\n",
    "                        _K\n",
    "                    ].neighbor_generation_result = _neighbor_generation_result\n",
    "                    _candidate_prompt_text = _neighbor_generation_result.new_prompt_text\n",
    "                except ValidationError or AssertionError as e:\n",
    "                    if (\n",
    "                        _neighbor_generation_attempts\n",
    "                        >= _max_neighbor_generation_attempts\n",
    "                    ):\n",
    "                        raise RuntimeError(\n",
    "                            f\"Too many failed neighbor generation attempts ({_neighbor_generation_attempts}).  Halting.\"\n",
    "                        ) from e\n",
    "                    else:\n",
    "                        print(f\"Neighbor generation failed: {e}; retrying\")\n",
    "                        _candidate_prompt_text = \"\"\n",
    "                        _neighbor_generation_attempts += 1\n",
    "                        continue\n",
    "\n",
    "        print(f\"  * Candidate generation for iteration k={_K} completed.\")\n",
    "        # wrap the candidate text in an AnnealerPrompt object and store it in the\n",
    "        # AnnealerIteration object for this iteration.\n",
    "        _candidate_prompt = AnnealerPrompt(system_prompt_text=_candidate_prompt_text)\n",
    "        self.iter_dict[_K].candidate_prompt = _candidate_prompt\n",
    "\n",
    "        ######### PROMPT EVALUATION ############\n",
    "        print(f\"* Evaluation for iteration k={_K} begins...\")\n",
    "        try:\n",
    "            _candidate_cost: float = self.iter_dict[_K].evaluate()\n",
    "            if not _candidate_cost >= 0.0:\n",
    "                raise ValueError(\n",
    "                    f\"AnnealerIteration {_K} evaluate() returned unexpected value: {_candidate_cost}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  * Evaluation {_K} finished.\")\n",
    "        except ValueError as e:\n",
    "            # TODO: add checks to identify transient errors e.g. http timeouts, rate limits etc\n",
    "            # since those are different from \"unscorabe garbage prompts\" (which is what this is\n",
    "            # intended to catch) and we can handle e.g. \"sequential failures\" differently for those\n",
    "            self.sequential_failed_iterations += 1\n",
    "\n",
    "            if (\n",
    "                self.sequential_failed_iterations\n",
    "                >= self.max_sequential_failed_iterations\n",
    "            ):\n",
    "                # Too many caught failures; terminate\n",
    "                raise RuntimeError(\n",
    "                    f\"Too many sequential failed iterations (count={self.sequential_failed_iterations}).  Halting.\"\n",
    "                ) from e\n",
    "\n",
    "            else:\n",
    "                # Handle iterations failing occasionally due to e.g. garbage instructions\n",
    "                _candidate_cost = (\n",
    "                    AnnealerScores.get_max_cost()\n",
    "                )  # max-bound used in various places\n",
    "                self.iter_dict[_K].was_failure = True\n",
    "                print(f\"\"\"\n",
    "Evaluation failed for iteration k={_K}.  Assigning max candidate cost = {_candidate_cost}.\"\n",
    "--> Failures: {self.sequential_failed_iterations} out of {self.max_sequential_failed_iterations} sequential failures alllowed.\n",
    "--> (This iteration failed due to exception: {e})\"\"\")\n",
    "        else:\n",
    "            # No unhandled exception = not a failure\n",
    "            self.sequential_failed_iterations = 0\n",
    "\n",
    "        print(f\"* Candidate for iteration k={_K} cost = {_candidate_cost}\")\n",
    "\n",
    "        ######### PROMPT ACCEPTANCE ############\n",
    "        _accept: bool = False\n",
    "        print(f\"* Candidate acceptance for iteration k={_K} begins...\")\n",
    "\n",
    "        if self.iter_dict[_K].was_failure:\n",
    "            # Note that in 'purist' implementations of annealing we might allow\n",
    "            # a \"failed\" or otherwise \"illegal\" iteration to be accepted based on,\n",
    "            # its (high) score and the (presumably high) temperature... as this\n",
    "            # could help escape local minima.  If we were to do so here we would\n",
    "            # want to add some neighbor moves like \"revert to the best-seen state\"\n",
    "            # so that failure states are more likely to be escapable.  Simply\n",
    "            # disallowing such situations for now.\n",
    "            print(f\">> Iteration k={_K} was a failure.  Skipping acceptance check.\")\n",
    "        else:\n",
    "            print(\n",
    "                f\">> Current state cost={self.current_state_cost} vs Candidate cost = {_candidate_cost}\"\n",
    "            )\n",
    "            _accept: bool = self.temperature_functions.check_candidate_acceptance(\n",
    "                k=_K,\n",
    "                current_state_cost=self.current_state_cost,\n",
    "                candidate_cost=_candidate_cost,\n",
    "            )\n",
    "\n",
    "        if _accept:\n",
    "            # note that we apply this action even if we are following the cache\n",
    "            # which will help us reconstruct the current state in some situations\n",
    "            print(\">> **Candidate accepted!**\")\n",
    "            self.iter_dict[_K].was_accepted = True\n",
    "            self.replace_current_state(\n",
    "                new_prompt=_candidate_prompt,\n",
    "                cost_for_new_prompt=_candidate_cost,\n",
    "                k_for_new_prompt=_K,\n",
    "            )\n",
    "        else:\n",
    "            print(\">> Candidate not accepted\")\n",
    "        print(f\"* Candidate acceptance for iteration k={_K} completed.\")\n",
    "\n",
    "        print(f\"\\n======= Iteration {self.current_iter_k} Ends =========\")\n",
    "\n",
    "    def run(self, init_k: Optional[int] = None):\n",
    "        # if the next iteration is unspecified, assume we want to pick up after the last\n",
    "        # completed iteration.\n",
    "        if init_k is None:\n",
    "            init_k = self.current_iter_k + 1\n",
    "\n",
    "            # The first thing that happens in iterate() is that k gets incremented.\n",
    "            # So, here, we will \"pre-decrement\" current_iter_k so that the run\n",
    "            # then starts at the correct iteration.\n",
    "        _pre_k = init_k - 1\n",
    "\n",
    "        self.current_iter_k = _pre_k\n",
    "        while self.current_iter_k < self.config.max_iteration_count:\n",
    "            self.iterate()\n",
    "            try:\n",
    "                if self.config.write_obj_to_disk_cache(obj=self):\n",
    "                    print(\"Annealer object saved to disk cache.\")\n",
    "                if self.config.write_obj_to_disk_cache(\n",
    "                    obj=self, obj_name=self.snapshot_name\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"Snapshot saved to disk cache as obj_name={self.snapshot_name}.\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving state to disk cache: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration / Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set up & Configure the Annealer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell is where most of the user input goes, i.e.:\n",
    "- Specify the objective to be annealed\n",
    "- Choose all the specific models to be enlisted for different elements, including the target model\n",
    "- Set the max_iteration count\n",
    "\n",
    "Note that the last objective that was run is more or less a \"toy\" example.  The task currenly underway\n",
    "is performing some end-to-end tests of:\n",
    "- The full-blown guidance pipeline\n",
    "- A revamped eval pipeline that eschews a bunch of langsmith-provided eval infrastructure in favor of batch calls, in the interest of improving performance.\n",
    "\n",
    "(Good news, the performance changes reduced our 50th percentile eval from ~20s per eval to ~0.4s per eval!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annealer objective: ```use json to enumerate all entities that might be relevant for a conversational home automation agent system to query or manipulate in responding to the user input```\n",
      "Client dataset entities_test-Annealer-TEST04 already exists.  Reusing without changes!\n",
      "LangChainDataset initializing with name: entities_test-Annealer-TEST04\n"
     ]
    }
   ],
   "source": [
    "# SETUP for ANNEALING RUN\n",
    "\n",
    "# VLLM loading models into GPU can be time consuming, and it's not clearly easy to unload these\n",
    "# from memory with LangChain in the middle.  So:\n",
    "# - Uncomment to load the model once per python kernel\n",
    "# - Restart the kernel to unload the model\n",
    "# - Comment this out entirely to skip loading the model at all (e.g. if analyzing a completed run\n",
    "#  written to disk\n",
    "\n",
    "# if \"_local_vllm_evaluator\" not in globals():\n",
    "#     _local_vllm_evaluator = get_local_vllm_completion_model(temperature=0)\n",
    "\n",
    "# TODO: analyze data & set reasonable max_tokens, temperatures, and other sampling & hyperparameters of interest\n",
    "# (Everything currently is fine for testing, but untuned.)\n",
    "\n",
    "\n",
    "OBJECTIVE: str = \"use json to enumerate all entities that might be relevant for a conversational home automation agent system to query or manipulate in responding to the user input\"\n",
    "# Note: the word \"conversational\" above really threw off the model given the lack of a chat task!\n",
    "\n",
    "SOURCE_DATASET_NAME: str = \"entities_test\"\n",
    "MAX_ITERATIONS: int = 500\n",
    "MODELS = AnnealerModels(\n",
    "    bootstrap=None,\n",
    "    guidance_planning=anthropic_claude_3_opus_model,\n",
    "    guidance_generation=anthropic_claude_3_opus_model,\n",
    "    # output_evaluator=_local_vllm_evaluator,   # comment out if just analyzing due to slowness, see above\n",
    "    target=get_desktop_ollama_model(\n",
    "        model=\"phi3:3.8b-instruct\"\n",
    "    ),  # TODO: attach structured json output to the target model for the current task\n",
    ")\n",
    "\n",
    "# Instantiate the global config object\n",
    "GLOBAL_ANNEALER_CONFIG = AnnealerConfig.from_objective_and_source_dataset_name(\n",
    "    objective=OBJECTIVE,\n",
    "    source_dataset_name=SOURCE_DATASET_NAME,\n",
    "    models=MODELS,\n",
    "    user_message_input_vars=[\"input\"],\n",
    "    client_dataset_limit=None,\n",
    "    max_iteration_count=MAX_ITERATIONS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Recover data from disk cache if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guidance_plan', 'guidance_dict', 'AnnealerSnapshot_0', 'AnnealerSnapshot_1', 'AnnealerSnapshot_2', 'AnnealerSnapshot_3', 'AnnealerSnapshot_4', 'AnnealerSnapshot_5', 'AnnealerSnapshot_6', 'AnnealerSnapshot_7', 'AnnealerSnapshot_8', 'AnnealerSnapshot_9', 'AnnealerSnapshot_10', 'AnnealerSnapshot_11', 'AnnealerSnapshot_12', 'AnnealerSnapshot_13', 'AnnealerSnapshot_14', 'AnnealerSnapshot_15', 'AnnealerSnapshot_16', 'AnnealerSnapshot_17', 'AnnealerSnapshot_18', 'AnnealerSnapshot_19', 'AnnealerSnapshot_20', 'AnnealerSnapshot_21', 'AnnealerSnapshot_22', 'AnnealerSnapshot_23', 'AnnealerSnapshot_24', 'AnnealerSnapshot_25', 'AnnealerSnapshot_26', 'AnnealerSnapshot_27', 'AnnealerSnapshot_28', 'AnnealerSnapshot_29', 'AnnealerSnapshot_30', 'AnnealerSnapshot_31', 'AnnealerSnapshot_32', 'AnnealerSnapshot_33', 'AnnealerSnapshot_34', 'AnnealerSnapshot_35', 'AnnealerSnapshot_36', 'AnnealerSnapshot_37', 'AnnealerSnapshot_38', 'AnnealerSnapshot_39', 'AnnealerSnapshot_40', 'AnnealerSnapshot_41', 'AnnealerSnapshot_42', 'AnnealerSnapshot_43', 'AnnealerSnapshot_44', 'AnnealerSnapshot_45', 'AnnealerSnapshot_46', 'AnnealerSnapshot_47', 'AnnealerSnapshot_48', 'AnnealerSnapshot_49', 'AnnealerSnapshot_50', 'AnnealerSnapshot_51', 'AnnealerSnapshot_52', 'AnnealerSnapshot_53', 'AnnealerSnapshot_54', 'AnnealerSnapshot_55', 'AnnealerSnapshot_56', 'AnnealerSnapshot_57', 'AnnealerSnapshot_58', 'AnnealerSnapshot_59', 'AnnealerSnapshot_60', 'AnnealerSnapshot_61', 'AnnealerSnapshot_62', 'AnnealerSnapshot_63', 'AnnealerSnapshot_64', 'AnnealerSnapshot_65', 'AnnealerSnapshot_66', 'AnnealerSnapshot_67', 'AnnealerSnapshot_68', 'AnnealerSnapshot_69', 'AnnealerSnapshot_70', 'AnnealerSnapshot_71', 'AnnealerSnapshot_72', 'AnnealerSnapshot_73', 'AnnealerSnapshot_74', 'AnnealerSnapshot_75', 'AnnealerSnapshot_76', 'AnnealerSnapshot_77', 'AnnealerSnapshot_78', 'AnnealerSnapshot_79', 'AnnealerSnapshot_80', 'AnnealerSnapshot_81', 'AnnealerSnapshot_82', 'AnnealerSnapshot_83', 'AnnealerSnapshot_84', 'AnnealerSnapshot_85', 'AnnealerSnapshot_86', 'AnnealerSnapshot_87', 'AnnealerSnapshot_88', 'AnnealerSnapshot_89', 'AnnealerSnapshot_90', 'AnnealerSnapshot_91', 'AnnealerSnapshot_92', 'AnnealerSnapshot_93', 'AnnealerSnapshot_94', 'AnnealerSnapshot_95', 'AnnealerSnapshot_96', 'AnnealerSnapshot_97', 'AnnealerSnapshot_98', 'AnnealerSnapshot_99', 'AnnealerSnapshot_100', 'AnnealerSnapshot_101', 'AnnealerSnapshot_102', 'AnnealerSnapshot_103', 'AnnealerSnapshot_104', 'AnnealerSnapshot_105', 'AnnealerSnapshot_106', 'AnnealerSnapshot_107', 'AnnealerSnapshot_108', 'AnnealerSnapshot_109', 'AnnealerSnapshot_110', 'AnnealerSnapshot_111', 'AnnealerSnapshot_112', 'AnnealerSnapshot_113', 'AnnealerSnapshot_114', 'AnnealerSnapshot_115', 'AnnealerSnapshot_116', 'AnnealerSnapshot_117', 'AnnealerSnapshot_118', 'AnnealerSnapshot_119', 'AnnealerSnapshot_120', 'AnnealerSnapshot_121', 'AnnealerSnapshot_122', 'AnnealerSnapshot_123', 'AnnealerSnapshot_124', 'AnnealerSnapshot_125', 'AnnealerSnapshot_126', 'AnnealerSnapshot_127', 'AnnealerSnapshot_128', 'AnnealerSnapshot_129', 'AnnealerSnapshot_130', 'AnnealerSnapshot_131', 'AnnealerSnapshot_132', 'AnnealerSnapshot_133', 'AnnealerSnapshot_134', 'AnnealerSnapshot_135', 'AnnealerSnapshot_136', 'AnnealerSnapshot_137', 'AnnealerSnapshot_138', 'AnnealerSnapshot_139', 'AnnealerSnapshot_140', 'AnnealerSnapshot_141', 'AnnealerSnapshot_142', 'AnnealerSnapshot_143', 'AnnealerSnapshot_144', 'AnnealerSnapshot_145', 'AnnealerSnapshot_146', 'AnnealerSnapshot_147', 'AnnealerSnapshot_148', 'AnnealerSnapshot_149', 'AnnealerSnapshot_150', 'AnnealerSnapshot_151', 'AnnealerSnapshot_152', 'AnnealerSnapshot_153', 'AnnealerSnapshot_154', 'AnnealerSnapshot_155', 'AnnealerSnapshot_156', 'AnnealerSnapshot_157', 'AnnealerSnapshot_158', 'AnnealerSnapshot_159', 'AnnealerSnapshot_160', 'AnnealerSnapshot_161', 'AnnealerSnapshot_162', 'AnnealerSnapshot_163', 'AnnealerSnapshot_164', 'AnnealerSnapshot_165', 'AnnealerSnapshot_166', 'AnnealerSnapshot_167', 'AnnealerSnapshot_168', 'AnnealerSnapshot_169', 'AnnealerSnapshot_170', 'AnnealerSnapshot_171', 'AnnealerSnapshot_172', 'AnnealerSnapshot_173', 'AnnealerSnapshot_174', 'AnnealerSnapshot_175', 'AnnealerSnapshot_176', 'AnnealerSnapshot_177', 'AnnealerSnapshot_178', 'AnnealerSnapshot_179', 'AnnealerSnapshot_180', 'AnnealerSnapshot_181', 'AnnealerSnapshot_182', 'AnnealerSnapshot_183', 'AnnealerSnapshot_184', 'AnnealerSnapshot_185', 'AnnealerSnapshot_186', 'AnnealerSnapshot_187', 'AnnealerSnapshot_188', 'AnnealerSnapshot_189', 'AnnealerSnapshot_190', 'AnnealerSnapshot_191', 'AnnealerSnapshot_192', 'AnnealerSnapshot_193', 'AnnealerSnapshot_194', 'AnnealerSnapshot_195', 'AnnealerSnapshot_196', 'AnnealerSnapshot_197', 'AnnealerSnapshot_198', 'AnnealerSnapshot_199', 'AnnealerSnapshot_200', 'AnnealerSnapshot_201', 'AnnealerSnapshot_202', 'AnnealerSnapshot_203', 'AnnealerSnapshot_204', 'AnnealerSnapshot_205', 'AnnealerSnapshot_206', 'AnnealerSnapshot_207', 'AnnealerSnapshot_208', 'AnnealerSnapshot_209', 'AnnealerSnapshot_210', 'AnnealerSnapshot_211', 'AnnealerSnapshot_212', 'AnnealerSnapshot_213', 'AnnealerSnapshot_214', 'AnnealerSnapshot_215', 'AnnealerSnapshot_216', 'AnnealerSnapshot_217', 'AnnealerSnapshot_218', 'AnnealerSnapshot_219', 'AnnealerSnapshot_220', 'AnnealerSnapshot_221', 'AnnealerSnapshot_222', 'AnnealerSnapshot_223', 'AnnealerSnapshot_224', 'AnnealerSnapshot_225', 'AnnealerSnapshot_226', 'AnnealerSnapshot_227', 'AnnealerSnapshot_228', 'AnnealerSnapshot_229', 'AnnealerSnapshot_230', 'AnnealerSnapshot_231', 'AnnealerSnapshot_232', 'AnnealerSnapshot_233', 'AnnealerSnapshot_234', 'AnnealerSnapshot_235', 'AnnealerSnapshot_236', 'AnnealerSnapshot_237', 'AnnealerSnapshot_238', 'AnnealerSnapshot_239', 'AnnealerSnapshot_240', 'AnnealerSnapshot_241', 'AnnealerSnapshot_242', 'AnnealerSnapshot_243', 'AnnealerSnapshot_244', 'AnnealerSnapshot_245', 'AnnealerSnapshot_246', 'AnnealerSnapshot_247', 'AnnealerSnapshot_248', 'AnnealerSnapshot_249', 'AnnealerSnapshot_250', 'AnnealerSnapshot_251', 'AnnealerSnapshot_252', 'AnnealerSnapshot_253', 'AnnealerSnapshot_254', 'AnnealerSnapshot_255', 'AnnealerSnapshot_256', 'AnnealerSnapshot_257', 'AnnealerSnapshot_258', 'AnnealerSnapshot_259', 'AnnealerSnapshot_260', 'AnnealerSnapshot_261', 'AnnealerSnapshot_262', 'AnnealerSnapshot_263', 'AnnealerSnapshot_264', 'AnnealerSnapshot_265', 'AnnealerSnapshot_266', 'AnnealerSnapshot_267', 'AnnealerSnapshot_268', 'AnnealerSnapshot_269', 'AnnealerSnapshot_270', 'AnnealerSnapshot_271', 'AnnealerSnapshot_272', 'AnnealerSnapshot_273', 'AnnealerSnapshot_274', 'AnnealerSnapshot_275', 'AnnealerSnapshot_276', 'AnnealerSnapshot_278', 'AnnealerSnapshot_279', 'AnnealerSnapshot_280', 'AnnealerSnapshot_281', 'AnnealerSnapshot_282', 'AnnealerSnapshot_283', 'AnnealerSnapshot_284', 'AnnealerSnapshot_285', 'AnnealerSnapshot_286', 'AnnealerSnapshot_287', 'AnnealerSnapshot_288', 'AnnealerSnapshot_289', 'AnnealerSnapshot_290', 'AnnealerSnapshot_291', 'AnnealerSnapshot_292', 'AnnealerSnapshot_293', 'AnnealerSnapshot_294', 'AnnealerSnapshot_295', 'AnnealerSnapshot_296', 'AnnealerSnapshot_297', 'AnnealerSnapshot_298', 'AnnealerSnapshot_300', 'AnnealerSnapshot_301', 'AnnealerSnapshot_302', 'AnnealerSnapshot_303', 'AnnealerSnapshot_304', 'AnnealerSnapshot_305', 'AnnealerSnapshot_306', 'AnnealerSnapshot_307', 'AnnealerSnapshot_308', 'AnnealerSnapshot_309', 'AnnealerSnapshot_310', 'AnnealerSnapshot_311', 'AnnealerSnapshot_312', 'AnnealerSnapshot_313', 'AnnealerSnapshot_314', 'AnnealerSnapshot_315', 'AnnealerSnapshot_316', 'AnnealerSnapshot_317', 'AnnealerSnapshot_318', 'AnnealerSnapshot_319', 'AnnealerSnapshot_320', 'AnnealerSnapshot_321', 'AnnealerSnapshot_322', 'AnnealerSnapshot_323', 'AnnealerSnapshot_324', 'AnnealerSnapshot_325', 'AnnealerSnapshot_326', 'AnnealerSnapshot_327', 'AnnealerSnapshot_328', 'AnnealerSnapshot_329', 'AnnealerSnapshot_330', 'AnnealerSnapshot_331', 'AnnealerSnapshot_332', 'AnnealerSnapshot_333', 'AnnealerSnapshot_334', 'AnnealerSnapshot_335', 'AnnealerSnapshot_336', 'AnnealerSnapshot_337', 'AnnealerSnapshot_338', 'AnnealerSnapshot_339', 'AnnealerSnapshot_340', 'AnnealerSnapshot_341', 'AnnealerSnapshot_342', 'AnnealerSnapshot_343', 'AnnealerSnapshot_344', 'AnnealerSnapshot_345', 'AnnealerSnapshot_346', 'AnnealerSnapshot_347', 'AnnealerSnapshot_348', 'AnnealerSnapshot_349', 'AnnealerSnapshot_350', 'AnnealerSnapshot_351', 'AnnealerSnapshot_352', 'AnnealerSnapshot_353', 'AnnealerSnapshot_354', 'AnnealerSnapshot_355', 'AnnealerSnapshot_356', 'AnnealerSnapshot_357', 'AnnealerSnapshot_358', 'AnnealerSnapshot_359', 'AnnealerSnapshot_360', 'AnnealerSnapshot_361', 'AnnealerSnapshot_362', 'AnnealerSnapshot_363', 'AnnealerSnapshot_364', 'AnnealerSnapshot_365', 'AnnealerSnapshot_366', 'AnnealerSnapshot_367', 'AnnealerSnapshot_368', 'AnnealerSnapshot_369', 'AnnealerSnapshot_370', 'AnnealerSnapshot_371', 'AnnealerSnapshot_372', 'AnnealerSnapshot_373', 'AnnealerSnapshot_374', 'AnnealerSnapshot_375', 'AnnealerSnapshot_376', 'AnnealerSnapshot_377', 'AnnealerSnapshot_378', 'AnnealerSnapshot_379', 'AnnealerSnapshot_380', 'AnnealerSnapshot_381', 'AnnealerSnapshot_382', 'AnnealerSnapshot_383', 'AnnealerSnapshot_384', 'AnnealerSnapshot_385', 'AnnealerSnapshot_386', 'AnnealerSnapshot_387', 'AnnealerSnapshot_388', 'AnnealerSnapshot_389', 'AnnealerSnapshot_390', 'AnnealerSnapshot_391', 'AnnealerSnapshot_392', 'AnnealerSnapshot_393', 'AnnealerSnapshot_394', 'AnnealerSnapshot_395', 'AnnealerSnapshot_396', 'AnnealerSnapshot_397', 'AnnealerSnapshot_398', 'AnnealerSnapshot_399', 'AnnealerSnapshot_400', 'AnnealerSnapshot_401', 'AnnealerSnapshot_402', 'AnnealerSnapshot_403', 'AnnealerSnapshot_404', 'AnnealerSnapshot_405', 'AnnealerSnapshot_406', 'AnnealerSnapshot_407', 'AnnealerSnapshot_408', 'AnnealerSnapshot_409', 'AnnealerSnapshot_410', 'AnnealerSnapshot_411', 'AnnealerSnapshot_412', 'AnnealerSnapshot_413', 'AnnealerSnapshot_414', 'AnnealerSnapshot_415', 'AnnealerSnapshot_416', 'AnnealerSnapshot_417', 'AnnealerSnapshot_418', 'AnnealerSnapshot_419', 'AnnealerSnapshot_420', 'AnnealerSnapshot_421', 'AnnealerSnapshot_422', 'AnnealerSnapshot_423', 'AnnealerSnapshot_424', 'AnnealerSnapshot_425', 'AnnealerSnapshot_426', 'AnnealerSnapshot_427', 'AnnealerSnapshot_428', 'AnnealerSnapshot_429', 'AnnealerSnapshot_430', 'AnnealerSnapshot_431', 'AnnealerSnapshot_432', 'AnnealerSnapshot_433', 'AnnealerSnapshot_434', 'AnnealerSnapshot_435', 'AnnealerSnapshot_436', 'AnnealerSnapshot_437', 'AnnealerSnapshot_438', 'AnnealerSnapshot_439', 'AnnealerSnapshot_440', 'AnnealerSnapshot_441', 'AnnealerSnapshot_442', 'AnnealerSnapshot_443', 'AnnealerSnapshot_444', 'AnnealerSnapshot_445', 'AnnealerSnapshot_446', 'AnnealerSnapshot_447', 'AnnealerSnapshot_448', 'AnnealerSnapshot_449', 'AnnealerSnapshot_450', 'AnnealerSnapshot_451', 'AnnealerSnapshot_452', 'AnnealerSnapshot_453', 'AnnealerSnapshot_454', 'AnnealerSnapshot_455', 'AnnealerSnapshot_456', 'AnnealerSnapshot_457', 'AnnealerSnapshot_458', 'AnnealerSnapshot_459', 'AnnealerSnapshot_460', 'AnnealerSnapshot_461', 'AnnealerSnapshot_462', 'AnnealerSnapshot_463', 'AnnealerSnapshot_464', 'AnnealerSnapshot_465', 'AnnealerSnapshot_466', 'AnnealerSnapshot_467', 'AnnealerSnapshot_468', 'AnnealerSnapshot_469', 'AnnealerSnapshot_470', 'AnnealerSnapshot_471', 'AnnealerSnapshot_472', 'AnnealerSnapshot_473', 'AnnealerSnapshot_474', 'AnnealerSnapshot_475', 'AnnealerSnapshot_476', 'AnnealerSnapshot_477', 'AnnealerSnapshot_478', 'AnnealerSnapshot_479', 'AnnealerSnapshot_480', 'AnnealerSnapshot_481', 'AnnealerSnapshot_482', 'AnnealerSnapshot_483', 'AnnealerSnapshot_484', 'AnnealerSnapshot_485', 'AnnealerSnapshot_486', 'AnnealerSnapshot_487', 'AnnealerSnapshot_488', 'AnnealerSnapshot_489', 'AnnealerSnapshot_490', 'AnnealerSnapshot_491', 'AnnealerSnapshot_492', 'AnnealerSnapshot_493', 'AnnealerSnapshot_494', 'AnnealerSnapshot_495', 'AnnealerSnapshot_496', 'AnnealerSnapshot_497', 'AnnealerSnapshot_498', 'AnnealerSnapshot_499', 'Annealer', 'AnnealerSnapshot_500']\n"
     ]
    }
   ],
   "source": [
    "# Preliminary disk-cache operations\n",
    "_cache = GLOBAL_ANNEALER_CONFIG.disk_cache.disk_cache\n",
    "print(list(_cache.keys()))  # type: ignore\n",
    "\n",
    "\n",
    "def clear_annealer_cache_snapshots():\n",
    "    \"\"\"assuming the ANNEALER_SESSION_NAME is being held stable,\n",
    "    invoking this will removes all the annealer state snapshots\n",
    "    from the disk cache, starting that from scratch.\n",
    "    (While retaining the guidance generation outputs which are\n",
    "    likely to be OK and also likely expensive / time-consuming\n",
    "    to recompute.)\n",
    "    \"\"\"\n",
    "    _cache = GLOBAL_ANNEALER_CONFIG.disk_cache.disk_cache\n",
    "    if _cache.get(\"Annealer\"):\n",
    "        del _cache[\"Annealer\"]\n",
    "    for _cached_key in list(_cache.keys()):  # type: ignore\n",
    "        if \"AnnealerSnapshot_\" in _cached_key:\n",
    "            del _cache[_cached_key]\n",
    "\n",
    "\n",
    "# Uncomment to clear the iteration snapshot (but not the guidance)\n",
    "# clear_annealer_cache_snapshots()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Instantiate the Annealer class hierarchy using cached objects (if any), or from scratch (if not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved object from disk cache for `Annealer`\n",
      "Restored snapshot Annealer from cache\n",
      "Annealer object restored from cache.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Annealer class hierarchy\n",
    "try:\n",
    "    a = Annealer.get_from_cache()\n",
    "    if a:\n",
    "        print(\"Annealer object restored from cache.\")\n",
    "    else:\n",
    "        print(\"No Annealer object found in cache.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error restoring Annealer object from cache: {e}\")\n",
    "\n",
    "if not a:\n",
    "    a = Annealer(\n",
    "        bootstrap_prompt_text=GLOBAL_ANNEALER_CONFIG.objective,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run the actual annealing process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the annealer -- (currently commented out to prevent touching the disk cache during analysis :) )\n",
    "\n",
    "# a.run()  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Analyze via a massive DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnnealerDataFrame Class -- later will become part of the hierarchy above\n",
    "\n",
    "from operator import index\n",
    "import pprint\n",
    "\n",
    "\n",
    "_ANNEALER_DATAFRAME_INDEX_NAMES = [\n",
    "    \"Iteration\",\n",
    "    \"Dataset Example\",\n",
    "    \"Evaluation Criterion\",\n",
    "]\n",
    "_ANNEALER_DATAFRAME_COLUMNS = [\n",
    "    # iteration level columns\n",
    "    \"k\",\n",
    "    \"temperature\",\n",
    "    \"current_state_prompt\",\n",
    "    \"current_state_cost\",\n",
    "    \"current_state_prompt_born_in_k\",\n",
    "    \"candidate_prompt\",\n",
    "    \"candidate_prompt_cost\",\n",
    "    \"candidate_was_accepted\",\n",
    "    \"iteration_was_failure\",\n",
    "    \"iteration_runtime\"\n",
    "    # dataset example level columns\n",
    "    \"example_input\",\n",
    "    \"example_target_output\",\n",
    "    \"example_reference_output\",\n",
    "    \"example_target_model_runtime\",\n",
    "    \"example_input_tokens\",\n",
    "    \"example_target_output_tokens\",\n",
    "    # output eval level columns\n",
    "    \"eval_criterion\",\n",
    "    \"eval_rating\",\n",
    "    \"eval_was_failure\",\n",
    "    \"eval_reasoning\",\n",
    "    \"eval_runtime\",\n",
    "    \"eval_input_tokens\",\n",
    "    \"eval_output_tokens\",\n",
    "]\n",
    "\n",
    "\n",
    "class AnnealerDataFrame(BaseModel):\n",
    "    \"\"\"Holds results and other stats for analysis.\"\"\"\n",
    "\n",
    "    class Config:  # pydantic config\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        return GLOBAL_ANNEALER_CONFIG\n",
    "\n",
    "    df: Optional[pd.DataFrame] = None\n",
    "    index_names: List[str] = Field(default=_ANNEALER_DATAFRAME_INDEX_NAMES)\n",
    "    all_columns: List[str] = Field(default=_ANNEALER_DATAFRAME_COLUMNS)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_new_index(\n",
    "        iteration_ids: List[int] = [],\n",
    "        example_ids: List[str] = [],\n",
    "        eval_ids: List[str] = [],\n",
    "    ) -> pd.MultiIndex:\n",
    "        return pd.MultiIndex.from_product(\n",
    "            iterables=[\n",
    "                (iteration_ids),\n",
    "                (example_ids),\n",
    "                (eval_ids),\n",
    "            ],\n",
    "            names=_ANNEALER_DATAFRAME_INDEX_NAMES,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_all_columns() -> List[str]:\n",
    "        return _ANNEALER_DATAFRAME_COLUMNS\n",
    "\n",
    "    @classmethod\n",
    "    def get_new_df(\n",
    "        cls,\n",
    "        index: Optional[pd.MultiIndex] = None,\n",
    "        columns: Optional[List[str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if index is None:\n",
    "            index = cls.get_new_index()\n",
    "        if columns is None:\n",
    "            columns = cls.get_all_columns()\n",
    "        new_df = pd.DataFrame(index=index, columns=columns)\n",
    "        return new_df\n",
    "\n",
    "    all_iterations: List[int] = Field(\n",
    "        default=list(range(0, GLOBAL_ANNEALER_CONFIG.max_iteration_count))\n",
    "    )\n",
    "    all_criteria_names: List[str] = Field(\n",
    "        default=list(AnnealerOutputEvaluatorFactory.get_criteria_names())\n",
    "    )\n",
    "    all_example_ids: List[str] = Field(\n",
    "        default=[str(ex.id) for ex in GLOBAL_ANNEALER_CONFIG.dataset.examples]\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.df = pd.DataFrame(index=self.get_new_index(), columns=self.all_columns)\n",
    "\n",
    "    def update_data(\n",
    "        self,\n",
    "        new_data: dict[str, Any],\n",
    "        iteration_id: int,\n",
    "        example_id: Optional[str] = None,\n",
    "        eval_id: Optional[str] = None,\n",
    "    ):\n",
    "        assert self.df is not None, \"DataFrame not initialized\"\n",
    "\n",
    "        update_index = (iteration_id, example_id, eval_id)\n",
    "        self.df.loc[update_index] = new_data\n",
    "\n",
    "        # update_df = self.get_new_df()  # this retains my index names\n",
    "        # update_df.loc[update_index] = new_data\n",
    "\n",
    "        # # Reindex self.df with the update_index\n",
    "        # existing_df_reindexed = self.df.reindex([update_index], method=\"ffill\")\n",
    "\n",
    "        # # Combine the update_df with the reindexed existing_df\n",
    "        # self.df = pd.concat([self.df, update_df]).combine_first(existing_df_reindexed)\n",
    "\n",
    "    def reconstruct_from_iter_dict(\n",
    "        self,\n",
    "        cached_iter_dict: dict[int, AnnealerIteration],\n",
    "    ):\n",
    "        \"\"\"Reconstructs the DataFrame from the AnnealerIteration objects in the iter_dict.\"\"\"\n",
    "\n",
    "        assert a is not None\n",
    "        assert a.temperature_functions is not None\n",
    "        assert a.iter_dict is not None and len(a.iter_dict) > 0\n",
    "\n",
    "        iteration_dict: Dict[str, Any] = {}\n",
    "        example_dict: Dict[str, Any] = {}\n",
    "        eval_dict: Dict[str, Any] = {}\n",
    "\n",
    "        # use these vars to retain/recreate the state history we didn't record\n",
    "        # TODO: remove this once fixed above\n",
    "        reconstructed_current_state_prompt = cached_iter_dict[\n",
    "            0\n",
    "        ].candidate_prompt.system_prompt_text  # type: ignore\n",
    "        reconstructed_current_state_cost = cached_iter_dict[0].scores.final_cost  # type: ignore\n",
    "        reconstructed_current_state_prompt_born_in_k = 0\n",
    "\n",
    "        # iteration 0\n",
    "        iteration_dict = {\n",
    "            \"k\": 0,\n",
    "            \"current_state_prompt\": reconstructed_current_state_prompt,\n",
    "            \"current_state_cost\": reconstructed_current_state_cost,\n",
    "            \"current_state_prompt_born_in_k\": reconstructed_current_state_prompt_born_in_k,\n",
    "        }\n",
    "        self.update_data(new_data=iteration_dict, iteration_id=0)\n",
    "        iteration_dict = {}\n",
    "\n",
    "        # rest of the iterations\n",
    "        iterations_to_restore = list(cached_iter_dict.keys())\n",
    "        iterations_to_restore.sort()\n",
    "        for _k in iterations_to_restore:\n",
    "            _ai: AnnealerIteration = cached_iter_dict[_k]\n",
    "            print(f\"reconstructing iteration {_k}...\")\n",
    "\n",
    "            iteration_dict = {}  # don't want leakage\n",
    "            example_dict = {}  # don't want leakage\n",
    "            eval_dict = {}  # don't want leakage\n",
    "\n",
    "            iteration_dict_written = False  # can only write each row once currently TODO: fancy multiindexing\n",
    "            example_dict_written = False  # can only write each row once currently TODO: fancy multiindexing\n",
    "\n",
    "            iteration_dict.update(\n",
    "                {\n",
    "                    \"k\": _k,\n",
    "                    \"current_state_prompt\": reconstructed_current_state_prompt,\n",
    "                    \"current_state_cost\": reconstructed_current_state_cost,\n",
    "                    \"current_state_prompt_born_in_k\": reconstructed_current_state_prompt_born_in_k,\n",
    "                    \"temperature\": a.temperature_functions.get_temperature(_k),\n",
    "                    \"iteration_was_failure\": _ai.was_failure,\n",
    "                    \"iteration_runtime\": None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if _ai.candidate_prompt:\n",
    "                iteration_dict.update(\n",
    "                    {\n",
    "                        \"candidate_prompt\": _ai.candidate_prompt.system_prompt_text,\n",
    "                        \"candidate_was_accepted\": _ai.was_accepted,\n",
    "                        \"iteration_was_failure\": _ai.was_failure,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if _ai.scores:\n",
    "                iteration_dict.update(\n",
    "                    {\n",
    "                        \"candidate_prompt_cost\": _ai.scores.final_cost,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                for example_num, example in enumerate(\n",
    "                    list(list(self.config.dataset.examples))\n",
    "                ):\n",
    "                    example_dict = {}\n",
    "                    eval_dict = {}\n",
    "                    example_dict_written = False\n",
    "                    # We didnt' record the example_id in the initial run's iter_df (hindsight)\n",
    "                    # so for the time being we can just use this \"num\" as a stand-in.\n",
    "                    # This means we may NOT actually be aligned on the index level w/ examples, across evals.\n",
    "                    # This isn't good! Checkable / fixable, but -- I'm cutting corners for the moment.\n",
    "                    # (I would worry about this more but this won't matter after we are building this\n",
    "                    # dataframe as we run.  So it's more like \"let's not slice by input until\n",
    "                    # we stop with the reconstruction\")\n",
    "                    # Should be doing this:\n",
    "                    #    example_id = self.config.dataset.examples[example_num].id\n",
    "                    example_id = str(example.id)\n",
    "                    example_dict.update(\n",
    "                        {\n",
    "                            \"example_input\": example.inputs,\n",
    "                            \"example_reference_output\": example.outputs,\n",
    "                            ## some stuff we didn't collect/retain in the test runs\n",
    "                            # \"example_target_output\": None,\n",
    "                            # \"example_target_model_runtime\": None,\n",
    "                            # \"example_input_tokens\": None,\n",
    "                            # \"example_target_output_tokens\": None,\n",
    "                        }\n",
    "                    )\n",
    "                    assert _ai.scores.batch_eval_dict is not None\n",
    "                    for (\n",
    "                        criterion,\n",
    "                        criterion_evals,\n",
    "                    ) in _ai.scores.batch_eval_dict.items():\n",
    "                        eval_dict = {}\n",
    "                        eval_dict_written = False\n",
    "\n",
    "                        eval_id = criterion\n",
    "                        eval_dict.update(\n",
    "                            {\n",
    "                                \"eval_criterion\": criterion,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        if (\n",
    "                            example_num >= len(criterion_evals)\n",
    "                            or criterion_evals[example_num] is None\n",
    "                        ):\n",
    "                            # skip an [iter,example,eval] if that example had zero evals\n",
    "                            # see above re the \"num\" vs \"id\" issue\n",
    "                            eval_dict.update(\n",
    "                                {\n",
    "                                    \"eval_was_failure\": True,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                            assert not (eval_dict_written)\n",
    "                            new_data = {**iteration_dict, **example_dict, **eval_dict}\n",
    "                            self.update_data(\n",
    "                                iteration_id=_k,\n",
    "                                example_id=example_id,\n",
    "                                eval_id=eval_id,\n",
    "                                new_data=new_data,\n",
    "                            )\n",
    "                            eval_dict_written = True\n",
    "                            example_dict_written = True\n",
    "                            iteration_dict_written = True\n",
    "                            eval_dict = {}\n",
    "                            continue\n",
    "                        else:\n",
    "                            example_eval = criterion_evals[example_num]\n",
    "                        assert example_eval is not None\n",
    "                        eval_dict.update(\n",
    "                            {\n",
    "                                \"eval_rating\": example_eval.rating,\n",
    "                                \"eval_reasoning\": example_eval.reasoning,\n",
    "                                \"eval_was_failure\": False,\n",
    "                                ## some stuff we didn't collect/retain in the test runs\n",
    "                                # \"eval_runtime\",\n",
    "                                # \"eval_input_tokens\",\n",
    "                                # \"eval_output_tokens\",\n",
    "                            }\n",
    "                        )\n",
    "                        assert not eval_dict_written\n",
    "                        new_data = {**iteration_dict, **example_dict, **eval_dict}\n",
    "                        self.update_data(\n",
    "                            iteration_id=_k,\n",
    "                            example_id=example_id,\n",
    "                            eval_id=eval_id,\n",
    "                            new_data=new_data,\n",
    "                        )\n",
    "                        eval_dict_written = True\n",
    "                        example_dict_written = True\n",
    "                        iteration_dict_written = True\n",
    "                        eval_dict = {}\n",
    "                    if not example_dict_written:\n",
    "                        # we didn't have any evals written for this example that would have written the\n",
    "                        # example as well.  So we need to write the example now.\n",
    "                        # Note: this shouldn't happen because of the nonzerl \"static\" per-eval data\n",
    "                        # but we will handle this appropriately anyway\n",
    "                        new_data = {**iteration_dict, **example_dict}\n",
    "                        self.update_data(\n",
    "                            iteration_id=_k,\n",
    "                            example_id=example_id,\n",
    "                            new_data=new_data,\n",
    "                        )\n",
    "                        example_dict_written = True\n",
    "                        iteration_dict_written = True\n",
    "                        example_dict = {}\n",
    "                        eval_dict = {}  # prevent leakage\n",
    "\n",
    "            # update the reconstructed current state for the next iteration, if needed\n",
    "            if _ai.was_accepted:\n",
    "                assert _ai.candidate_prompt is not None\n",
    "                assert _ai.scores is not None\n",
    "                print(f\"  * Iteration {_k} candidate was accepted.\")\n",
    "                reconstructed_current_state_prompt = (\n",
    "                    _ai.candidate_prompt.system_prompt_text\n",
    "                )\n",
    "                reconstructed_current_state_cost = _ai.scores.final_cost\n",
    "                reconstructed_current_state_prompt_born_in_k = _k\n",
    "\n",
    "            if not iteration_dict_written:\n",
    "                # we didn't have any examples or evals written for this iteration\n",
    "                # so we need to write the iteration now.\n",
    "                new_data = {**iteration_dict}\n",
    "                self.update_data(\n",
    "                    iteration_id=_k,\n",
    "                    new_data=new_data,\n",
    "                )\n",
    "                iteration_dict_written = True\n",
    "                iteration_dict = {}\n",
    "                example_dict = {}\n",
    "                eval_dict = {}  # prevent leakage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spite of my intentions, the initial test runs didn't organize the bookkeeping very nicely.  So this next call will run over the cached annealer data and reconstruct a dataframe object... later will just build this (or some better-organized subsets of this) as we go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructing iteration 0...\n",
      "  * Iteration 0 candidate was accepted.\n",
      "reconstructing iteration 1...\n",
      "  * Iteration 1 candidate was accepted.\n",
      "reconstructing iteration 2...\n",
      "  * Iteration 2 candidate was accepted.\n",
      "reconstructing iteration 3...\n",
      "  * Iteration 3 candidate was accepted.\n",
      "reconstructing iteration 4...\n",
      "  * Iteration 4 candidate was accepted.\n",
      "reconstructing iteration 5...\n",
      "  * Iteration 5 candidate was accepted.\n",
      "reconstructing iteration 6...\n",
      "  * Iteration 6 candidate was accepted.\n",
      "reconstructing iteration 7...\n",
      "  * Iteration 7 candidate was accepted.\n",
      "reconstructing iteration 8...\n",
      "  * Iteration 8 candidate was accepted.\n",
      "reconstructing iteration 9...\n",
      "  * Iteration 9 candidate was accepted.\n",
      "reconstructing iteration 10...\n",
      "  * Iteration 10 candidate was accepted.\n",
      "reconstructing iteration 11...\n",
      "  * Iteration 11 candidate was accepted.\n",
      "reconstructing iteration 12...\n",
      "  * Iteration 12 candidate was accepted.\n",
      "reconstructing iteration 13...\n",
      "  * Iteration 13 candidate was accepted.\n",
      "reconstructing iteration 14...\n",
      "  * Iteration 14 candidate was accepted.\n",
      "reconstructing iteration 15...\n",
      "  * Iteration 15 candidate was accepted.\n",
      "reconstructing iteration 16...\n",
      "  * Iteration 16 candidate was accepted.\n",
      "reconstructing iteration 17...\n",
      "  * Iteration 17 candidate was accepted.\n",
      "reconstructing iteration 18...\n",
      "  * Iteration 18 candidate was accepted.\n",
      "reconstructing iteration 19...\n",
      "  * Iteration 19 candidate was accepted.\n",
      "reconstructing iteration 20...\n",
      "  * Iteration 20 candidate was accepted.\n",
      "reconstructing iteration 21...\n",
      "  * Iteration 21 candidate was accepted.\n",
      "reconstructing iteration 22...\n",
      "  * Iteration 22 candidate was accepted.\n",
      "reconstructing iteration 23...\n",
      "  * Iteration 23 candidate was accepted.\n",
      "reconstructing iteration 24...\n",
      "  * Iteration 24 candidate was accepted.\n",
      "reconstructing iteration 25...\n",
      "  * Iteration 25 candidate was accepted.\n",
      "reconstructing iteration 26...\n",
      "  * Iteration 26 candidate was accepted.\n",
      "reconstructing iteration 27...\n",
      "  * Iteration 27 candidate was accepted.\n",
      "reconstructing iteration 28...\n",
      "  * Iteration 28 candidate was accepted.\n",
      "reconstructing iteration 29...\n",
      "  * Iteration 29 candidate was accepted.\n",
      "reconstructing iteration 30...\n",
      "  * Iteration 30 candidate was accepted.\n",
      "reconstructing iteration 31...\n",
      "  * Iteration 31 candidate was accepted.\n",
      "reconstructing iteration 32...\n",
      "  * Iteration 32 candidate was accepted.\n",
      "reconstructing iteration 33...\n",
      "  * Iteration 33 candidate was accepted.\n",
      "reconstructing iteration 34...\n",
      "  * Iteration 34 candidate was accepted.\n",
      "reconstructing iteration 35...\n",
      "  * Iteration 35 candidate was accepted.\n",
      "reconstructing iteration 36...\n",
      "  * Iteration 36 candidate was accepted.\n",
      "reconstructing iteration 37...\n",
      "  * Iteration 37 candidate was accepted.\n",
      "reconstructing iteration 38...\n",
      "  * Iteration 38 candidate was accepted.\n",
      "reconstructing iteration 39...\n",
      "  * Iteration 39 candidate was accepted.\n",
      "reconstructing iteration 40...\n",
      "  * Iteration 40 candidate was accepted.\n",
      "reconstructing iteration 41...\n",
      "  * Iteration 41 candidate was accepted.\n",
      "reconstructing iteration 42...\n",
      "  * Iteration 42 candidate was accepted.\n",
      "reconstructing iteration 43...\n",
      "  * Iteration 43 candidate was accepted.\n",
      "reconstructing iteration 44...\n",
      "  * Iteration 44 candidate was accepted.\n",
      "reconstructing iteration 45...\n",
      "  * Iteration 45 candidate was accepted.\n",
      "reconstructing iteration 46...\n",
      "  * Iteration 46 candidate was accepted.\n",
      "reconstructing iteration 47...\n",
      "  * Iteration 47 candidate was accepted.\n",
      "reconstructing iteration 48...\n",
      "  * Iteration 48 candidate was accepted.\n",
      "reconstructing iteration 49...\n",
      "  * Iteration 49 candidate was accepted.\n",
      "reconstructing iteration 50...\n",
      "  * Iteration 50 candidate was accepted.\n",
      "reconstructing iteration 51...\n",
      "  * Iteration 51 candidate was accepted.\n",
      "reconstructing iteration 52...\n",
      "  * Iteration 52 candidate was accepted.\n",
      "reconstructing iteration 53...\n",
      "  * Iteration 53 candidate was accepted.\n",
      "reconstructing iteration 54...\n",
      "  * Iteration 54 candidate was accepted.\n",
      "reconstructing iteration 55...\n",
      "  * Iteration 55 candidate was accepted.\n",
      "reconstructing iteration 56...\n",
      "  * Iteration 56 candidate was accepted.\n",
      "reconstructing iteration 57...\n",
      "  * Iteration 57 candidate was accepted.\n",
      "reconstructing iteration 58...\n",
      "  * Iteration 58 candidate was accepted.\n",
      "reconstructing iteration 59...\n",
      "  * Iteration 59 candidate was accepted.\n",
      "reconstructing iteration 60...\n",
      "  * Iteration 60 candidate was accepted.\n",
      "reconstructing iteration 61...\n",
      "  * Iteration 61 candidate was accepted.\n",
      "reconstructing iteration 62...\n",
      "  * Iteration 62 candidate was accepted.\n",
      "reconstructing iteration 63...\n",
      "  * Iteration 63 candidate was accepted.\n",
      "reconstructing iteration 64...\n",
      "  * Iteration 64 candidate was accepted.\n",
      "reconstructing iteration 65...\n",
      "  * Iteration 65 candidate was accepted.\n",
      "reconstructing iteration 66...\n",
      "  * Iteration 66 candidate was accepted.\n",
      "reconstructing iteration 67...\n",
      "  * Iteration 67 candidate was accepted.\n",
      "reconstructing iteration 68...\n",
      "  * Iteration 68 candidate was accepted.\n",
      "reconstructing iteration 69...\n",
      "  * Iteration 69 candidate was accepted.\n",
      "reconstructing iteration 70...\n",
      "  * Iteration 70 candidate was accepted.\n",
      "reconstructing iteration 71...\n",
      "  * Iteration 71 candidate was accepted.\n",
      "reconstructing iteration 72...\n",
      "  * Iteration 72 candidate was accepted.\n",
      "reconstructing iteration 73...\n",
      "  * Iteration 73 candidate was accepted.\n",
      "reconstructing iteration 74...\n",
      "  * Iteration 74 candidate was accepted.\n",
      "reconstructing iteration 75...\n",
      "  * Iteration 75 candidate was accepted.\n",
      "reconstructing iteration 76...\n",
      "  * Iteration 76 candidate was accepted.\n",
      "reconstructing iteration 77...\n",
      "  * Iteration 77 candidate was accepted.\n",
      "reconstructing iteration 78...\n",
      "  * Iteration 78 candidate was accepted.\n",
      "reconstructing iteration 79...\n",
      "  * Iteration 79 candidate was accepted.\n",
      "reconstructing iteration 80...\n",
      "  * Iteration 80 candidate was accepted.\n",
      "reconstructing iteration 81...\n",
      "  * Iteration 81 candidate was accepted.\n",
      "reconstructing iteration 82...\n",
      "  * Iteration 82 candidate was accepted.\n",
      "reconstructing iteration 83...\n",
      "  * Iteration 83 candidate was accepted.\n",
      "reconstructing iteration 84...\n",
      "  * Iteration 84 candidate was accepted.\n",
      "reconstructing iteration 85...\n",
      "  * Iteration 85 candidate was accepted.\n",
      "reconstructing iteration 86...\n",
      "  * Iteration 86 candidate was accepted.\n",
      "reconstructing iteration 87...\n",
      "  * Iteration 87 candidate was accepted.\n",
      "reconstructing iteration 88...\n",
      "  * Iteration 88 candidate was accepted.\n",
      "reconstructing iteration 89...\n",
      "  * Iteration 89 candidate was accepted.\n",
      "reconstructing iteration 90...\n",
      "  * Iteration 90 candidate was accepted.\n",
      "reconstructing iteration 91...\n",
      "  * Iteration 91 candidate was accepted.\n",
      "reconstructing iteration 92...\n",
      "  * Iteration 92 candidate was accepted.\n",
      "reconstructing iteration 93...\n",
      "  * Iteration 93 candidate was accepted.\n",
      "reconstructing iteration 94...\n",
      "  * Iteration 94 candidate was accepted.\n",
      "reconstructing iteration 95...\n",
      "  * Iteration 95 candidate was accepted.\n",
      "reconstructing iteration 96...\n",
      "  * Iteration 96 candidate was accepted.\n",
      "reconstructing iteration 97...\n",
      "  * Iteration 97 candidate was accepted.\n",
      "reconstructing iteration 98...\n",
      "  * Iteration 98 candidate was accepted.\n",
      "reconstructing iteration 99...\n",
      "  * Iteration 99 candidate was accepted.\n",
      "reconstructing iteration 100...\n",
      "  * Iteration 100 candidate was accepted.\n",
      "reconstructing iteration 101...\n",
      "  * Iteration 101 candidate was accepted.\n",
      "reconstructing iteration 102...\n",
      "  * Iteration 102 candidate was accepted.\n",
      "reconstructing iteration 103...\n",
      "  * Iteration 103 candidate was accepted.\n",
      "reconstructing iteration 104...\n",
      "  * Iteration 104 candidate was accepted.\n",
      "reconstructing iteration 105...\n",
      "  * Iteration 105 candidate was accepted.\n",
      "reconstructing iteration 106...\n",
      "  * Iteration 106 candidate was accepted.\n",
      "reconstructing iteration 107...\n",
      "  * Iteration 107 candidate was accepted.\n",
      "reconstructing iteration 108...\n",
      "  * Iteration 108 candidate was accepted.\n",
      "reconstructing iteration 109...\n",
      "  * Iteration 109 candidate was accepted.\n",
      "reconstructing iteration 110...\n",
      "  * Iteration 110 candidate was accepted.\n",
      "reconstructing iteration 111...\n",
      "  * Iteration 111 candidate was accepted.\n",
      "reconstructing iteration 112...\n",
      "  * Iteration 112 candidate was accepted.\n",
      "reconstructing iteration 113...\n",
      "  * Iteration 113 candidate was accepted.\n",
      "reconstructing iteration 114...\n",
      "  * Iteration 114 candidate was accepted.\n",
      "reconstructing iteration 115...\n",
      "  * Iteration 115 candidate was accepted.\n",
      "reconstructing iteration 116...\n",
      "  * Iteration 116 candidate was accepted.\n",
      "reconstructing iteration 117...\n",
      "  * Iteration 117 candidate was accepted.\n",
      "reconstructing iteration 118...\n",
      "  * Iteration 118 candidate was accepted.\n",
      "reconstructing iteration 119...\n",
      "  * Iteration 119 candidate was accepted.\n",
      "reconstructing iteration 120...\n",
      "  * Iteration 120 candidate was accepted.\n",
      "reconstructing iteration 121...\n",
      "  * Iteration 121 candidate was accepted.\n",
      "reconstructing iteration 122...\n",
      "  * Iteration 122 candidate was accepted.\n",
      "reconstructing iteration 123...\n",
      "  * Iteration 123 candidate was accepted.\n",
      "reconstructing iteration 124...\n",
      "  * Iteration 124 candidate was accepted.\n",
      "reconstructing iteration 125...\n",
      "  * Iteration 125 candidate was accepted.\n",
      "reconstructing iteration 126...\n",
      "  * Iteration 126 candidate was accepted.\n",
      "reconstructing iteration 127...\n",
      "  * Iteration 127 candidate was accepted.\n",
      "reconstructing iteration 128...\n",
      "  * Iteration 128 candidate was accepted.\n",
      "reconstructing iteration 129...\n",
      "  * Iteration 129 candidate was accepted.\n",
      "reconstructing iteration 130...\n",
      "  * Iteration 130 candidate was accepted.\n",
      "reconstructing iteration 131...\n",
      "  * Iteration 131 candidate was accepted.\n",
      "reconstructing iteration 132...\n",
      "  * Iteration 132 candidate was accepted.\n",
      "reconstructing iteration 133...\n",
      "  * Iteration 133 candidate was accepted.\n",
      "reconstructing iteration 134...\n",
      "  * Iteration 134 candidate was accepted.\n",
      "reconstructing iteration 135...\n",
      "  * Iteration 135 candidate was accepted.\n",
      "reconstructing iteration 136...\n",
      "  * Iteration 136 candidate was accepted.\n",
      "reconstructing iteration 137...\n",
      "  * Iteration 137 candidate was accepted.\n",
      "reconstructing iteration 138...\n",
      "  * Iteration 138 candidate was accepted.\n",
      "reconstructing iteration 139...\n",
      "  * Iteration 139 candidate was accepted.\n",
      "reconstructing iteration 140...\n",
      "  * Iteration 140 candidate was accepted.\n",
      "reconstructing iteration 141...\n",
      "  * Iteration 141 candidate was accepted.\n",
      "reconstructing iteration 142...\n",
      "  * Iteration 142 candidate was accepted.\n",
      "reconstructing iteration 143...\n",
      "  * Iteration 143 candidate was accepted.\n",
      "reconstructing iteration 144...\n",
      "  * Iteration 144 candidate was accepted.\n",
      "reconstructing iteration 145...\n",
      "  * Iteration 145 candidate was accepted.\n",
      "reconstructing iteration 146...\n",
      "  * Iteration 146 candidate was accepted.\n",
      "reconstructing iteration 147...\n",
      "  * Iteration 147 candidate was accepted.\n",
      "reconstructing iteration 148...\n",
      "  * Iteration 148 candidate was accepted.\n",
      "reconstructing iteration 149...\n",
      "  * Iteration 149 candidate was accepted.\n",
      "reconstructing iteration 150...\n",
      "  * Iteration 150 candidate was accepted.\n",
      "reconstructing iteration 151...\n",
      "  * Iteration 151 candidate was accepted.\n",
      "reconstructing iteration 152...\n",
      "  * Iteration 152 candidate was accepted.\n",
      "reconstructing iteration 153...\n",
      "  * Iteration 153 candidate was accepted.\n",
      "reconstructing iteration 154...\n",
      "  * Iteration 154 candidate was accepted.\n",
      "reconstructing iteration 155...\n",
      "  * Iteration 155 candidate was accepted.\n",
      "reconstructing iteration 156...\n",
      "  * Iteration 156 candidate was accepted.\n",
      "reconstructing iteration 157...\n",
      "  * Iteration 157 candidate was accepted.\n",
      "reconstructing iteration 158...\n",
      "  * Iteration 158 candidate was accepted.\n",
      "reconstructing iteration 159...\n",
      "  * Iteration 159 candidate was accepted.\n",
      "reconstructing iteration 160...\n",
      "  * Iteration 160 candidate was accepted.\n",
      "reconstructing iteration 161...\n",
      "  * Iteration 161 candidate was accepted.\n",
      "reconstructing iteration 162...\n",
      "  * Iteration 162 candidate was accepted.\n",
      "reconstructing iteration 163...\n",
      "  * Iteration 163 candidate was accepted.\n",
      "reconstructing iteration 164...\n",
      "  * Iteration 164 candidate was accepted.\n",
      "reconstructing iteration 165...\n",
      "  * Iteration 165 candidate was accepted.\n",
      "reconstructing iteration 166...\n",
      "  * Iteration 166 candidate was accepted.\n",
      "reconstructing iteration 167...\n",
      "  * Iteration 167 candidate was accepted.\n",
      "reconstructing iteration 168...\n",
      "  * Iteration 168 candidate was accepted.\n",
      "reconstructing iteration 169...\n",
      "  * Iteration 169 candidate was accepted.\n",
      "reconstructing iteration 170...\n",
      "  * Iteration 170 candidate was accepted.\n",
      "reconstructing iteration 171...\n",
      "  * Iteration 171 candidate was accepted.\n",
      "reconstructing iteration 172...\n",
      "  * Iteration 172 candidate was accepted.\n",
      "reconstructing iteration 173...\n",
      "  * Iteration 173 candidate was accepted.\n",
      "reconstructing iteration 174...\n",
      "  * Iteration 174 candidate was accepted.\n",
      "reconstructing iteration 175...\n",
      "  * Iteration 175 candidate was accepted.\n",
      "reconstructing iteration 176...\n",
      "  * Iteration 176 candidate was accepted.\n",
      "reconstructing iteration 177...\n",
      "  * Iteration 177 candidate was accepted.\n",
      "reconstructing iteration 178...\n",
      "  * Iteration 178 candidate was accepted.\n",
      "reconstructing iteration 179...\n",
      "  * Iteration 179 candidate was accepted.\n",
      "reconstructing iteration 180...\n",
      "  * Iteration 180 candidate was accepted.\n",
      "reconstructing iteration 181...\n",
      "  * Iteration 181 candidate was accepted.\n",
      "reconstructing iteration 182...\n",
      "  * Iteration 182 candidate was accepted.\n",
      "reconstructing iteration 183...\n",
      "  * Iteration 183 candidate was accepted.\n",
      "reconstructing iteration 184...\n",
      "  * Iteration 184 candidate was accepted.\n",
      "reconstructing iteration 185...\n",
      "  * Iteration 185 candidate was accepted.\n",
      "reconstructing iteration 186...\n",
      "  * Iteration 186 candidate was accepted.\n",
      "reconstructing iteration 187...\n",
      "  * Iteration 187 candidate was accepted.\n",
      "reconstructing iteration 188...\n",
      "  * Iteration 188 candidate was accepted.\n",
      "reconstructing iteration 189...\n",
      "  * Iteration 189 candidate was accepted.\n",
      "reconstructing iteration 190...\n",
      "  * Iteration 190 candidate was accepted.\n",
      "reconstructing iteration 191...\n",
      "  * Iteration 191 candidate was accepted.\n",
      "reconstructing iteration 192...\n",
      "  * Iteration 192 candidate was accepted.\n",
      "reconstructing iteration 193...\n",
      "  * Iteration 193 candidate was accepted.\n",
      "reconstructing iteration 194...\n",
      "  * Iteration 194 candidate was accepted.\n",
      "reconstructing iteration 195...\n",
      "  * Iteration 195 candidate was accepted.\n",
      "reconstructing iteration 196...\n",
      "  * Iteration 196 candidate was accepted.\n",
      "reconstructing iteration 197...\n",
      "  * Iteration 197 candidate was accepted.\n",
      "reconstructing iteration 198...\n",
      "  * Iteration 198 candidate was accepted.\n",
      "reconstructing iteration 199...\n",
      "  * Iteration 199 candidate was accepted.\n",
      "reconstructing iteration 200...\n",
      "  * Iteration 200 candidate was accepted.\n",
      "reconstructing iteration 201...\n",
      "  * Iteration 201 candidate was accepted.\n",
      "reconstructing iteration 202...\n",
      "  * Iteration 202 candidate was accepted.\n",
      "reconstructing iteration 203...\n",
      "  * Iteration 203 candidate was accepted.\n",
      "reconstructing iteration 204...\n",
      "  * Iteration 204 candidate was accepted.\n",
      "reconstructing iteration 205...\n",
      "  * Iteration 205 candidate was accepted.\n",
      "reconstructing iteration 206...\n",
      "  * Iteration 206 candidate was accepted.\n",
      "reconstructing iteration 207...\n",
      "  * Iteration 207 candidate was accepted.\n",
      "reconstructing iteration 208...\n",
      "  * Iteration 208 candidate was accepted.\n",
      "reconstructing iteration 209...\n",
      "  * Iteration 209 candidate was accepted.\n",
      "reconstructing iteration 210...\n",
      "  * Iteration 210 candidate was accepted.\n",
      "reconstructing iteration 211...\n",
      "  * Iteration 211 candidate was accepted.\n",
      "reconstructing iteration 212...\n",
      "  * Iteration 212 candidate was accepted.\n",
      "reconstructing iteration 213...\n",
      "  * Iteration 213 candidate was accepted.\n",
      "reconstructing iteration 214...\n",
      "  * Iteration 214 candidate was accepted.\n",
      "reconstructing iteration 215...\n",
      "  * Iteration 215 candidate was accepted.\n",
      "reconstructing iteration 216...\n",
      "  * Iteration 216 candidate was accepted.\n",
      "reconstructing iteration 217...\n",
      "  * Iteration 217 candidate was accepted.\n",
      "reconstructing iteration 218...\n",
      "  * Iteration 218 candidate was accepted.\n",
      "reconstructing iteration 219...\n",
      "  * Iteration 219 candidate was accepted.\n",
      "reconstructing iteration 220...\n",
      "  * Iteration 220 candidate was accepted.\n",
      "reconstructing iteration 221...\n",
      "  * Iteration 221 candidate was accepted.\n",
      "reconstructing iteration 222...\n",
      "  * Iteration 222 candidate was accepted.\n",
      "reconstructing iteration 223...\n",
      "  * Iteration 223 candidate was accepted.\n",
      "reconstructing iteration 224...\n",
      "  * Iteration 224 candidate was accepted.\n",
      "reconstructing iteration 225...\n",
      "  * Iteration 225 candidate was accepted.\n",
      "reconstructing iteration 226...\n",
      "  * Iteration 226 candidate was accepted.\n",
      "reconstructing iteration 227...\n",
      "  * Iteration 227 candidate was accepted.\n",
      "reconstructing iteration 228...\n",
      "  * Iteration 228 candidate was accepted.\n",
      "reconstructing iteration 229...\n",
      "  * Iteration 229 candidate was accepted.\n",
      "reconstructing iteration 230...\n",
      "  * Iteration 230 candidate was accepted.\n",
      "reconstructing iteration 231...\n",
      "  * Iteration 231 candidate was accepted.\n",
      "reconstructing iteration 232...\n",
      "  * Iteration 232 candidate was accepted.\n",
      "reconstructing iteration 233...\n",
      "  * Iteration 233 candidate was accepted.\n",
      "reconstructing iteration 234...\n",
      "  * Iteration 234 candidate was accepted.\n",
      "reconstructing iteration 235...\n",
      "  * Iteration 235 candidate was accepted.\n",
      "reconstructing iteration 236...\n",
      "  * Iteration 236 candidate was accepted.\n",
      "reconstructing iteration 237...\n",
      "reconstructing iteration 238...\n",
      "  * Iteration 238 candidate was accepted.\n",
      "reconstructing iteration 239...\n",
      "  * Iteration 239 candidate was accepted.\n",
      "reconstructing iteration 240...\n",
      "  * Iteration 240 candidate was accepted.\n",
      "reconstructing iteration 241...\n",
      "  * Iteration 241 candidate was accepted.\n",
      "reconstructing iteration 242...\n",
      "  * Iteration 242 candidate was accepted.\n",
      "reconstructing iteration 243...\n",
      "  * Iteration 243 candidate was accepted.\n",
      "reconstructing iteration 244...\n",
      "  * Iteration 244 candidate was accepted.\n",
      "reconstructing iteration 245...\n",
      "  * Iteration 245 candidate was accepted.\n",
      "reconstructing iteration 246...\n",
      "  * Iteration 246 candidate was accepted.\n",
      "reconstructing iteration 247...\n",
      "reconstructing iteration 248...\n",
      "  * Iteration 248 candidate was accepted.\n",
      "reconstructing iteration 249...\n",
      "reconstructing iteration 250...\n",
      "reconstructing iteration 251...\n",
      "  * Iteration 251 candidate was accepted.\n",
      "reconstructing iteration 252...\n",
      "  * Iteration 252 candidate was accepted.\n",
      "reconstructing iteration 253...\n",
      "  * Iteration 253 candidate was accepted.\n",
      "reconstructing iteration 254...\n",
      "reconstructing iteration 255...\n",
      "  * Iteration 255 candidate was accepted.\n",
      "reconstructing iteration 256...\n",
      "reconstructing iteration 257...\n",
      "reconstructing iteration 258...\n",
      "  * Iteration 258 candidate was accepted.\n",
      "reconstructing iteration 259...\n",
      "reconstructing iteration 260...\n",
      "  * Iteration 260 candidate was accepted.\n",
      "reconstructing iteration 261...\n",
      "  * Iteration 261 candidate was accepted.\n",
      "reconstructing iteration 262...\n",
      "  * Iteration 262 candidate was accepted.\n",
      "reconstructing iteration 263...\n",
      "  * Iteration 263 candidate was accepted.\n",
      "reconstructing iteration 264...\n",
      "reconstructing iteration 265...\n",
      "reconstructing iteration 266...\n",
      "  * Iteration 266 candidate was accepted.\n",
      "reconstructing iteration 267...\n",
      "  * Iteration 267 candidate was accepted.\n",
      "reconstructing iteration 268...\n",
      "reconstructing iteration 269...\n",
      "reconstructing iteration 270...\n",
      "  * Iteration 270 candidate was accepted.\n",
      "reconstructing iteration 271...\n",
      "  * Iteration 271 candidate was accepted.\n",
      "reconstructing iteration 272...\n",
      "  * Iteration 272 candidate was accepted.\n",
      "reconstructing iteration 273...\n",
      "  * Iteration 273 candidate was accepted.\n",
      "reconstructing iteration 274...\n",
      "reconstructing iteration 275...\n",
      "  * Iteration 275 candidate was accepted.\n",
      "reconstructing iteration 276...\n",
      "reconstructing iteration 277...\n",
      "reconstructing iteration 278...\n",
      "reconstructing iteration 279...\n",
      "  * Iteration 279 candidate was accepted.\n",
      "reconstructing iteration 280...\n",
      "  * Iteration 280 candidate was accepted.\n",
      "reconstructing iteration 281...\n",
      "  * Iteration 281 candidate was accepted.\n",
      "reconstructing iteration 282...\n",
      "reconstructing iteration 283...\n",
      "reconstructing iteration 284...\n",
      "  * Iteration 284 candidate was accepted.\n",
      "reconstructing iteration 285...\n",
      "  * Iteration 285 candidate was accepted.\n",
      "reconstructing iteration 286...\n",
      "reconstructing iteration 287...\n",
      "reconstructing iteration 288...\n",
      "  * Iteration 288 candidate was accepted.\n",
      "reconstructing iteration 289...\n",
      "  * Iteration 289 candidate was accepted.\n",
      "reconstructing iteration 290...\n",
      "  * Iteration 290 candidate was accepted.\n",
      "reconstructing iteration 291...\n",
      "  * Iteration 291 candidate was accepted.\n",
      "reconstructing iteration 292...\n",
      "  * Iteration 292 candidate was accepted.\n",
      "reconstructing iteration 293...\n",
      "reconstructing iteration 294...\n",
      "  * Iteration 294 candidate was accepted.\n",
      "reconstructing iteration 295...\n",
      "reconstructing iteration 296...\n",
      "reconstructing iteration 297...\n",
      "reconstructing iteration 298...\n",
      "reconstructing iteration 299...\n",
      "reconstructing iteration 300...\n",
      "reconstructing iteration 301...\n",
      "reconstructing iteration 302...\n",
      "  * Iteration 302 candidate was accepted.\n",
      "reconstructing iteration 303...\n",
      "reconstructing iteration 304...\n",
      "  * Iteration 304 candidate was accepted.\n",
      "reconstructing iteration 305...\n",
      "reconstructing iteration 306...\n",
      "  * Iteration 306 candidate was accepted.\n",
      "reconstructing iteration 307...\n",
      "  * Iteration 307 candidate was accepted.\n",
      "reconstructing iteration 308...\n",
      "reconstructing iteration 309...\n",
      "reconstructing iteration 310...\n",
      "  * Iteration 310 candidate was accepted.\n",
      "reconstructing iteration 311...\n",
      "  * Iteration 311 candidate was accepted.\n",
      "reconstructing iteration 312...\n",
      "reconstructing iteration 313...\n",
      "reconstructing iteration 314...\n",
      "  * Iteration 314 candidate was accepted.\n",
      "reconstructing iteration 315...\n",
      "reconstructing iteration 316...\n",
      "reconstructing iteration 317...\n",
      "  * Iteration 317 candidate was accepted.\n",
      "reconstructing iteration 318...\n"
     ]
    }
   ],
   "source": [
    "#### Reconstruct a dataframe from the completed / uncached Annealer object\n",
    "if \"adf\" not in globals():\n",
    "    adf = AnnealerDataFrame()\n",
    "\n",
    "adf.reconstruct_from_iter_dict(a.iter_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "At last!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we can just confirm that the dataframe has data by visuzlizing the current state cost over the range of iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGwCAYAAABPSaTdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAACq4UlEQVR4nO2deZwcVbn3f73PPpNtZhKyLxACScAAYdhEEnZRrlyvIArKdsGgl+VFDCIIKkH0unERroqgVxBXQHZCgLCFBAIhG0SIgQTIZLLNPtPTS71/dJ+qU6dOVVdVd1X1dJ6vH5zuquo6p6o7fX79rCFFURQQBEEQBEFUKOGgJ0AQBEEQBOElJHYIgiAIgqhoSOwQBEEQBFHRkNghCIIgCKKiIbFDEARBEERFQ2KHIAiCIIiKhsQOQRAEQRAVTTToCZQD2WwWH3/8Merr6xEKhYKeDkEQBEEQNlAUBT09PRg3bhzCYXP7DYkdAB9//DEmTJgQ9DQIgiAIgnDBtm3bMH78eNP9JHYA1NfXA8jdrIaGhoBnQxAEQRCEHbq7uzFhwgR1HTeDxA6guq4aGhpI7BAEQRDEMKNQCAoFKBMEQRAEUdGQ2CEIgiAIoqIhsUMQBEEQREVDYocgCIIgiIqGxA5BEARBEBUNiR2CIAiCICoaEjsEQRAEQVQ0JHYIgiAIgqhoSOwQBEEQBFHRkNghCIIgCKKiIbFDEARBEERFQ2KHIAiCIIiKhsQOQZiQzmQxlM4GPQ2CIAiiSEjsEIQERVFw9K3P4vAfPIN0hgQPQRDEcIbEDkFI6B/KYEdPEl0DKWzvGgx6OgRBEEQRkNghCIIgCKKiIbFDEARBEERFQ2KHICQoQU+AIAiCKBkkdghCQlYhuUMQBFEpkNghCAkKJWARBEFUDCR2CEICb9kJhQKcSBmjKAoyWbKAEQRR/pDYIQgJ5MYqzOV/fBOHfX8puvpTQU+FIAjCEhI7BCEhw4kd0j1yHlu7HXv7U/jL6m1BT4UgCMISEjsEIYEXOCR2rOlNpoOeAkEQhCUkdghCAu/GIpeWNT2DJHYIgihvSOwQhAQ+7tZK6tzz8hb8Ytm7ns/HDr9avhln3fky+of8FR+9JHYIgihzSOwQhIRstrBlR1EU3PTIRvxk6T/xUeeAX1Mz5ZYn3sHqDzrxh1c/8HXcHnJjEQRR5pDYIQgJ+pgdudjh064HfLamWDGY8rdIUO8gZWMRBFHekNghCAn6mB35MZkyjeWJhP0tDNQ3lPF1PIIgCKeQ2CEICXYClLNlWmU57EMVRN7a1UduLIIgyhwSOwQhgbfmmIma8rXseD8G78LrKyMXHkEQhAwSOwQhgbdcKCb5WNkyLTzoh2UnrYtXIjcWQRDlDYkdgpBgp4Jytkz7QvkRs6MTOykSOwRBlDckdghCAu+6MovZ4V055SR7oj6InUymfC07H3UOYMuuvqCnQRBEGUFihyAkmGVjpTJZbPy4O9fxu4yqLPPCK+yD2ElxajCrAINlYt1RFAVn/OIlnPbzF8tmTgRBBA+JHYKQwGsXXshc9ac1OO0XL+JXL/5Lb/0JODMrldEmEPEhZicjuPC6Bsqj1k4yncWe/iEMpDLUxoIgCBUSOwQhwSz4+JG12wEA/7v8X2Vl2UmX2LJz1/ObccnvX9eJKLPxAG8sO4qi4IPdfaZFHWX0cy41J68jCKKyIbFDEBL0Yse4aNYloroAZdHS4TeptCZKSuHFuvXJd/D0xh145K2PpfvTgggaSpfetPWzZ97FJ3/0PG55/G3br+H7gpVp/DhBEAFAYofYp9m6u1/aOFNXZ0eyaNZXRXWCKOiaO3wMTQjFqR1exP1zR4/0GNGyk/RA7Pw832D11y9usf0a3sIU9HtCEET5QGKH2Gd5d0cPjvvRczjpJy8Y9ikFXFT1VVGdNSfoNPR0pnQuNT7WZXvXoPQY0ZLlhdhxA+/GCvo9IQiifIgGPQGCCIon1rcDAD6UdCzPSgKU+cWzNiFYdoJ2Y2X47Kji5rK3f0h9/MHu/oLjAUAyrY/ZURQFIR8CpRm7e5N4Y2snauMRbg6+DU8QRJlDYofYZzELvgUEwZB/2M11966NR8G/PGiXSYqz7PBWHjfwYud9k3o1orjjY3b+8vo2fOfh9bjji5/AggNbipqLXT59+0vY3jWIY2eMVrcFHTROEET5QG4swncGUxk8ub4dvQE3kORdLyf99AU8uX67+py34rCHe/o0EbCzN4nnN3Vwx3s4URuU0rLT2a+Jus6BFP6x5iPDMWLMDhM7mayCa/66FoOpLO5cvrmoeTiBudtefHeXuo3EDkEQDBI7hO98+8F1uPQPq3HR714PdB5JLpj1nzt6cOkf3lCfy9xYvMVjxebdWPLEO+rzoC07+pid4s7FizoAuP6h9YZjzGJ2lnMCcPqYuuImUiQUskMQBIPEDuE7f3sjZyl49V+7A51Hj4VlKSsJUN7TZ144L2grwhBn2ckUaWba1ZsEAHxiYhMAoHswbUi/N8Ts5IXjmg+71G1V0QiChOrsEATBILFD+I4P3QxssVewYPDIigru7JFnJgHBZ/7wdW+KDNnBrt7cfRlZG1e3ibpBtOywZqB86rdZt3i/IMsOQRAMEjuE7/iZpWNFp0WLA318cu7Jzp6k6fFBZ2PxMTTFWjT29OWuc1RtQjunxXgAMJh3Y/GuwaCT0YN+TwiCKB9I7BC+Uy6Wna5+e24p5hXqtei1VFZurGKzsfLuutH1mmVHvD4x44uJnEG+3k7AWiPo94QgiPKBxE4ZMJjKWLpUKo1ysezwqeQisgDllIWlwCKL3Rd48XHbU5uw+O9rXZ+rcyD3WWyu5yw7BjeW/oIHUxLLTsBig7QOQRAMEjtlwGf+5yUc+r2l6Og2jwmpJMrFstNt01LDNI5VxlXQ2Vh8wLAC4I+rtrn+PLEKyk01FpYdk2yscqmkDAQvtgiCKB9I7JQB/9zRCwB4fN32AkeWjkxWwWV/WI3bn33XtzEZ4TKw7AymMobmlbGINi9FMcbAiM0veTIBm3ZkBRLdCg92rqqYeTZVSjj3YL6CcjJVuno/xRL0+ARBlA8kdgLgjQ/24KSfLsePnnpHt+C+19Hr2xyWvb0DT6xvx38//U/fxmSUg9jplMTrVHOLO2+4YA+tAl6DDoaVdR13u9izaszxqPb1UMiyM8TcWBkuGyvwmJ1gxycIonwIVOzceeedmDNnDhoaGtDQ0IC2tjY88cQT6v7jjz8eoVBI99+ll16qO8fWrVtx+umno6amBs3NzbjmmmuQTgdbmbcQf3/zI/xzRy/ueG4zHnhtq7r9vZ3y0vxesCNAl1k5uLF6JPE6sYh8cWePLcVO0AHKUrHj7lzpfDxOgrsf4uWZ9cbiLTtBix2qs0MQBCPQ3ljjx4/HrbfeihkzZkBRFPzud7/DZz/7Wbz55ps46KCDAAAXX3wxbr75ZvU1NTU16uNMJoPTTz8dra2teOWVV7B9+3acd955iMViuOWWW3y/Hrv0cbEiHd1aOvMHu/0TO10WaddeUw4Byn1DGcM2XsxkJO0iRGuG2WuDYEjixnJr2WHBzomYuWUnJWZjSWJ2qM4OQRDlQqBi54wzztA9/8EPfoA777wTr776qip2ampq0NraKn39008/jY0bN+KZZ55BS0sLDjnkEHzve9/Dtddei+9+97uIx+PS1yWTSSSTmsjo7u4u0RXZo5dbaPn+UNu7BtVfo14LApkbxy8iZeA87ZdUT9b1l5LUrSlnN5YsZsftnJioi0e4DuKGY0TLTu45b2Hyy7Bidp1BvycEQZQPZbDs5MhkMnjggQfQ19eHtrY2dft9992H0aNH4+CDD8bixYvR39+v7luxYgVmz56Nlhats/LJJ5+M7u5ubNiwwXSsJUuWoLGxUf1vwoQJ3lyUCQND2kLbJyy6b27rxKHfW4ofcn2XvGBvgGInhPK07KQk1hxAWzTLWezI3FibO3pdNVtlgdi8ZUfJiseIjUBz93Mo438FZbPu9eTGIgiCEbjYWbduHerq6pBIJHDppZfiwQcfxKxZswAAX/ziF/GHP/wBzz33HBYvXoz/+7//w5e+9CX1te3t7TqhA0B93t7ebjrm4sWL0dXVpf63bds2D67MHH6hFfsz/cddK9DZn/K8Y3T3oFbXx++FuhwClPuSObE3vbkOM1vrAYgtF4ztIizdWEHH7EgKCV523xs45ofPOj6XZtnhxA5EN5ZeYAyplh3jffMamQsPIDcWQRAagbqxAOCAAw7AmjVr0NXVhb/+9a84//zzsXz5csyaNQuXXHKJetzs2bMxduxYLFiwAJs3b8a0adNcj5lIJJBIJAof6BEDnNgRLTtWC2op6RrQxk1ns4iE/WvaWAZaR60l09qQwO3nfAKHfm8pskpO+EXCIZ0by1aAchm6sQB37kpV7OiyscTxchuqYxEMpDKq4OCFh19iR0yDZ1DqOUEQjMAtO/F4HNOnT8e8efOwZMkSzJ07Fz//+c+lx86fPx8A8N577wEAWltbsWPHDt0x7LlZnE850G8Ss8MzeVSNdHup6OYClEWXhNeUg2WHtX6oSUR1izoTDY6zsXy+hyJmC75TsllFvU7r1PPceNXxnEhO5i06KV3Mjl9uLPk4JHYIgmAELnZEstmsLniYZ82aNQCAsWPHAgDa2tqwbt06dHR0qMcsXboUDQ0NqiusHOE7Q/cnjbEjgPdthXiR5Zc1iVEGWkd1H9bGo7qUcxZoK4/ZMRcU6cDdWBZzc1DwMMVdIy92xMtLc5YdICdyOnoG0R9AI1DzmB2fJkAQRNkTqBtr8eLFOPXUUzFx4kT09PTg/vvvx/PPP4+nnnoKmzdvxv3334/TTjsNo0aNwtq1a3HllVfiuOOOw5w5cwAAJ510EmbNmoUvf/nLuO2229De3o7rr78eixYtCtRNVQhe7PQNyS07XltbePeZk8WwFJRDnR12/bWJiK5ysmrZkQQrW92mTFaBoij4zUtb8L/L/4XvffYgnDp7bOknboLZgg8AfckMGmvs/a7hrSTxSBjhUO76RSsNG68mb9nZvLMXR/xgmdNplwSzStFk2SEIghGoZaejowPnnXceDjjgACxYsACvvfYannrqKZx44omIx+N45plncNJJJ2HmzJm4+uqrcdZZZ+GRRx5RXx+JRPDoo48iEomgra0NX/rSl3Deeefp6vKUI4Nc4bV+SVYQ4O0XtaIoaswK4H+8STnU2WGWrbpEFKFQCNG8AmOLuD5A2V7Mzkvv7sIPHnsbu3qT+MdbH3s1dSlWYqcnaT9uhxe+MV2AsnBc/l4wsSO7M6X+WP1i2T9xye9fN7wPZtdOAcoEQTACtezcfffdpvsmTJiA5cuXFzzHpEmT8Pjjj5dyWp6SySo6l4MYoMzw0rXUk0zrzm/VzdsLyiFmpy/vPqxL5P4JxCJhpLNavyynjUCzioJ1H3V5NNvCmMWtANAJ20Kwz2YIQCQcQjgUQlZRDOI7o4odi6+QEgv2nyzN9XF76b1d+OT+Y9TtZmIn6KBxgiDKh7KL2al0+gW3lZkJ3ssv6l09+pgov4Nry8KNNaRZdgAgGhEsOw6zsTbv7MO/dmm9zawsLV5gNd4zG3dg/i3P4Jm3d5geo50nd43RiP5NMmsXwQKUZdj9VGWyiqno18bXzibWFKI6OwRBFILEjs+Yua1EvBA7iqKgN5nGrt4h3faUReCtF5SDG4u9D/VVMQBALJz7p8DEJ79OamLH/D49ub4df139kfrc76BvK8vOfy/9J3Z0J3HR714vfJ789Ufz94NZ4QzZWBm9G0uGXbFx2R9W45Cbn8a/duob4fKfEt71KwoxvrYPDxl2CIJgkNjxmUK/YBleiJ1F97+Bg298Ci++u9PzsawoB8sOq3VUa7Ds5O4FH6CsZWPZP385WXbcnEcVFPk/om5hLj12/2TY/Vg9vXEHUhkF33/sbdNj+OzBiCCWzWN2SO0QBJGDxI7PBGnZeXxdrqr07c++p9vu98JcDjE7zJ1Ym8hZJlgwLnORyOJznLwnKRNrg1eUKqOOxezEVMtObruZG8vasuNs7Jff26V7zn9MeLEjun5J7BAEUQgSOz4TpNgph7HKhYF8+j8LsBVjdrI6N1bur5OWEH67Bq3cWE5ICzE7rI+Z2C6CuelG1sRNO5057Y2VTGdNi2z26cSO/t+QqdjZBz/XBEHIIbHjMywwtqHaOhHOz15LpVoo7VIOlh0WA8IsO6wP1FBGko1loxGoiN+1i6yKCjqBCYdI3qTD3irx0pkoqopH0FAdk57L7t0aWRtXH+/oHpQew2eUDQo/GGR9wQCK2SEIQoPEjs+wismNVfIFgsGK1PmB7zE7ZfCpY5ad2riWeg5obizeKvDK5l3YtqffmRvLZwHZVaIu9qobK38/mCwVP4vsXsTCIZ1Y4bH78dVlvpncY96yM5ASxE7K/1pVBEEML8pg2dm36M0XeGussRY7gH8ixGsrRF8yjZ5BbTEOmTo+/CGTVVRRw2JOYhZurJfe241jb3vO0eLpdzaWm4afMtTUc9Wyw7Kx9Mex3ljRSBij68zEjr17wH/O+fvGZ+31coURRbETRPkGgiCGFyR2fIYtSiNq5AsEj18LppfjKIqC2d99CrO/+7TaJoPPxvIrruLJ9dtx0k+X472OHl1cSG1Cb9lJpoxuLIYzy45/bqxsVkHXoD2xI8a7ALn3aMPHXUimM2rquWjZEZ1SzI0Vi4TQXF8lHctJnR3ZY56uAc6NldLf2yQVFSQIogAkdnzmrQ87AQAHjm0oaN94bO3HOPa2Z7E2/5pS05iPG0p7GEybziqqVeDDvf0A9L/Y/RJ0l/7hDfxzRy++fv+b6OzP1RlKRMOoiumzsZgYKFbs+NlJvnswZXtusmrKD7y2Daf/4iWcd/cqVaQxS5dZzI7qxoqE0dpgInbsurEkHeYBfZ2dbk7MDYpuLOqNRRBEAUjs+MzabbmWAm1TRyIWtb79f3vjI2zbM4CnNrR7MpfG6px1ycuFmV9v2C9y3rLj96/vzoEU9uata/Vc3BTr8M1cIjJjgZOpeikgRcQikVbIxM49L28BAKzcskdtHRJllh2zooLccWMainNjyWoaWc1bFDtJ05gdW8MTBLEPQGLHRzr7h/Bh5wAA4JAJI9QMIDO6BnKL8oDNdHWnsFYJXlpX+EWSBb/y2Vh+Zp0x9uYtOw1VWkYcs2TIemMx7LjcZrbWA/DXsrO7N1n4oDzdA0Z3Fx9MzYJ9RcuOoaggi9kJh1CX0ETj/RfNx5ULZ+ReY3NOstYcIj1Wlh0zNxZZdgiCyENix0fWfpiz6oxtrMKI2rhqTTCD/ZoVv9xLQVUsjER+fL/iS1g8DJ95fuPDG0xrq3jF3rwlpIkLEk8Ilh2ZsCm0eB41bRR++oVDAPjnnhtMZfDQmo8KH5hHZtnhA9STQsxOGNbtIqLhkK6w4FHTR6M+n4puR2tks/pqPPxHkf+c6C07+s+rabsIMu0QBJGHxI6PsAac45qqAaCgZYfFKQykSi9GauJRtZaKl1YImWWHj9n52xsf4odPvOPZ+DJ29eXeh6Zqzf1ijNkxvq5Q1lpDVUwVsH65se5+aQv+uGqb7eO7JYHMvGUnKaSem7WLGMyLoqpYBCcd1Iox9Qkcf0CuE7mTLxVRFNpxYxmKCgoxO8wqRTE7BEEwrCvbESWlN19QsDqWWw6CtOzUxCOGqsH/2tmLv6z+EJd+choaTQrFOYVfb4bUJpv6RWjdR10lGcsuLMZlRK0xZodZCbISJ4xZijMjFg2rAtYvN9ZfXteETiQcKhgDtbfPGN/DV3vWsrFynw2zdhEsTqY6HkFdIopXFy9QjzWL85Ehzpd/nitRkHuuC1BOi9lY+n8fzfVV+KhzwHG7CoIgKhey7PgIi71hGUCFxA774vdC7NTGo2otFSZ2TvrpC7jz+c246R8bSjYOv+Ct/6gLe/uGDIuQX8UTGXvylp2RtQl1WyKae09SFm6sQq6peCSkCki/3FiHTxmpPrYT7N0lidnhhRkTpNpnU94uggmO6vxnORIOqSJHNdzZuAWiBczMVchbdoYESyf7/H5m7jj86svzcNB+DQDIjUUQhAaJHR9hfbHYApHgxA7bJkOMUSgFtYmImnHDFgu2QL+xdW/JxuGXm58vexdHLllm+MXvd0bW3r7cgs8Xw2OWDGYlcOMCiUXCqvvHtwrY3BDnzp9Y8HCZdYp3z4kVlMMmqefMlVQtaQSqaZ3C1y9awHQChW8EauHGYv+upo6uxUkHtapd0dNk2iEIIg+JHR9RxU7cKHasukfLCsE5RfyVW18V02J2hH2l7F2lCGtrMm10EPm9JLFsrNF1mmWHXTNbH93or1hUEzuAPy0jmFD8z09OxfypowoeL5sR//4PCW4s9XWCcGDB5jKRrt7LgrOxH7PDWzdFwba9M9dPa8LImvz4ue1k2SEIgkFix0f68zE7NbFcqBRf54VV8pVRCsuO2IW7riqKWFjfD4oRLp3Wkf66N+uz5AcKNFcO39MpHNbHmbiy7IRDOpHgR5Ybc/vEwmFb75vM2iRzY2mWHWO7iHQmq4oUmUhXCxHauHxDzI5JUUG+RYT4eW3PNw8dP6LadM4EQezbkNjxEfbrtCqeu+186rOVG6sUlh1xUWmoiprGl4RKaNmRLTjiNj+9DSEA3XmXCH//mVBg98mNCypusOwUXu3/8dZH+Mo9q6RZUnbQivuFbFnkZJeVkWTMJSK5z6N2Su0YXnhUST63jtxYgiIys8bw1hz+cTarYGc+y3E/JnYE4UoQBEFix0dYgDLrtD2CS32uSViJneItBKKgqUtEERMClBnhEpp2ZKJBXM/8XJQURYv/aOL6k0XCemuAlU45dEKTdHssElaDvgF7bqxv/HENnt+0Ez949O2Cx8rI5MeIhG2KnQICRG0XEdX3xuLfMyZ2QtC7Yhns82PnbbWy7Ji9nLfs7OxNIp1VEA5BbVsRdpANRhDEvgGJHR8ZSOljdvjUZyaAgFzBPx6z3j9OyAgLbzwaRoRZdoR9pfxQSH+oC4uQn4tSJquo7wOfXi8ukFZz+t2FR+DHn59r2B6PhhEKhQxZbnZY8a/dto/lYeIgatuNZb2fCbR4VN/1nH8dn1UotwLaF8tWMTtmVh6+YvKHe3MVyUfVJdSAe7OgaoIg9l1I7PgIi71hcQ6juABZPqulKqq38pRC7IiLSpzLHPLUsiP5fR6kG6tvSMvqqefaRaiWnSwTO+bniEfCmDK6VrodgKF+kR227um3fSyQizvKZBXOsmMvsLzQrRazsbRGoEY3lsyqA5jX5pEhWnZ44W1q2eHu60f55rKtDcZgcwpQJgiCQUUFfYTF7LBg5FFcgGwVn4Yej6CTq4dSCjeWuKjEo1rquSFmp+jRNArFiAD+Wnb6VauEPr6GLZAZG5adaDikiiMe5vqJhsMAso6zsRRFsRUvtW1PP4697TnMGd+IEXlXXDQSRtjGT5dCAiClxuzo3ViKktsXi4RVy45ZnahiUs/TNiw7fMXkLbv6AGhVyQEKUCYIwghZdnyEiR0WjMy7UfiFVwz6NGt06ATRyhCLhNSYHbENQmljdgrPJYhFiXcbArxlJ/dcsZhUJBzSxeYwmEBw4sbi099ZVlEhHs73wlr7YZeuIWcpLDtqBWUWs5M/59oPO7H/t5/AbU++YyiOKSJzfZlhrKCsVdm2Y9lR46+q+cw6/bkIgiBI7PgI308IABq5bKCYRYHBTFYp2JepEOKiMrI27otlR2YhEXsZlSLbzCl1VXqxY7TsyF8XDuUWc5mwYJYOJ24sXjN93Gktdrbs6sOabZ26bWwIM7Ej9l8rJECYkGDXwua35Il3oAD45fObVTeWGFvGUN1Y1kMBMJZEyNhwI6YyWsFG9n7xl0mWHYIgREjs+EhSCFDms4F4S4EsFkLsB+QUXtCccnArzpg7jrNACAHKJY3ZMSKOVwo3nVPqE6JlJ/dXDVA2WarZQhqNWIidMIuFcubGKeTO+9SPn8eZd7yMjzhRxKwXkXBYKnYMn6UCY7R35c7dks9skhmLNLFjZtlhQxW+frPeWLJ7wU/lnF+/isfWfqweHwkbXZKUjUUQBINidnwkKfQTauLcWJNG1aiPZfEgg6kM6iwKDxaCLQpN1THc9aV5ADTXmaeWHcnPa/HXfNKDdhiF4As6AlzV3/xUzOJFmBCUxuy4CFDm68zYbR76XkcP9/p8UcFISBqzk4iFkS9DA6CwtWXbnlx20wEt9QBYM049fUnW0LZ4N5ZZzI5MqNQlouhNpqEAePVfe7Btz4DaaT3CqTInAdIEQewbkGXHR4ZUN1butvPVZ6vjUTx5xbF46dpPSa0GxTYDZYsqb7VRiwpmsrpf2KVsFyFDFAHJdMbTPlKyCs0N1YIbK2zPjcUW1YjkHsVcxOzwc7NbSZo/jD02q7MjirJC1o6hTBaxSEhtvSD7KKhix6TFiSp2LEfKYWrZkdy66ngEVyycgZmt9epc2fXwQk+rmURqhyCIHCR2fCQpxOzwmTd9yTRmtjZg/IgaafBrsS0jNHO/du44Z9nh09tLqXXkMTtiNlZpgrDNkIkOPjgcAKKC68NMfEUsLDtMPKoWMztuLG6xT2WzuPQPq/GLZe9avkYvkFiAstGNNb25TjrPQkwYUaO+Tiag+oSGtiJaBped6zeL2TG+Nh4N478W7o8f/fvc/PkhdWOFyI1FEIQAiR2fyGYVdUGXLRJjuKyciMQfUaxlh8WP8BYJ3rLDi51SWnZkxgqZ+Bgc8k7siG46AGjksncAWW8s+bmYdpCJCK3OTr7nmHCd/UNpfOWeVfjTa1vVbbxwWb5pJ55c346fLP2n4dy6YnsKL3a0+fBT+sLhE/DUFcdJLDvy6+KZ1lxnub+wZSf3txjLjlieANDuLx8TpIkd7Titzo6NCRAEsU9AYscn+CBcPrDzt185DBccPRmfPWScuk3mxio2Y0lm2WGNQNNZBcmMdv5S/iCW/bqXnX6gSDFnhSyTbUSN3rLDbjlbIM1cSuz+yaxvbJtZG44/rtqG5zftxLV/Wwcgd294IWbVH4u3gPDxRGk1QDmkc1FWRyOIhENq7I2KjfeWL5goE779QtsTETXOx07MjlhUkPUmkwiVREzfr0vhjuc/11oFZbLsEASRgwKUfcKseeIJM1twwswW3bFeuLH4RVEdh2sXwVt2RNdCMdhN/+3nKhuXGpllRxQ74bzwK+TGYou/LGMtJqaeC1lmovgR58UPKRYY1Lmu+MadaXmdHWYcXPK5OUg8sgEf7R3Amm2dyAgKRCYE+QwuaTbWkD6rUEQTI+6zsWSWnYTar4sFQCuq8NNlY7H4K8o9JwgiD1l2fIKJnVhEXn2XJxopvRvLKmYnk9W7sUq7SNg7l7eWHYnYqdW7saI23VhWlh12P2Mmbiy+YnYqkzXMi7/vYjo+L4z444YyLEBX78Zii/+Y+gTu+OInMHd8Y26HcF0bt3cbroMXTbKKzkyY1piIHSe9qQy9sSwqWDOxw3SNomjHx8iyQxCEBSR2fIKJFbMS+zz8QhrLWwmKt+woxnNHuQDlTNZwbCmwe6pixZwVshghY+p57q9VgCxgN0A591cULHxQ9J6+IWNBPW5M8X7wjVx1Yifv3hQtO6JeZtYQ/ro27+zFZ/7nZcN18Jcmk+W9SX3bEyP2U89F65eWjSUTO5H8/LRsL5mIDzvIBiMIYt+AxI5PMNN/Iir/NcwT42J2WO+jgSLdPFrDSM6NFZZnY5XSsmP3xzWLA/ECmXgTrRLM9cHmW8iNZVVnR7XsCAs5byXZ2ZM0LPT88VaWHb5YIXtNOCSKHWF+EtXy3Dsdxo3QW3ZkRshVW/YAgJoCLuIkvl10mTK3muwjmIjp+3VldQHK3LVTI1CCIAQoZscnBgt0iuaJcvEH9VVRdPQki7a2yGJ22GNjzE4pLTu5czVVxzCiNoYtu+TdvZ10CHeKLC5FjDcRa7MUcmNJxU7+fWMWM1HM8ALqza17cft7u3T7+5OaoBUtO7wo4Du3M1EUFYoKihl9WqE9bQ5mApMXK2YVlCePqsEx00fLXw/jWGaIVabZZcosa4aSDYp2vM6yQ3V2CIIQILHjE+wXqJ20bj4bqz7fw6l4sSPJxopogZx6y07phAdbb0IhvYgzzM9hh3Cerv4UXnt/Dz55wBhdQ1X13JJ7VyVY2MQWA2YLpZp6LisqGNVnYyUtGp5+5+ENhtf3DvFiR7R4aC8e4EQKb9nh5yRJ6AOgd+1s7xqQHhMpELMDACfOajXd56SooChEmStPZl2sUpuT5p5noXC9sWQxOzYmQBDEPgG5sXyCffHaqfHGi4LaRC7Oo1jXEltUojLLTjarW5iLER4iTDSEEJIGXovHueHf73oFF/3+ddxuUoxPdj2iZUdsHmk2HcuYnbC1G6vQ8s9bWgwxO1m5RYZtjYZDOvEhzk+NY+Gm8OEeudjhM83MPq5WFkrNsmN6iErKtOu58Vhm2eGvxdKNRZYdgiDykNjxCZaGa/ZrmIdZBhLRMGd9KTJA2SJmx2jZKf0iEQrpM2YM8ytizHc7egEAD775kXS/GAgMGJtYihk8hQKUZe8jy8Yya0RZaO3lLTZWMTtm8+JvrygsZQLko0652OEvzcwSKasFJb7eTuq5MSMt/9fCjcX3vmL3WNcbK6wXrgRBECR2fIJ35xQimv/VXB2PqItNsW6slGrZ4euR5P5mFXgesxMKGRfIueMbMTHfg0kWV+MUs3nLLDusPxlDjdlRY0bkY8jcVwx2fSFuMeYpdFsHbFp2pPMS6+yI01RdS9p5dvQMQkZI58aSjydLvdfGtp+NZWgXobAAZfP3TK2zAy1Amf9sqW4uUjsEQeQhseMT7Lvbnhsrd1B1LMIFzhY3viz1nLdA8GKnlOZ/TeSFDDE7P/r8XLQ05NpklMJ1Zi52jEIqHhEDeO3F7FjVSIqJ7QyE/YUsHVZurEKFHiPhkGC1E91Yub/suroHU+hLygOUeU1qZomUxUZpL8r9sRezI7fsyIRKtVhBmbfsSNyz5MYiCIJBYscn+NiVQrCFJGfZyW0r1vLBLDuRiHFRUBT94uqJZQdGy04IfNxQCcSOyT0S40IA4yJutzeWtdjJW3ZM6swUtOxw78Gg6MYqIAYj4ZDOCiNmY4nWpt5B81IGhVLPAXkFaXUssLFsuLHE1POs+f2vihvFjibiOYulEH9FEARBYscn2PeuHTdWjLfslOiLu1APIb73Vknr7OT/hkJGa0DO2lNCsWNyDjHeSeaCCQtioFDXcxlMQIVN/Fh2Fn/GwJBo2SkgdgrU2QkJriWrs+ncWCbHWMVfiWNZYUw9N3dj1QgByllduwi5xZIgCAIgseMb7IvXTup5WOLGKj4by9qNlUx5E6CscBYtmWuFWSBKUWfHzPohLqgywcLuhVW7AkC++F/6yWlYfs3x2jH5g2RBtgAwcWSNaasFhlhE0k7MjpUbS7W2sL/5ucUjYSw+dabuWF0FZdMA5cLZWFbs6B7ErU+8Y0h/VwOUJdfblO9nxk/Jyo3lRFwSBFHZkNjxCfWL18ZKMGe/RoRDwOFTRpasqWFKknrOB5IOehSgrKuzI7qxuNowpbHsyAWTKILklh3B8mEynQ8lGUz/Pm88Jo3SOoWb6Vn2Ho6ui+PnZx8qPyiP0bJTOGZH78YS73X+gdroVNt+tFAcUN8bSz5erMhsrKv+tAZ3Ld+MR97anpuvIDZl939cU7VufrkKysY5U50dgiBEqKigT6gByjbUTtv00Vhz40loqIrhqj+tAWBuJbCLGtsQkcU2KLqGkKXMYslyGs8YFKzFEGVKEaBscg5RKEQkC7W6QBbojbWjO6k+XrH4BOzsSWJ6c53uGHZ28TbywdqFKmn3O8zGMvbGEi07+ffaMBejtZF/qWnquUWBSLOYJZ5V7+/RPY9FQ8ikNLeU7PM+trFanTODApQJgrADWXZ8IsstLnZoyDeqLJkbS5aNlX/3d/UOYenGHbpjS+UCUN1YIWNRwbAuZqd4N5bZjMXifrKF2m67CJ6xjdWYM77JsJ1vZ8CjuTKNdX5EBh3G7IQLiR3hc6fWfULIsC/M3R/T1HNblh1zmHBhMCFs5UZkXeO1pqZ8UUFu/g5ihgiC2DcgseMT/KLvBDVLqFixk5bU2bGYS6mMO7zIE10fuRYSpXNjmSHGA8ncWKGQtoACxniPrx0/Da0NVbj1c7Ntjyu6cTQrV2HLjtNsrGg4ZFkDyHh92j4ry45Z9qB1oLZxDJFRdXHdc9ZPzKrrOfu3wA8ti4WjAGWCIETIjeUTvDvHCaUSAynVjWVcFKTHZ7KIhAt3aC8EW/DDoZA0G4stYKVsUSEiBijL4k0KWXYOHNuAV6+bKb5MiipQDXuY4C1s2RlIiQHK1pYvq1RwgPvcsZgY9rqQ0XpjJ2bHqqig5sYyf0/FJqmqZcci9Vybkza2VZahF5XACYIYngRq2bnzzjsxZ84cNDQ0oKGhAW1tbXjiiSfU/YODg1i0aBFGjRqFuro6nHXWWdixY4fuHFu3bsXpp5+OmpoaNDc345prrkE6bV5DJCgUzoXhBDFLyC3SHkLCu3/qwa2G44uFn3ZM0om7lG4sM4YMlh3jx14MahUXaitLhuFckJ+D3dNQyLq3FCBpBOowZsdQR0i9PsFNFDJ+Ju3E7Ih1fHjsuLH6BDddXLDsWH3++Pmpn2tJuwivDDtd/SlvTkwQhGcEKnbGjx+PW2+9FatXr8brr7+OE044AZ/97GexYUOuI/SVV16JRx55BH/5y1+wfPlyfPzxx/jc5z6nvj6TyeD000/H0NAQXnnlFfzud7/DvffeixtuuCGoSzKFfe86dWOpdXaKjtkxZmOJc4lzC7CVpWXDR124fdm7kkaXRvjK0aJFJcxVVS5B5rkpaTFmR5I2rcV5yC0LdkoGMMwCdDV9EbJh2RFidgrcILHruWFOQidyfi6ivVHXCNSVZUc/hox+IbWeiT8mwkShOCKfdp6bkzY2EzthnWXHOzfWr17YjLk3P417Xt5S8nMTBOEdgbqxzjjjDN3zH/zgB7jzzjvx6quvYvz48bj77rtx//3344QTTgAA3HPPPTjwwAPx6quv4sgjj8TTTz+NjRs34plnnkFLSwsOOeQQfO9738O1116L7373u4jH47JhkUwmkUxqWTXd3d3S40oJ3yPKCVr/qiLFTl688K4kcXHk91lZWr52/xv4YHc/Nu3owf988ROW4+p6Y0VFN5bmVitFbywzRMuOzI0lBrWK99tqcTegxqzoz6G6jsKFLTt83aMd3YNqirYZ0XBIZ/0RZ2uMo1HU48RL48WEWfagZQVlGx9yMbU+Ec2JP5kba/yIatx5rvY5408vc2OVqsWKjFsefwcAcNMjG/HVo6eUfgCCIDyhbAKUM5kMHnjgAfT19aGtrQ2rV69GKpXCwoUL1WNmzpyJiRMnYsWKFQCAFStWYPbs2WhpaVGPOfnkk9Hd3a1ah2QsWbIEjY2N6n8TJkzw7sLyaBYOh5adsN6875a0jZidKNc528pt8sHufgDAo2u3FywGqFq0IInZARez42EwqXgtsr5OoqgUp+PIjWXixuFbZxSy7PAVreffsgzL3umwPF5sBCp+zERrC596bubyAoCQyTeEpWXHROzx9Ju4sdhbxdy2U0fX4qVrT8BsLuuNv05WskBWZ4eKChIEwQhc7Kxbtw51dXVIJBK49NJL8eCDD2LWrFlob29HPB5HU1OT7viWlha0t7cDANrb23VCh+1n+8xYvHgxurq61P+2bdtW2ouSwC90TmDrcvEVlI3ZWOKCyFfhPeP2lwyuBsbU0VoBvTXbOi3H5S07cYMbS2s74KVlx5CNJQtQLtAItFAAsAxTN5aNOjvMsrNtT3/BccJ5wWKVRaVarqC3nIiv44/NnUeOZTZW/q/ZJzaZzhgEqOrGEuocycbht1i5Zyk+mSAIRuDZWAcccADWrFmDrq4u/PWvf8X555+P5cuXezpmIpFAIpHwdAwRxaUbSzXJe2DZEReScCgndlIZBR09STy85mOcc8REy/N2FgrW5CxacWGBz43HFjk7V+EOMf5IZtkxpGYL+63iYUTM6rwwa0WYy0IzYzBv2Xl4zce2x3MiQBTOjWUURtxjk3PaseyYIeu2rgYmM7GZNf/3orPsSESRGH9FEAQRuGUnHo9j+vTpmDdvHpYsWYK5c+fi5z//OVpbWzE0NITOzk7d8Tt27EBray5rqLW11ZCdxZ6zY8qFLLfoO4GJgXSR39tW3aHVsYQgV7P1jP9V3jNgLXb4rB8xMFgXs1OE2ilkdBEtO1ap54rqxhItO/bnExKsKCoOFt+dPUmkM1m8JlQalsEEia6Bp+jGEjKU1KmEZIKiSMtOgaJ+rON6VUy7qbt7hwAYY3ZkIpPfpAYo+9wuwolbkyCI4Alc7Ihks1kkk0nMmzcPsVgMy5YtU/dt2rQJW7duRVtbGwCgra0N69atQ0eHFs+wdOlSNDQ0YNasWb7P3QqrX6pWsHU5W6Tpg8U26GJ2hHc/HNbq8QBaFWcRXjx0DlqLHb5NhtguolS9sQotPOK5xXkAxgVSnI5VewSRkHou/Uk0wVv4HHv7U3hifTt29yYLHmvH6qTF7OjdRGFZBWXesmOael7YimQWVN+TzH1mqrm4pV3562SWGi1N31rsMKud313PHQWsEwQROK7EzgknnGCwuAC5rCaWOWWHxYsX44UXXsD777+PdevWYfHixXj++edx7rnnorGxERdeeCGuuuoqPPfcc1i9ejW++tWvoq2tDUceeSQA4KSTTsKsWbPw5S9/GW+99RaeeuopXH/99Vi0aJHvbqpCMHng1LKjNQJ1N66iKNi8sxedA7lfzvwCI7Ps8OnkiZj848GLh26blp1cBWVJnZ1I8WKnkBARrTSsoaR+LvoF0lhnx/58xGBghha3Zf0Z+GLedfjAa9sKuwlhz+ok1r7hLTvGCsrmFiKGHcuOGcyNVROP4qSDcjF2Fx2by2xiPwoU1T1lfD0/v6zkODOxWUqs2mUQBFF+uIrZef755zE0NGTYPjg4iBdffNH2eTo6OnDeeedh+/btaGxsxJw5c/DUU0/hxBNPBAD89Kc/RTgcxllnnYVkMomTTz4Zv/zlL9XXRyIRPProo7jsssvQ1taG2tpanH/++bj55pvdXJanuI0fiBZZVPDvb36Eq//8lvr84P0a1ceGRU5YwEwba3LKq5DYUdfUUEgSoKz1xiomJqnQr2x25imjazBhRA2uOfkAwzFq1WOTbCxHdXbUbCCT/QXESdu0Ubh/1Vbs7RtCVwHLGWDi6jHMKe9ayhqPM/bGMr5OxLoRaA6zd7Q3b9mpiUfwv1+ah87+FNZ/3AVAu/8ZxcKywz2WuYe9TD1nOLH0EQQRPI7Eztq1a9XHGzdu1GU8ZTIZPPnkk9hvv/1sn+/uu++23F9VVYU77rgDd9xxh+kxkyZNwuOPP257zKDISmIL7MA6dLv9lfp/r3ygPo6GQ5g1tkE7dyGxY7Ja8CKoN2ldrVoNzIbW/4jBW3uKcmMV+JXNPIALZrbg+k/L3ZthQaCI99tJjIZYwE+dhxa+ZMmImlx9qO7BlBrfYoUsiNg09Rx6MReSWHZ4y5PZkl5Mbyx2TbWJKEKhEEbUxtXPItPRVjE7sn9DfgcokxuLIIYXjsTOIYccglAohFAoJHVXVVdX4/bbby/Z5CoJp13PGap7xaUYOHBcPdZ82KmOzWdEiRYGcWExFTt8gHKBxZivoCz+GuazkjJF9MYqbNkpnPYvpp4XU2dHPVIsKmgRh8KIR8Koq8r9s+zoSVq2XFDnJj2faEXLzyH/PMtZToxWIP6xmWXHKmbH2rbTlbcG1iU4l6rBsmYe41YoQ0tzY+mPURTFcQVzM8iNRRDDC0diZ8uWLVAUBVOnTsWqVaswZswYdV88HkdzczMikeKbR1YiWndmZ68r1iTPD3fUtNH6c4sxO8LkxGaN6nbOjVXI8lCo67laZ8dLNxabg404k2wJLDuqQBXnoe43f20sEkJdIvfP0k47DsBmDSAhQ4oL2ZEUFSw2Zge6sUR68zE71XHt60cssZCWFAvUzu/csvPIWx/j+ofW45df/ASOnjHa8HqnUDYWQQwvHImdSZMmASg+M2hfhC8o5wS2kBfqem1Gklswf/T5Obp9xsBU/Wtl6eCKouiESUE3FjQLgph6Hua2ZcVgEgdYNaXMnduGZYe7eEVRDOLSSZ0ds8VejUOxmEksGlbFjl0KpWcDRncULwSM2VhG4WAY047YMdnPPssRyThqnR3248BknHBI/wNAno2l7f/6H98EAJz321XYvOQ007nbhWJ2CGJ44epf7O9+9zs89thj6vNvfvObaGpqwlFHHYUPPvjA4pX7Lpplx2HMTpHZWEzsXLFwBprrq3T7xKmIC5jMsiBaYAqJHT5ORbTAhKBZe0qVei5z9/HxKWaInbTFGjlOKiiHJJYFfh5snWSLfXO9ljkYj4RRm3BmHeWnNnd8I6piYRw9XW+90CxXeTeRul0Ss8O7sUzGtOPGMouZEe8Dfz61XYQa42Y6jA692GHjGMd3G+gvQjE7BDG8cCV2brnlFlRX59J3V6xYgf/5n//BbbfdhtGjR+PKK68s6QQrBSYbXFdQdvklzcSOrBeTKG5EC0lKIhzEAn1ijyMRhRN54gKh73peGjdWSmaN4sYzgxczWcXoNnRk2RHG1c6rtzAtveo4fOOE6fj26Qeqx8QiYdTGHVp2uLk/+LWj8daNJxmsQyFhUvz7YhWzY2bAKMayw8cLiedjYpV9Hszuu6xsgjZ+ca5fO5AbiyCGF65Sz7dt24bp06cDAB566CH8+7//Oy655BIcffTROP7440s5v4pBLSro8HValoq7b25mnZH1YjIsGMIhsn5VKSGQ2Kx/FoMvpigGdfLbSmXZSWUUiF4gOx3nxdotxjo7btxYcssOW4ynjqnDVScdgOc2aUUx49EwwuEQqmMRDKRYPZqIKirjkbChizsv1MLhEBJho7DVtI4xANuyzo7JJ9Yy9dxM7eWRpYuLdY6sUs9lhCWWHaqzQxAEw5Vlp66uDrt37wYAPP3002pdnKqqKgwMDJRudhUEX2/GCcVbdnKLZJVU7IjPQ3jg4iPV52KdnfUfdeLzd72i28YWZDP46xYtR7ltxYk5QL/wnPzTF0wFmJVlJ2IQO8J+F9lY4hWZfQb4sVl1Z77449hGzf04qi5uGM+O1UlrBCrMhZ+wcKz4WHeMxTeH6sYy2S8L1hcD8ZlINrvvRqFutBKVWurw4pVidghieOHqX+yJJ56Iiy66CBdddBH++c9/4rTTcgF/GzZswOTJk0s5v4pBkXzB2yGiBvC6G3fIwo0lph2HwyEcOW0UTp+d6ysmWhA+ffvL+OeOXt22wVTWUqjwFq2o5Nd3KYoK8lfxUeeAoXmmZl0q7HoBcsKrFNlYhgrKJnEo/H1hpQGq49r7NXmU1mV+dJ2xMriduanp2Pm3lLcyiS+348ayY9kxi9mRBYwzq6Lqxir070WMN5MINH58WT80p/BWTScVtQmCCB5X/2TvuOMOtLW1YefOnfjb3/6GUaNGAQBWr16Nc845p6QTrBSYbnBs2Smyzo6V2MnNR3vMFl2WIcVbdkSXFr9AWwUp8/2geAsMuw+REsTsiIuqeC41XsriHHo3ljcVlM3q/fAuGOZurOGClCeP1sTOwgObMa6xSmftsSV2hDnwrj1j6nnB0znqsC6i1V6ycGMVEKgGq6S0XYS2LS6xbDqFWUmBwhmABEGUF65idpqamvA///M/hu033XRT0ROqWFzX2cn9dZtFwqwzZl/24VBI6y+UXyWYkOGDkVd/sFf3uupYBIPpDFIZBb3JNBqr5U1Ddann3ALBbgO7vmJidsRXGlx2Nu69mNElntNJ9o3mMhJEl0n5gahE7PBByhNH1qiPaxNRvHTtCejoSeLIJct041nOyaTreQjG+xKy4cayzMYqGLPD0sq1bao7U0091283jCFIxkKWnUQ0ovbkcgufnegkYJ0giOBxJXYAoLOzE3fffTfefvttAMBBBx2ECy64AI2NjSWbXCVhp8aKjGIrKGsBynLLTu78+vgIWQuHN7d16l4XjYSQQASpTBoDFhlZGT5AmVu42FpRGsuO/rkYRyQLiBXh11SZ8HKUei6My1Ak7htAv6Cz96maa8I6cZQmdmKRXADziFpNXBaKm9LPKj8XToSKn0l7MTtWYqdAzE7WeG4tGyv3nAltM1FlVTZB5kaUBeg7Re/W9TDViyCIkuPqG+D111/HtGnT8NOf/hR79uzBnj178JOf/ATTpk3DG2+8Ueo5VgSy2iJ2EH/xOoXFGZhZdvhFg43F/vKuq0FhQY1Gwpb1TBi8yyKiEzt6K1IpM2fEFhZ26uzw8UuyYoqOApRVK4o8G0v8DPDnjudFDl8Mcj+uSzu7X7x47StQ6wgw9v4Cd0/E+8I/dWPAUA07pnV2jMJfDMRX6+zYEDshiNYo6M4F6MUO745yAm/ZKaK7CUEQAeDKsnPllVfiM5/5DH79618jGs2dIp1O46KLLsIVV1yBF154oaSTrATsVPGVIasG64SUDTeW+Fhm2RHr60TDIaRChYUYHxsS46I62YJUimws0V3ULXQKz4K5sazvfiiUEwOybu+lqbPD9gsuGIkbi7fW8JlZYhVqwJ7YMcbsaNstKyg7/sTasOywc/NuLJOYHbP7rmtWKharlPyb4T//fcmMqaXTCl6AetlklCCI0uPasnPttdeqQgcAotEovvnNb+L1118v2eQqCTuuFBnsi/y9jl58+vYXbfdLymQV/PrFf6GjJwnA3IzPrxMsgFjtV5XhxY7+yz0WCRsyfGSo1gwIlh3orUjFiR09f3n9Qzy85iPDHArdefbeyN1Y9udjGrNiUkWbvy9V+UW4n4svSXAuLVlWUV+Bwo65OQkxO9BEqGUF5aIsO/L9WlaaUbDkgsMVTezYsOwYSyiw8bUJ8HOx00leBv9vz8uChQRBlB5XYqehoQFbt241bN+2bRvq6+uLnlQlwr4bncR+APpftus/6sbTG9ttve4fb32EHzz2tvrciWUnmj+Wt+bIsrHEDBoZWU5p8Au1IsQJFVXGX3jp7r4h/NcDa7C7Nyf01LT/AvdeFTuSYopO6qqIwoKR4axc+nNzlp28sOHfLzdWCOOccn9VLxZnZRLvip1GoHbGMkMm/PV1jjjLjsltl8X7iM95QcK7JnuSesufXXjLTjGlEgiC8B9XYucLX/gCLrzwQvzpT3/Ctm3bsG3bNjzwwAO46KKLKPXcBLdfjuIaa9cC8q+dfbrnZpYd/S9kfRwNb+FIChalnGXHhtjhzq3LeMq/ROyJ5Aazl678157cfhsxO4B2r6WWHQeLvmpZMJmZMbhWe2+YZedH/z4HLQ0J/PQLh+jeO7dZa6K1g3cvWll23GQdaUUF5XPNQhPA6vy4G5zJKlpRwZDJ55Z7bFYBmh+f169uLTt8rI+X1ZkJgig9rmJ2fvzjHyMUCuG8885DOp374ojFYrjssstw6623lnSClYJiM25ERFxs7L5e7I1kx7LDfkVrMTu8ZUd0Y2nF6KzWX4VbVHnriFght5R1dhgvvLsTp80Za7vjPFukZXNxUh+JHSmexsyVyb/HVXnLziETR2DldQsB6K+Pn1sI9nOCjG4sbbtVzI4b045ZnSGG+t5LApSBnJBQiwqa/ByTiXRxn86yw6kdMYDdLuTGIojhiyuxE4/H8fOf/xxLlizB5s2bAQDTpk1DTU1NgVfuu2SNP2ZtIZro7a49tYLYSUSsUs/1j1nMzmAqi+7BFBqqYoYA5VgkrB5vWUGZi9nhiwqqtX1KUEHZbFFduSVn2dEClK3Pw/aL1+oYkzfJTJRFIrzYkVe6ZvCWnfqqKLptLtyGoGFdgLIoqOWPnWL2jmouXW0b78obymQLx+zwAcomYo2/3/x96xpw58bSix1SOwQxnHDlxurq6sKePXtQU1OD2bNnY/bs2aipqcGePXvQ3d1d6jlWBEqBX6pmGDJNbMql+ipB7MQcuLHyVqBn3+nA3O8+jWv/+pahA3qUs+xYp57zlh1zN1Yxlh1t/vrnO7oH83PIPbcboMzPpaUhgU9MbHI2j/xfcUHUspAKW3bMyHBC7OsnzAAAnDirpeCctKDh3JwUwZXEz4gXP25idsKaH0+K9pngYpWiYfXad/cOqeIkaubGMnGBAdr7mMooeHdHDwD9e7H4wXW44eH1jgV2ksQOQQxbXImds88+Gw888IBh+5///GecffbZRU+qEpEVUrODMR7B3utEC0HcJNKTXyjUooLcNgXA3974yBC0G0JIXayshAofLyNzBbF4lWIWD7Zwf2PBDN125nqTtSeQwXbzmWevfGsB/nbZUY7mo16n6MZiWUjC8bpsLJO2HgzeQnHhMVPw6NePwR1f/ISNOemnpH4e8zLHzHPl9PMK8G48k5gdyb+FUCiEUbW5vl/tXYNcnR2TMSzmyAvGi36fyw7l79tQOovfr/gAG7c7+2GW5MoBkNYhiOGFK7GzcuVKfOpTnzJsP/7447Fy5cqiJ1WJyAqp2UGM2XHzSxuwKM4mOXdMEEbprGJIec8qijTrRaRQE07VslOM2Mm/VIxTSmWyUBRFakmQERasTCHkhIjTfmaasDAJULaos1NdQOzwAiIcDuHg/Rpt9X1SxzTE7LC/Rncmv98JhbOx5Flpo/Md3du7BtT3wCwLzqqlRVNNHOccMQEAsLdvCID2nv7pkiPV4/ptpOzzDPJFBSlohyCGFa7ETjKZVAOTeVKpFAYGBoqeVCUiK6RmB/F4uwuvXe0gS+GVFa4TC/VlFIULBLWw7KjjmIxfwjo7outOyZ9XsRuzkxcEqbzpwbWwNCmqJ+sJBdiz7LD+WCfOanU1J1GAia0r+EvVx+y4sezIr5+h3gfh1C355qYfdmpiJ2LSrdxsvoz/PG4aAM1Kx87X3FCFKaNz99JuzSoGX0XcTMgSBFGeuBI7RxxxBH71q18Ztt91112YN29e0ZOqRMz6IhVCDNC0bwCxdyB/emZFiksWGDEQNpNVbNXZKdQTrBQxO5plx9iMNJVRTBtwiqip5xl3mXMMLT5GPk8RWZ0dkaVXHYdV1y3AFK4DejFz4rOxcn+1Y+30xrIcq0Asl5lbcVxDri3Gx50D6ufGtIIy/7mVqJ24UCtKjQEKh1TLpdO2EbzYKTaGnSAIf3GVjfX9738fCxcuxFtvvYUFCxYAAJYtW4bXXnsNTz/9dEknWCnYXXBFxC97u7EtdrWDPhjV3LLTI2Sw5MSO9tiMQj3BeFeYoiiO7w9PXZXx4zyUztoOUGZjs/gkt1Mxs3ixp+J7quuNZZI1l4hG0Nzgvrigsaig3pWUu3ZZyr2LsQrs1yw7gtgZkbPsbO8aRH3eJWmWjVVIkDGxk87m3JjMnRqNhNS6RWLtqEIMpqhdBEEMV1xZdo4++misWLECEyZMwJ///Gc88sgjmD59OtauXYtjjz221HOsCMy+4AshxtrYLSpn342lPda6njuz7FiNVagnmCxDyyls4ZEFYQ9lsrbdWGx3ukCcUSHM7otWTdpc7JhZdorFUGdHEIA6t5Ckg7izwfRjiKjlCIQ3ZMKInHupozupWXZsRORbWXaAnKhJczFA8XzhxqStbvEa/PEUskMQwwtXlh0AOOSQQ3DfffdZHnPrrbfi0ksvRVNTk9thKgYxINQuohVA1spAPp5NN5ZuYcuPKTHDiM0mebFjy7JjcuH8QpXOZhEJF2e9uPDoKXh/dx+W/3NnLrA6k+WsS9Y3Xyxw6LbGjNl7zPej0o0rpGB7gebGYqnn0M1FbynhXudK61jH7Ji5dJsbcpadXb1JTBiZc2nZs+wY9/PCl3c/xSIhdd+AQ7EzSKnnBDFs8eabNc8tt9yCPXv2eDnEsEHW/NAO4pe93YJ3biw7TAxEJZYd8XS8G8teb6zCYsdt3A7/qu+cMQt3f+VwLWYjndVqHBWsoJyD3WOnmXPimcxidgxF8MLaSKXogyWdkRA0zffGEik2Zoe/Ppm7Jys5DgD2a8oJnF29SdVlFDURO1Z1dgC92OEbpUbCIdV6NuhU7FDqOUEMWzwVO+TX1jDLQCmEuNaI3cfNsHvnZc0YzRYYnoyicJ2q7Vh25Pv1lh2XYkcSk8OuYSiTtV29WswMc5o5x2BaUbSuWcVtsfvgvWWHzUVvZdJZc7jXuZkNf32yj4ai3l/9fWhpSKAuEUVWATbv7JUeI5ujLIg5HA6pn4GBIc0qGYuE1XvMx+DYYTBFvbEIYrjiqdghjDgOUBZjdmxbduxmYxl/xdvp8K237Jgfp4o8k/2yfllu4e8ty7gZSmcNmUdmqJV31eJ/7iw7Zl3PrTLyxjZWoToWwZj6hKsxC88pPweWeq5uZxHK/LHcZ8KFL49/hewdZW+zsYZUCNPG5LLNPtybK2Fh2i7CxhzZZ6AvqbfssPT+QacBytQbiyCGLa5jdghnaBVrnSEuCGLbBvPx7B2ndwfk/srcWCJ2U88LCQ1+nXJv2THGwrBrGMrwbizr86jZZSXKxhKvhj2XuYYe/caxSKYyhp5mpcI0G4vtN+k1VWxRwdw4+pMw+Sk79axxDXjrwy71uR03lll6eiwSwkAK6OMsO9GwFrPj1I2VpGwsghi2kGXHJ9QveIe/lA3ZWC7cWItPnWl6HL9QqEUFLebI6rxcsXB/e41As0YhwhMKaT22io3Z4Ydgv+pTafsBymrqucv4KvU8bF5ibyyL8KXG6pgaoOsFYgVlq/dFbOPgeiyYWHZM3FgAMGe/Rt1zs8+iPtZMPg/2GejPW3ZYNWxm2XGcjZWmmB2CGK6QZccn7FoXRAzZWFlnAcrzJo3Af35ymulxsrL7Vm6sH39+LprrE5gwsgZ/fn0bAGsrkpU1gxEJh5DNKK4tOwx+iBhv2VH3F3Jj5f6yuKhiW3MYFkSX5QdKgmjZYZslc+E3uZqpzrJj3G0Vsz69pV73XJYZmBvCGGsmorqx8pYddlyVyzo71PWcIIYvnlp2jj32WFRXV3s5xLCBaRSnv5RdW3ZsVmzmS9OoYsfCjRWPhDEh37rAVlHBApad3Bzy7rAiA5T5q1UtO5lswVo/DHb9WlHB4kSJeDVmPaH8QLQ2iUHdfDB10TE7vNiR2HasyhGIfdncZmPlzsUClDXLDgDEXWZjUddzghi+uBY7mzdvxvXXX49zzjkHHR0dAIAnnngCGzZsUI95/PHHMXbs2OJnWQFoFg5nrzMEKNu17NgcT2/Zyf3lFxixwCAvhFQ3llWAsnCsDPaL23XMjqR+jS5A2Tr7XSUszMN9nR1m2dFfj8zd5hdioUNZnJN2LPe4iLH48XiYrU0mUkRxI6kTaRjDzLLDyg/0CWJHdWM5tOzwbiwKUCaI4YUrsbN8+XLMnj0bK1euxN///nf09ubSRN966y3ceOONJZ1gpaBZFxxadoTD7Vp2NCtCIdeNJGaHW2FG1sZ1x8dkYsdCgFktqk7OY4Us9ZwFoQ5lFDCZUch9xDwmatdz1xWU8/Mymacba0mxiNlYGfWeGedSdG8s7rFU7FgE64tWRTM3Fj+I2f2MqzE7OTdWtEixw7uxKECZIIYXrsTOt771LXz/+9/H0qVLEY9ri+EJJ5yAV199tWSTqyTcLnTiYuNpUUFJnZ1RtfpUaN7NwB5ap57rzy2DiaziY3aMFqlkKmO/zo6aes6KCrqch0lRQbvuNC8wzslchNpxEVmOVciNZSE+xXgx0wrK3F00c3XF8pad/pRg2Ykyq58zN9ZQhlLPCWK44ipAed26dbj//vsN25ubm7Fr166iJ1WJlKrruV1BYDc+RPcrXlJBWbTs8FafkGqRsYjZEVb8SMjo9lLTxB3+0lbHYPPhtsW5IFTNjWXPypXJT9BOXyYZqhXFxI1l5nbxEmPquX47r0l02VhuxuKzsaQByvbdWGYiWSfIzNxY+c9qb76vG3s/qwtYdgaGMnhj615ksgqmjqnF+HzPLgpQJojhiyux09TUhO3bt2PKlCm67W+++Sb222+/kkys0jArkV8IcWG0m55t1Q6ARxb7wC/yBjcWt0/tJWXVLoKNkz82HA6pYoJRE48CGEKv0H/LLrKYHGaBSqYztvuSsUtTG4G6mg0f9KtHTfcOwo2V/2uooCy5Sr2QMJ7rvz8/13osnWXHiGbtM+4zurHMxI7xcyiixuwk9WKnKp4TO2bi+ht/fANL387FISaiYbx2/UI0VMV0lh3SOgQxvHDlxjr77LNx7bXXor29HaFQCNlsFi+//DL+3//7fzjvvPNKPceKwKxEfiGKrbNTqBhyWJeNlfsb4zaOqjO37DBXgp06O+xVMutKbX7x6R10J3ZUlwyfjcVcFamsbataWHWnFZeNpVl2ZLMMxo0lDqp1HjceKrP2MZZeeRzOmjfe9rDS3lgWKfhGN5b8vOECggzgA5T1MTtxLnhdxjvtPerjZDqLrbv7AQCptHYtZNkhiOGFK7Fzyy23YObMmZgwYQJ6e3sxa9YsHHfccTjqqKNw/fXXl3qOFYHduJFC2K+z4yzdGtAWtgj367qpWhQ7nIsjrB/LCiYcZC4cVjW4x6XYkVl2EqxKLl9U0KYbiwlK9wYY+QvtZoV5gRazo+j+yqbCbxMFnx2xXsiyY+VWFC07pm4s7rGZZYf1wOKLCgJAooAbq7M/lR8793x33xAURRFidkjsEMRwwpUbKx6P49e//jVuuOEGrFu3Dr29vTj00EMxY8aMUs+vYmBBmcXGa9iP2cn9LeTGCkncWLxlp7Fa/xHh96luLAtrk9gAVWZJqKvKjdE5MGQ5VyeoqeeZrFa92rYbqzjLjpqNJcbs2MyQ8wJDzA7bzkQQd6xOAAvnsZOdZTdmR1bOSYzZMStwac+NlRM1qmUn/5lgIogP9l/9/h6cf89ruO60mejJu72mjq7Dezt7sasnqRM6ZtdFEET54sqyc/PNN6O/vx8TJkzAaaedhv/4j//AjBkzMDAwgJtvvrnUc6wIVPFRZLyGXbGjKPZMSfyCzBYyfvForInpjtfV2WFuLBtdz9niJFss66tyY3QNuLTsSLYl8oXjkqmMeoBdy06qSMuOWmfHZJ42Wo+VHDFmR1GEHfyxFsG/du6J7hhZ6rlFZmJU8FuZZp5zLzX7AZGI5rb3szo7+eOY2OEFzKNrt6M3mcZPn3lXvYbJo3OByTu6Bw0d0hVQ+jlBDCdciZ2bbrpJra3D09/fj5tuuqnoSVUibttFiFhZUXjsFPMD9Gud2giUm2R1LKqbM7/PXuq5PmZHNp+GvGWneyBlOVdTJO6huFpBWbFc2HmMYsdlzA6blhizYzdS2gNEYZGxiJvhrSbibluWHe4YmbuHbZNZZAxFBU3Gs4orYiTylh0mdphQZ7E8fAzOjp5BAMDOniSAfK+yfAf6ju6krqCgdh3SYQmCKENciR1FUaSm+LfeegsjR44selKVSKm+GO3G7EB1mdg/N1t8+MUjFgmhOh9AHM03UmTYaQSqWnbYuSXzqa9iMTvuxI4iCVBmLoyhtIN2EWpRwdJ0Pc8KZg1R+PkJu5SsYNop6NoT3jA796SAYcfSBWSsoFyCmB0hQJmJIN6y09Gd1L12RE0czfW5xqw7ewd1Hc8ZFLdDEMMHRzE7I0aMQCiUW/D2339/3cKXyWTQ29uLSy+9tOSTrAQUi1+zTnAes1MAC5cFkJtvdSyKvmTGMHe2EFp96YtCQ/YrvDHvxupxm3qe/6uz7ORdGENc6rlzN5ZLy45q2pFPNIhGoOKQorHL7C00BCjbsuxw4zjMxjL7jFmNUahdRL/QLkIWs7OrVy92RtbG0dKQyO8bUoOZwyHt31YmqyAf60wQRJnjSOz87Gc/g6IouOCCC3DTTTehsbFR3RePxzF58mS0tbWVfJKVgLYgF7fQ2XZj2W5PYbTU8OzXVI0azrLD48Syo2Z6ydxY1Xmx4zJmR+bG4n+922lZwc9NrbPj1rIjCfoFYDtQ2hvyc1Lr7LCthQSgcBY7lh3uIKllRz233IUWCYfUz5QdN1bEJAiKtYVgjUBjkgBlZqXe3acPjh9dl0BzQ86ys6dvSG0aWhWLqOKJDDsEMXxwJHbOP/98AMCUKVNw1FFHIRaLFXgFwbDqB+QEpy0VCi5OugBlbfM9XzkMO7qTmNFSr4od8Ve3FrNjJXYEy45kQk01ufT2vmItO9zCneArKFuMzSN2PXedOZd/mTEby948vEDLxhJSzwt8IEUx5HTulhWUTU4VCYXA7HHmRQX1x8tgYodNgcXsMCGcVXL/nhRFMZQ9GNOQwKi6nGWncyClWnYS0bAqdsiNRRDDB1ep55/85CfVx4ODgxga0v8qamhoKG5WFYhVPyAn2G2WaTcol9/PLyyfmtmiPtYsO0KmjGrZMT+9GChtlXruvoKy0WKi/XrXApTtxuxolh1375XYYZxht4WHF4juKtv3xIVlh51Xgbw3lmpVMjlZJBwCMmx8G5adAhWUGezzyzL1gFxMV2c+MD4aDmHiqBr8a2cfWuoTGJWvHt7ZP5TL6gMTSrnjrbIQCYIoL1wZGvr7+3H55ZejubkZtbW1GDFihO4/wohdV0ohbKee5//aiLDQHplMrjaeEyOiuyBiK2aHnTv/GskYdfmiguwXs1Nk1xpXmz1qbiy72VjMsuM69VyYlzrPQC07+jELCQ7tdfrnduduGrcE42dChBcvdmLczI6pEsQOOy7Opbf/5/+txu3LcunmTTVxXH3iATigtR5nHrofmhsSiEfCSGUUrP5gb+6cnFBS3LVyIwgiAFyJnWuuuQbPPvss7rzzTiQSCfzmN7/BTTfdhHHjxuH3v/99qedYEZRqobPdG8tmewo7s2EVjt3F7OgtWrLFlWVjsawZx0gWT+bCSGUcuLHC+uspOkBZGrESUFHB/F82I7vi202AMv866R2wCFAG5OUNRGQ93USqhOhh5sYKh0MYka8f9dJ7u/DAa9sAAKNq4zh9zlg8dcVxGD+iBoloBMfuPxoA8IeVHxjOSW4sghg+uBI7jzzyCH75y1/irLPOQjQaxbHHHovrr78et9xyC+677z7b51myZAkOP/xw1NfXo7m5GWeeeSY2bdqkO+b4449XM8DYf2LG19atW3H66aejpqYGzc3NuOaaa5BOu+2z5A1WzQ+dYFfssB+dpVhW66pMApRtWHbERVX2K7xUlh3+auNc4TinLptSBSiLb5VYTdpP1JgdNUCZpcHbi2PSntscD/pxeAr9WwjrxI78K4rfbJ56rhc7fBPbP15yJG749IE4r22Sum3O+EaInHP4RADAjnxqOvusAuTGIojhhKuYnT179mDq1KkAcvE5e/bsAQAcc8wxuOyyy2yfZ/ny5Vi0aBEOP/xwpNNpXHfddTjppJOwceNG1NbWqsddfPHFusrMNTU16uNMJoPTTz8dra2teOWVV7B9+3acd955iMViuOWWW9xcnieoi0uRK51tsWMzG0sWUyFSl8j9CjYEKNtpBJrfpXZUl0yHWXbSWQWDqYzhF3khFEmWk76ooL17r6WeZ3XPnSIKC3WeAbqxtDEV7v9tWHbE5yUIULZKPQcEy47JMfxWUzdWTC+UYpyZaGZrA2a25mILLzluKnb3DuGgccZYw0/NbMYX50/EP9t7EI2EsOhT03He3auggCw7BDGccCV2pk6dii1btmDixImYOXMm/vznP+OII47AI488gqamJtvnefLJJ3XP7733XjQ3N2P16tU47rjj1O01NTVobW2VnuPpp5/Gxo0b8cwzz6ClpQWHHHIIvve97+Haa6/Fd7/7XcTjcenrgqLYhc52NlaBuAhG4dR0TYwYApSFbKwtu/owtrFKJ1bE5pcywcFigoBcRpZTsQOJ5SbGpxfDuF8GWzSLt+zICTJAWZtD7q/4vpiJXtGwYjtAWT2vEXWbjZgd83YRheN6Dp8yEnPHN2JHdxKJWBj/btKtffyIGowfUSPdFwmHcMu/zRbGzt0/u/U9CYIIHldurK9+9at46623AADf+ta3cMcdd6CqqgpXXnklrrnmGteT6erqAgBDFeb77rsPo0ePxsEHH4zFixejv79f3bdixQrMnj0bLS1a9tDJJ5+M7u5ubNiwQTpOMplEd3e37j+vsVvFtxCOLTslWFiZ2DFYdriso1c278Knfvw8zrzjZf08hCw0tuAc0FKvHhMOh1SXhhvXgKyGEet6PsR1PS9YLVgNUC42ZofdF5OYHVdnLQ4xYNju59Ft6rnYZZ0nWyAmyk6Asp1srIaqGB6+/Bi8et0CLL/mUzhmxhhbcy8EG5ssOwQxfHBl2bnyyivVxwsXLsQ777yD1atXY/r06ZgzZ46riWSzWVxxxRU4+uijcfDBB6vbv/jFL2LSpEkYN24c1q5di2uvvRabNm3C3//+dwBAe3u7TugAUJ+3t7dLx1qyZInvPbzE4npucZyNVWBxsuPGashXOI4KPqgQF9D74BsfAQDeae/Rn1/I+rnwmKk4eFwjZkviI/jj3cDPjt1nRVE4N5f1vWCXx1pyFO3GEraX6jPgBq3QIauzk5+L42ws2wPqxuHRAsblL43YcWNxm4utSu6U3D1TSOwQxDDClWXn97//PZJJrbz6pEmT8LnPfQ4zZ850nY21aNEirF+/Hg888IBu+yWXXIKTTz4Zs2fPxrnnnovf//73ePDBB7F582ZX4wDA4sWL0dXVpf63bds21+eySzGWnUWfmqY+tp2NxRazAsfZcWPNaKkDAEwYqTf186nntQm5bhaDciPhEI6aPlrtdM4o5tey7CXa+ezXlGHiLV1s13OTeWnWtgDEjiDAFHGH6evcWnbMETP0RHiBY9ouwuR4P2DDkRuLIIYPrt1YzOXE09PTg69+9auOz3f55Zfj0UcfxXPPPYfx4+V+dcb8+fMBAO+99x4AoLW1FTt27NAdw56bxfkkEgk0NDTo/vOaYn7VX3PyTPztslwbDrtiIGvXd2ODeZNGYumVx+FnXzhEtz2sfukrqE3I42zsTsMsqNcJ+l/7+bkpCmdJKGTZ0Qdcu7XAmLmxClk0vER0Y4lVjM3uu+uighbnLVRnJ2zLsqNtFy2OXkNuLIIYfpS06/mHH36o65dl5zyXX345HnzwQTz77LOYMmVKwdesWbMGADB27FgAQFtbG9atW4eOjg71mKVLl6KhoQGzZs2yPRevKbaCMkvBdWzZKdE6MKOl3hA4zOaUhVZl2TgPe9YMLV3bTcyOMfNMExz2BVdYCFB2bdkxdWPZy5DzArFfl937XHTMjsRNWug+yNyRIvxm/91Yub8kdghi+OAoZufQQw9Va90sWLAA0ShXcyKTwZYtW3DKKafYPt+iRYtw//334+GHH0Z9fb0aY9PY2Ijq6mps3rwZ999/P0477TSMGjUKa9euxZVXXonjjjtOjQ066aSTMGvWLHz5y1/Gbbfdhvb2dlx//fVYtGgREomEk8vzlEK/ZgsR5eJj7GAnFgcoLlhWDSrOKqiJaZ+FZDqj1jhhsyj047sYy45MzPC/vu2KDK3OTjZ/PpeWHQfz9Avt/spjdsymJOoIpxWUZR/XgnV2bAQf67Oxiu0454wQWXYIYtjhSOyceeaZAHLWlZNPPhl1dXXqPtb1/KyzzrJ9vjvvvBNArnAgzz333IOvfOUriMfjeOaZZ/Czn/0MfX19mDBhAs466yxcf/316rGRSASPPvooLrvsMrS1taG2thbnn3++ri5POVBsjRVmqrebrWR3vGK+rvmYnWrOstM7mEaiLqKbR6EV3qyflB1kL4moC5L9mjKRkN56Vnw2lnyegbixhDnYTccXD3BaVNAsIw2wiMfhrTYm74HesmNvTqUibCHkCIIoTxyJnRtvvBEAMHnyZJx99tlFW06svggBYMKECVi+fHnB80yaNAmPP/54UXPxGrH7t1N4y042q+CPq7bi8MkjsH+rPN7I9mJWBHyMC78m9SbTasdo9uvXbhCpq1/LEosJb8mw2xpBtewUG6CsurGEmJ1CBWY8RBRg4j0xu+ui4LNt7bI4b6FK0nZcVLyVLuqzqYxidghi+OHqN9EJJ5yAnTt3qs9XrVqFK664Ar/61a9KNrFKw24quBlqfExWwSNrP8a3H1qPk372IgBgV28ST6zbrlb+ZcflxnM/58Jz0qwnvHutZ1Br1eFUaBQVs8Nn8fBuLGGbGRHBVejassPmJVp2yqFdRP5u2HWrhkweFxwPenHFI5YjMLzWhhtL1y7CZ9MOm55dlzJBEMHj6lvii1/8Ip577jkAuVo2CxcuxKpVq/Dtb3+77NxH5UKxvbFUy46i4K1tnbp9n/7FS7jsvjdw90tb1G12XTfFiCEtBVfRmfR1Yif/t3A9F30ArRNkqeVsMVQUSC0/MsLCHIrNxhKx22ncC9QRVctO7q/d5qiAs8+KdqjxHS1UhkH3PpocpMvGCqTOTnGZgwRB+IsrsbN+/XocccQRAIA///nPmD17Nl555RXcd999uPfee0s5v4rBblaSGWorg4w+PqYvmUZ79yAA4Jm3tRR8TQB4txBEuZidrM6yk1Ifs1okhWZhJ8ajELJYj6yicIurPcuO7HyO5pH/K/7w17LG/McgJh0GbeceO1c7Vm+n2fn4fyNm/174Ly6/s7HYnMiyQxDDB1diJ5VKqfE6zzzzDD7zmc8AAGbOnInt27eXbnYVRLGp4LywSGW0L9mDbnxKfZzgzfk23UfF/DrlC/fx7ie9Zcee68Yqe6cQWnyScZFUFNgODBaTegp1BDdDu+fymB2fk4cAGF1rGZsxO9DdU/vjiVYynmwBq5KdfyP6zuh+W3ZyfylmhyCGD66+dg866CDcddddePHFF7F06VI13fzjjz/GqFGjSjrBSqHYXlV8fMyevqT0mHhUeztZ9I6XHbbZgpNzY2lf/F0DQ+pju8UUi8rGktxb3YJk040lBlG7FSVm8SoyUeYXZtduN5Yqd6z9eWvWLYs6OzZcVHbwu4s8L/IJghgeuPo6/+EPf4j//d//xfHHH49zzjkHc+fOBQD84x//UN1bhBy3biy+4/ju3iHpMfEo323cnrgqZplQXUVQwBmb0DlgDFAuuCCV4NcyPwIvDrOSAGYZoiBz24agYFHBIAKUhSJ/onXFNH5GF/TtaMDceJYByvKX2rLsBBizEyrBZ5UgCH9x1Qj0+OOPx65du9Dd3Y0RI0ao2y+55BLU1Gj9k15++WUcdthhZVXcLyiK7XrOl8Tf0ycXOzHuGDUYttCJi1gn+OwlPtame4CL2bE5D+YyKqo3liS+RIECKNYLuvga9bnrAGU2Mfk8A+2NJRbakT9V0Vl2HHxYRLcZTxbWAtjO/dEFMQcUoJwl0w5BDBtcRw9EIhGd0AFy9Xeam5vV56eeeio++ugj97OrIIotKsjHJeztT0mPielidnJ/CnY9L+L7ms1JUQAu6x3dXICy3cymoioos3NI4kuyWftZUIYAZedT0c3DUGcnwABlbQ45Mjbbl8juqR2s2kWgwL8FO/9GdKnnvruxcn9J6xDE8MHTUMliMmsqjWJ7Y/Gm+s7+nGVHXJz5p3bbRRSz8rJrES070jo7haZR4nRevrozo2CAsmjZKbkbq7jzFkNYFaYs9zz3x0lpAkfTthCvhYoK2hnHTksJr6B2EQQx/AggL2TfpNjeWJFwSF0cuvNi4rdfOVx3zFCaKyqY/1vQouJuOgA011pWUXRtLPjihnYDlEsRB6EPUGaWBXmjUBlibTrXAcomC32xn4FiEN1KquBwkI7vJDvN6shC7jw72oV/L/12Y7HPCYkdghg+kNjxiWKbQIZCIVQLXccnj6rRPU/qRAZbzLxD+4Wrj1/IcNHKWhaaPdHldPngLUr8CLKCh3aLCpo9t4uWjSW/miDcWKaXUuDG893sHbmxrCw7asC42ZTK243F5kd1dghi+EBixyfsdt62gi8mCAAtDVW657xlx25RwaJidnSF+7Ttae6J7Ro3Ll0D/OGydhEK7GdBGd2C7t4rdhrxSuwKPy/QYmj0cyn0vtQlXOUwcEUMncfs2Lk9+gBlh5MrkjAXq0YQxPDA06+JIL7UyxW7i74VvGUnHgmjSrD0JFNGy04hbVXMW6QGagp1dvjHdkWeZglwKHb4c3CP+V/7tgOUDZYdR1PR5lEwG8vdeYtBvL92M8NqObGT5MR0wfHyf2XGj0KtU+yITF3/rMAClEntEMRwgQKUfcJu7IoVNXFt4amrMv7iHuLdWPm/XoYzqEUFFf0Xv86yU2BhY4gxJXbRubEkMTuA5mIrNAeDZcf1zdNbUUSCCFAWR1TflwKvS3CFKp2IHW0c410oFKxvK0A50ArK5MYiiOGG667nnZ2dhu3d3d044YQT1Oc9PT2YOnWq68lVEsXW2QGA2oRmyamXiR2ZG6vAynHFwhkAgLM+sZ/j+YRN3Fj8ImC7u7bLqrR6y442SMiYhe+oDxQ/J6do2Vj6iym2inZRCDE0ipCOZZa9V/w9MFLoPbZjqeG/uAKrs0NahyCGDa4c8s8//zyGhoyF7QYHB/Hiiy8WPalKphjXXi1n2WmQiB1pFlSBc86bNBLrbzoZtUI8kB34rBRdgLIkZqdggLJL14CiVzva3CTjFe6Npb9brisoy+Ym2e8nZjE7Xgkvs5YZuW15y47JG2IrZqcM3FhkuSaI4YMjsbN27Vr18caNG9He3q4+z2QyePLJJ7Hffs4tBPsCpVhc+PiJxuo4AKChOorufHsGnWXHbiEVFB+EmnNjadt1YsemRcttbyzeImHmxtIOsD6X2HbAfcyO/FrKoYJybh5K0Y1p7Y8ncWMVitmxMSn+kMDcWCR2CGLY4GiVO+SQQxAKhRAKhXTuKkZ1dTVuv/32kk2ukihFQbm6Ks360lgdAwD87dKjcMn/rcaWXX26mB0/XCZ82XzzAGX9sWa4DlDms7Ek59ON4dCNVaxlRyRINxZ//xXFmfAKwXlJAH4swzbJnPTjOQxQ9lvscLFqBEEMDxyJnS1btkBRFEydOhWrVq3CmDFj1H3xeBzNzc2IRJy7Q/YFtLgR99QnYurjEbW5xzNa6vHf/zEXn/vlK1I3lpfLABMDCvQVlHUxOwVqqjCssnfsUmgBLBigLFQVDLlcRHV9uSz2+wk/ogLj59FKY1bFIhhIZVyNJzttoVIAthqBco/9r6Cc+0tuLIIYPjgSO5MmTQIAZLPOszL2dWx3/7aAz8AaURNXH8fzi3QqY7SoeGvZyf3NZHM9qBi6LBXblh1rgWCGmWVHNl7Bruch6+d2KVhB2d1pi0J0YzmxMlXHXYgdC7ekdh+KiNnhKzv7HrOTGy+dIbFDEMMFd8EaAN59910899xz6OjoMIifG264oeiJVRqlEB8N1ZplZ2StJnZYenBKmnru3ULA91vKcCIlI3NpFQoOVgOUnc3BPGbHfAwzjHV2irt3sqRrIKh2EZwbi9tu5xrFyt22xjMJOOetIWbvhx1LTZAxOxGXwfQEQQSHK7Hz61//GpdddhlGjx6N1tZW3S/mUChEYkdCScROlSZ2RunETm4xSknaRfgSs6PAPBvLoWWnqArK0H8ODWMULGxYGrFTqKhgEG4s/tIVhS8JUHgufMkDp8OZBWkD5vdh1tgGPL6uXbpPPX+A2VhsbKqzQxDDB1di5/vf/z5+8IMf4Nprry31fCqWYrueA/p080bejRWVuLHyf4tpT1EIXeq5ScyO1oTTGovkHduItzYcEixFhSw7JSoqaCbcgjQE6NxYXIyVVWwNoybm/GvCzC2p70Ivv78XHTsV/UMZnDirxfT8/Fvje7sIEjsEMexwJXb27t2Lz3/+86WeS0VTCssOn3rexLm0mNjJZHP1bsLhkC8dtvlFnV/I3WRjue6NVeCcWRtuE7P9rmN2TLaXQvC6RRegrDiz/NW4sOxog+mf8vogZCJSqmIRfPOUmZan5UW8724s6npOEMMOV7+JPv/5z+Ppp58u9VwqmlLE0Ojr7BjFDsC1jPCh6aSajaVAsOxox9ivoKw/3i5WGTHimIUDlPX73S6ifBNSnmB7Y+kHdfJ5rHVRh8msgjL/OSnmNujaRZAbiyCIAriy7EyfPh3f+c538Oqrr2L27NmIxWK6/d/4xjdKMrlKwm6XaSv44n9NNZzY4VKmk6ksqmIRML3haW8sXbsIeZ0dht12EUU1AjVYZkK6IwpXUBbETrGtEkQ3lrDfT4yWHftzcVNd205KezHCX+/G8tuy484KSRBEcLgSO7/61a9QV1eH5cuXY/ny5bp9oVCIxI4E7XvR/Rcz64cVDgH1XLByLBJSC78lMxkAMc2y43q0woRVc74+9ZwPVtZ6gtlL+3Zu2dEei2OIi6lvvbFMtpei/IBbdI1R+dRzttHivo+uSzgeT7PsOI/ZsXV+7rFY+dprwvnRKfOcIIYPrsTOli1bSj2PikeL13B/jrGNVfj8vPEYU5/QuVhCoRBikTCGMlm1ZYQfmT9mlh15Npb1uTRLgOPcc+0cBrFi/VxEtORE3Aa+qpYd/WY/Cj2aoQ9QdubG+voJM/DSe7tw5iH2W8GY9cbKWrxfTuCtOb43As1/LsiNRRDDB9d1dgBgaGgIW7ZswbRp0xCNFnWqiqcUfZFCoRB+9Pm50n2xSAhDGa0/lvo97OE6wASXougNA/oAZXuxQ1r2jjN0dXaEfQbLjk1Xmtlzu4hNNxmaGysIucPNgwsotzOXxpoYnrziOEdjmNXZ0cXsFHEb+Jf6HbPDYuSSDgstEgQRHK5+u/b39+PCCy9ETU0NDjroIGzduhUA8PWvfx233nprSSdYKXjdeDGWN0OwAGW7Kd/FoLqeskLXc17sCMcWPFcxdXYKiJtCbiwxIDni8uaJ1YrFx8EEKGuPFfAuNY/GMxN8nLuzGKsjL3D8zsZK5P+tJdNUSZ4ghguuxM7ixYvx1ltv4fnnn0dVVZW6feHChfjTn/5UsslVEpr48OaLOcq+gFNM7OTw8levmnqOwm4su1YV5xWUuXMI+8TF1HnqefExO7wY88HYZoqugrIkQNlpm46C46l+Sf12RRcwXoSVM8B2EfF8Ec+hIsROV38Kv31pC3b3Jks1LYIgLHDle3rooYfwpz/9CUceeaTu1/RBBx2EzZs3l2xylYTXdW/ieTOEatlh43n4q1efeq5t1z+268ZC/lxOLTvmbhHHqeclKiqo6zDObQ+ygrJuSJ3Y8XYuxgBl7XExH80g20XEY8Vbdq76yxose7sD/3jrYzy06OhSTY0gCBNcWXZ27tyJ5uZmw/a+vr7A4xHKFrbQefTFHMvHEWgByn64sQoHKIvHFjyXQ9OOPvW8UDaWNUZLkEvLjpkbK9DeWBpK/n+Ah24skyBtfcxOcfFrDJ+1DqpUN5b7mJ1lb3cAANZs6yzFlAiCKIArsXPYYYfhscceU5+zL57f/OY3aGtrK83MKgxDqm+JYbV2xGwsT3tj8ann4qKW32C30zfbX0zquWF+DrOxSldBuRwtO4Ibi233ajzTbKzS/Dtg70045H/AN2/Z2byzF/et/AD3vfoB/m/F+/ioc8DXuRAEYQ9XbqxbbrkFp556KjZu3Ih0Oo2f//zn2LhxI1555RVD3R1Cj1cLHTPlp/NqwY96Z+xa0pmswZqTziqIh0OAakEoZNnJ/XVeVNB88XSaXWWsoOwy91xn2ZE/9hteuOUClPNPQnJRUqrxDKctkQhn72UQwrEq3wU+mcrii79+FTu6tbibB17bhse+cazvcyIIwhpX3+bHHHMM3nrrLaTTacyePRtPP/00mpubsWLFCsybN6/Uc6wItNgVb84vNifUXCYexuyolWSBZ9/p0O3LiJYdmwHKjgu1WZxfV2XXxm0wiJ0SZGNly8WNZVJU0LMAdpNeZ9kSWbfYF1cQYifBuYx39w4BAOaObwQAdPRQwDFBlCOOxU4qlcIFF1yAUCiEX//611i1ahU2btyIP/zhD5g9e7YXc6wovPpuFpsTatlY3owHWF/Lv3b15qw0tsVO7q/jooIWOF0IS9b13GR7kG4swzw8dnOqyVhmbqwix2X30O/gZECz7AykM6ol9VunHgiguAwtgiC8w7HYicVi+Nvf/ubFXCqaUv2iNcNg2RHcFF5gZRU4/Rcv4XevvI+szZR7L7qe8yPasXCJ62axjUABk9TzgLSOlg2uWLr/SjKWemJ5UcFiSzAwIRqA1kEin3rel0yr21ivOpnYufrPa7D/t5/A/t9+Apff/4Y/kyQIQocrN9aZZ56Jhx56qMRTqVx06dEejaHG7Aip5247HtihkHD7weNvc1lohc6V++s2QFkmZnjLjJ37LlpyXFdQ1sXHGIsKBmXZ4WvfZIX7VupwIjPLTqkC59nLg4nZyX2YewYlYidjFDuPrt2ea+WSyeKJ9e3+TJIgCB2uApRnzJiBm2++GS+//DLmzZuH2tpa3X5qBKqH/8L3OkBZs+x4H7NTyM2TyihqNeVC181+6Tu37JhbKPgx7dwGp0UIzRAL+ImPg3Vi6QOUPU89F8cuVcwOy8YKwLTDYnZ6BlMAcq1aqvOurUxWQTqTVYt8DqWzuno8GaHaOEEQ/uBK7Nx9991oamrC6tWrsXr1at0+6npupFT9gKxQ3VhqzI73wbB21hlm1q/Km/7NUC0MJUw956/djRurJHV2IHkclBsrFALyfbG0OjveJZ8DHsbsBOjGYr2x+pIZ9TmL4wGAwXQWdXmx0zWQMrx+KJOFlqeYE0BBxB4RxL6EY7GjKAqef/55NDc3o7q62os5VRxWhe9KhebG0gcoe2nmt/MFzX7VVsWt/VhqJ2mXMTuFsrFsubFKZNnhYRY23pUZmBsr/1eBYnAnXXvKAfjhk5vw5SMnlWYs1bJjErNTogDlIO5lPJIPUM43Ak1EIqq1BwAGUxnUJXJfrUzsVMXCGMy3ckmmssjrTvU87HiCILzBldiZMWMGNmzYgBkzZngxp4rDD8uO0Y2VH8+b4QA4W2iqYwUsO/m/bttFyAJeHbuxPIjZYR4LXcNSV2ctHXxvLDabSz85DafNHouJI2tKMoZWN0m/XSsyWawbKy92ArCIxKNhw/NQKIR4JIyhTBaDXDd0JnYaq2NIppJQkKu8zHuy+pNpEjsE4TGO41fD4TBmzJiB3bt3ezGfisTPmJ101r8KyoloGAeOrbd1bEGxo2ZjOZuDlnVm3KcTOzYWVzG7zH0jUKMfi7+soCw7bNxc5jmrs5PbFwqFMGlUbcksj3wM1lvb9uK5fB2mUnV+Z68P0o0lPo9Fc5NhFhwA6BrI1eGpr4ohlndtdQ/qXVu9XFYXQRDe4CpZ59Zbb8U111yD9evXl3o+FYle7HgzBluo02oxP+8zf0KhEB78WuEmhtFwSA3YNMNtBWV1LrJtIfnjQnNgFJiyrfMwUeGHda8gLOMty7uxvIpQ1h5+9o5X8NV7X8OWXX0lc69G1JidINxY+g9GIp+dlci7t3jLTmd/Ttg0VEVVMbS3Xy92+ofc99giCMIermyn5513Hvr7+zF37lzE43FD7M6ePXtKMrlKQbfQeeTEYOZ80TLi9VoQtaHeqgpYdYDiLTuy64wE5sYyZmPp3VjBxuwA3lf0ZqflM482tXdj8ujako4biNgRLDtV+edsO98gdE9fzrLTUM0sOxnszW9j9JFlhyA8x5XY+dnPflbiaVQ2+gBlb8ZQLTsZfddzrxcDO0HKiWhhE4lq2XHc9dw8Zifk0I1Vsq7nuvmxv5zg9bL4kQValWpum2djaS4zRtdACnkvK8LDOGZH/DyzIoOq2OHcWEzsNFbHVItQp2DZ6RsisUMQXuNK7Jx//vmlnkdF40uAckQIUGbjeTOcSigUQjhkbZGxJ3b0qfN2sdv13M59MMbsOJqKNhbvxlKzsZzNxQvUTuRcNpZXYkELONe2dfantH8LJYrZ8bIdihni55lZLtn2wTTvxsqJnRHVcTVmp3NAtOyQG4sgvMaV2Nm6davl/okTJ7qaTKXiR4ByVA1QFrKxfDDzh0Mhy2KA9txYub+O6+wIr9fNi6+gbOM2iBYXt+4mnRuL/fXhM2AXrmWZZ58PWa+zvf3aIl+sxiqnbCwWs8MqK/MByp35bKym2hjiJjE7PYPGWjwEQZQWV2Jn8uTJll+SmQz9UuHxo8aKset5Dj/W1dzYxYodLXvHCVrqudm89Oe3QnxvYiUwG7DrKYcAZb6qcdbivpVkrPxf3uK3ty/FxQoV68Zif4MXOzWqZSf39187e7H472vRP5TB4+ty7SFG1nKWHUHsXPfgepx8UCtG1SW8njpB7LO4ih5488038cYbb6j/rVy5EnfddRf2339//OUvf7F9niVLluDwww9HfX09mpubceaZZ2LTpk26YwYHB7Fo0SKMGjUKdXV1OOuss7Bjxw7dMVu3bsXpp5+OmpoaNDc345prrkE6XT5+8KwPLgyWIGJoF+HReDyF+l6xX7yW53AboMweSC7UaTaW6MYSFzUnqAYHSep50AHKTrrRux5LyA4EclYOrc5O0SMAsG5G6xViNpbqxsp/zn/89D/xx1Xb8PCaj9VjRtbGVTdXZ7/ejQUASzfuMGwjCKJ0uLLszJ0717DtsMMOw7hx4/CjH/0In/vc52ydZ/ny5Vi0aBEOP/xwpNNpXHfddTjppJOwceNGtd/WlVdeicceewx/+ctf0NjYiMsvvxyf+9zn8PLLLwPIWZFOP/10tLa24pVXXsH27dtx3nnnIRaL4ZZbbnFzeSVH8SNmJ++DyQpuLD/M/IV+XdfEC1t2tEagblPPjXPQ19mxcQ7hoGLEDkNzYwVv2QFn2fG6XQQ7bSarrzlTqiywSaNyxQ8njipNEUQnRCNhXZxaVf7zzbdECQH4ytGTcc/L7wPQW3ZY0PJhk0bgnfYe9CbThto7BEGUlpKW7TzggAPw2muv2T7+ySef1D2/99570dzcjNWrV+O4445DV1cX7r77btx///044YQTAAD33HMPDjzwQLz66qs48sgj8fTTT2Pjxo145pln0NLSgkMOOQTf+973cO211+K73/0u4vF4KS/RFTrLjlcxOyxAWe2N5e14PIV+XdtyY+X/lrI3Fu+FslVUUBCG4i94J/B9qADxM+D6tEXBMqD4CsreubFyZ05ltAvvGkiVLEvwwLENeOnaT6G5vqqo87glGgmrfd9YwUzegnn09NG48YyDMH/KSLyyeTfmTxmlurn4DK3jDxiDR9dupyBlgvAYV9/m3d3duv+6urrwzjvv4Prrry+qhURXVxcAYOTIkQCA1atXI5VKYeHCheoxM2fOxMSJE7FixQoAwIoVKzB79my0tLSox5x88sno7u7Ghg0bpOMkk0nDNXiJ9ivauzGYBScViBvLepRaG6Xwwy5jdgDz69Slntu4EeICXIxlR/NiGf1YQbmxVMuOomjuJM/cWLm//PvZO5guqcgaP6KmJNY3N/DxXJrY0UT9/i11AIBTDh6Lmz97MCLhkDrX3VztHWb17KdaOwThKa4sO01NTQaLgaIomDBhAv74xz+6mkg2m8UVV1yBo48+GgcffDAAoL29HfF4HE1NTbpjW1pa0N7erh7DCx22n+2TsWTJEtx0002u5ukGl54ZRzDrSjaQAGXr/YVaRQDyxdEOtttF2BI7+uclcWOplh0+SL3o07pCCCMStpZ4rPxp+ZidnmSaE1nBZqQVCysQCGhuWl7sjJf0GGMxPaplpyqm3ieqtUMQ3uJK7Dz33HO65+FwGGPGjMH06dMRjbrzjC1atAjr16/HSy+95Or1Tli8eDGuuuoq9Xl3dzcmTJjg2Xhq/IyHX/As9dwYoDxcYnY0F4sTrOoJ8V4oO4ureEysKDdW7m9WcCvanYsXhLh7rLmTPBor/zfDiZ2+ZJprY+LNuH7Bfzaq85/vGl7sNFUbXpOI6v8dNNTE1B8nfdQygiA8xZUyeeWVV9DS0oILLrhAt/23v/0tdu7ciWuvvdbR+S6//HI8+uijeOGFFzB+/Hh1e2trK4aGhtDZ2amz7uzYsQOtra3qMatWrdKdj2VrsWNEEokEEgn/0jy9Ls0PaK4kQ9dzPyw7BVaumrgTN5azsa3qCekrKDunODdWLh1faxdRTpYdxQcLi/7zCOTid5L5OJdhb9nh3kQmcqo4Ud/aaIwlSkT119xUHcVQPqaJ+mMRhLe4+jb/3//9X8ycOdOw/aCDDsJdd91l+zyKouDyyy/Hgw8+iGeffRZTpkzR7Z83bx5isRiWLVumbtu0aRO2bt2KtrY2AEBbWxvWrVuHjo4O9ZilS5eioaEBs2bNcnppnpD1wcpiKCqY3+7HwlpojFoblh1ZETo7KBYxO8Va0ooLUNY/9yNIvSDqPba+byUZirmxMlnd9iFV7Hg0sE/wQrg6L+b5z4tc7Oj/HYyoiav/NgbIjUUQnuLKstPe3o6xY8cato8ZMwbbt2+3fZ5Fixbh/vvvx8MPP4z6+no1xqaxsRHV1dVobGzEhRdeiKuuugojR45EQ0MDvv71r6OtrQ1HHnkkAOCkk07CrFmz8OUvfxm33XYb2tvbcf3112PRokW+Wm+s8MPKwjKJVLeJVQGaUo9d6MJsuZByf0sZs8PPy43wKYXYUS07FkUX/YLPeFPFsEfxvWKRSwYTP8PejcWJHZaFxcfdjK41fveIbSYaqmPqY7LsEIS3uPqqmzBhglrnhufll1/GuHHjbJ/nzjvvRFdXF44//niMHTtW/e9Pf/qTesxPf/pTfPrTn8ZZZ52F4447Dq2trfj73/+u7o9EInj00UcRiUTQ1taGL33pSzjvvPNw8803u7k0T/BT7ARSQbnAypXOZi33AyVwY0n2hXQxO87OCxRXo4jvQ5V/kDtngIu8bk6KflvJx5IEKANatmBgGWklghfCLDC5a0CrlSP77IglGOqrYqjJZyoOkNghCE9xZdm5+OKLccUVVyCVSqn1b5YtW4ZvfvObuPrqq22fx47LoqqqCnfccQfuuOMO02MmTZqExx9/3Pa4fuOHG0sUO1C8dVPwFLKa8LVWCp3DvRvLOAenRQW9QFZnJzA4a1OpGnIWGMrQ2DWVd2MF0dOqlBw6sQkbPu5GdSyCGc25NPMxBWr+iJad+qookvmmoQMpEjsE4SWuxM4111yD3bt342tf+xqGhnJplFVVVbj22muxePHikk6wEkjlTfdRD1s0R4Wu4X6m+Bby9jiJ2SlsA9JjnXrOn9/fxZXvQ5X7623FYjvw98OvooJpQegyK9/wljrA9z57ML52/HQ0VsfUOlIXHzsF7+/qw78dKrduVwv/DhqqY+jPu7745qEEQZQeV2InFArhhz/8Ib7zne/g7bffRnV1NWbMmFE2MTLlBstAKSYGpBARVkE5w9xY/qX4mi3gN33mIDyxfju+dOQk2+fIujSByFPPg1tS2chZQXwGiz6uC/BOfLHPY0oIUGZWvuEeoBwKhTBOSC+vr4rhF+ccavoaMUC5viqKrv7cV/Bgmiw7BOElRbWLqKurw+GHH16quVQsqtjxsNqrWmdHCFD2Y1ExExXnHzUZ5x812dY5wqqLxV2Asuw6Q7oAZUenLRq+pk3ub/CLPB80rbr/PJoPS81m2VcMLUB5mKsdF/AxOyEAdfGoWoMqSZYdgvCUYGqt72Mwv3wxReoKEcmn1YjZL34Egpai87QqDhy+TrEoK6ivoOyzG0t9JIrP4K1NCjg3p0efj0j+s55Mm1l29kWxoy9EGA6HUJPIiZ3BVMax0CcIwj4kdnzAD8uOaep5GWRj2TqH6sZy93rZ2qlrBOr32iqmnnscI+MEhWtQ6tV9iZpYdoYylRGz4wbejcXifFjBTQVGYUgQROkgseMDQz7E7IjtIrI+ZmOVxrKT++u6zo6EILOxeCtK7m/wAcpi0DS/rdSwYPxkRnRjVUa7CDfw2VgsaJ/vG2dWa2dnT1KX1k4QhHNI7PiArzE7ohvLh8W1NJad3F+nxfesDFjhIursRIu8JjFmpxwClNU6O1xvLK/cWJplR7+Aq9lY+6Abixc7Y+pzyRyRcEj9EdQvqaL8qxc24/AfPIN531uKNz7Y689ECaICIbHjA4P5X2xinY1SYiwq6Gc2VinO4c6NZRXnEOGqCjpd1IvN5NIsO0Jj1jIIUIauN5Y3Y0XzC/hQWqizsw9bdvjeWVefeID6mHVDlxUW/PPrHwLIFWdc91GXxzMkiMqlqGwswh6D+YJhnsbs5BeXrCFGxPtVpVgrCFBMbyz963mKsewUnbbO3HJ58eZnRetC8LfYq+mwuk+iZYelou+Llp2545vwmbnjMG9SE46cNkrdXhWLoGcwbXBjdfQM4r2OXvX5IBUeJAjXkNjxAVZDw1OxI9SpKad2EbbOwebv8HVWoo6Pj3EaK1Os2AkL7SK8dhvZmhOX8cbm41UlY5Z5KFbP3t45AABo5PpC7StEwiFpHZ6q/PfCWXe+glAo9xlRoBji0UjsEIR7yI3lA8w8XeWpGyv311Bnx7MR+bFLKHYcx7aYu4d0Aicgy44hG6sMDBrZrFXCfmlgAcpDQoDyu3lLxbQxtR6NPPw4bPJIADlXVSqjYCiTRSqjGPqKUbYWQbiHLDs+wL6kxAqqpUSss6P4qHZ4UTFtTC027+zDJyY2OTqHazeWxeGRIrKxShWzw8j6KD7N4LOxPE89Z5YdYYF+f3cfAGB6vp8UAfzkP+bi2lNmSjMRE9EwfvTUJjzw2jYSOwRRBCR2fICZnxMxHy07+e1+pDrzwuDnZx+KN7buxRlz5P2BzHDfCDRHoWwsp/chFi7uvQqJlh3VAlUGRQW5CspeyS+1grJJu4hpY0jsMEKhEFobzZuIssQGcmMRhHtI7PiAH5YdsbeUr24sbgFvrI7hvLbJjs/B9JJTN5ZVZeJi6uwU75oTY3bczaOUaFWqtXgQr7Ki1GysjNwaMWU0ubHsksjX4hELNBIEYR+K2fEB1i6iylPLjr6CMsOXrudcqWK3Aa9azI5TN5b58fxcnN6Hot1YgmWHFdOLRoKUO3kUPoDdozo7rBGoZIGuioXVOjNEYZhlh9xYBOEeEjs+wJr8eRuzw+rs5J77Wdclost6cncO972x2OuN+/i5OL0PsSJFCXs1E2/MwhEt0j1WDNKYHY/GiptkYwHA6LrEPpl67hYmdsiyQxDuIbHjA4P5Lym+63GpYYIjI6aeezYiN3aYFzvuRmQxR1mHfiyrRbsYN1axokRszcAWqiAtO3zMjlUWWylgtZdSEjeWl21TKhFyYxFE8dC3jg+wL6lqH91YfnbZjujcRcWdy2njZ6vA30iQbiyuNQOgLfqxABd6TYDxMTteubFy1ymmTwN6tydRGNY/K5mhAGWCcAuJHR/QYnY8DFAWxY7Hv9x1YxdRvE98nVOxY+ecgIveWMW6sdSX5y5IEztBWna0e1zK+yzD6jqLzXTb16giNxZBFA196/jAkOrG8qGCsqFdhPfwRhDXYoeJNadRO5Zdz/nHzuZVihYYQHladrKK4rkYtrrOUt3bfQVyYxFE8ZDY8YEhP2J2mFgIout5CQKU2QfRuRsrh7w3VoAxO/m/bH4skyZQsaOmw/MxXR65sSw+CGWRkTaMYG1mhiTB3gRB2IPEjg8k87/qPe2NxbKxhKKCfiwruq4MRbqxnKee58eV7IsW4cYqOhtLqHvEBG88SDcWr8BYzI5HH0mrmKcgM9KGI1o2FsXsEIRb6FvHB4Z8aRehZb8k0xlfU89DpbDshN1WULbojeUiQLkukauzueDAZkfzEBGzsVTLjoeC1y4KFFVUemXZsbJgxaJk2XEC+5EkS+MnCMIeVEHZB1Kq2PFuoWOWkb5kBkfeskzNgvGjXQQ/gtvxtLo0zl5n1fXcTR/QZ676JF57fzdOPXiss4mIYwvzY7/KyyFmR1Gs3X+lgCw7pSOhih2K2SEIt5DY8QHma/fDjQUAe/tTno0jozTZWLm/bmN2ZETDzi1OrY1VOGPufs4mIYFvzQBohSXLImaHd2N5XGdHuo9idhwRj+QswiR2CMI99BPLB9iXlJduLLPFxZ/U8+LHC+d/7bttFyGvoOy+zk7JyF+OH3FbhWC34PF129GTTLOtnowVtXJjkWXHEXEu9fz9XX14p7074BkRxPCDLDs+oIkdD91YZmLHhxDlUBlYdqSNQANMcTaroBxk9WB2N/7+5kfatgDcWEHWGhqOJLiYneN//DwA4I3vnIiRtfEAZ0UQwwv6ieUD6kLnpRvLZNXy27LjOkBZcPvYxiIbK1ICEeYWMWYnVQ5iR3IPvLor1m4s+tpxAvveGEhp2Vgf7O4LajoEMSyhbx2PyWYVNVjYW8uOyXY/ApRLaNlxHKBsoXZK4V5zi5p6nlc7g2XkxuLx6vNBlp3SIfvM9KpuSIIg7EBix2OGuKDCSrXshEogKlynnls1AuUUoN8eLbGoYMoH614hZLcgiArKQQZpD0dkn5mO7mQAMyGI4Qt963gM3wjRyy95s3ReX4oKcqO4DQRWs5dKmHrOGxD8iF3i0dK8haKCZWbZ8eq+WFp2KEDZETKL8PaugQBmQhDDF/rW8ZgMJ3a8dCmZrR9+WHZKoeE0N5bTooI5ZNepW3ADcmOplp1M8DE7spvg1efDKmaH3FjOkH1mdpBlhyAcQWLHY/heVV42QDT/Je1HzE4JzgG3lh1WCdhIdVxLNgwqQFlMPa8qN8uOV2LHqut5GVSRHk6EQiHDd8eOnsGAZkMQwxP61vEY3o3lZSq02WLuR6xKKVwhauq502wsdRLGOYwfUW2121uE60mVQbsIecyOR3V2LFxVFLPjHPGe7eohyw5BOIG+dTyGuWW8Fh1mlh2/u567RctecvY6q4anE0bWqI8HU/42UQwLlio/ai0VQh6z4w2UjVVaxFiv3X1DAc2EIIYnJHY8JuNTjyrTbCxPR82PUQo3ltuighbHN1bH1Md+Z6+w68mqYoe1DPGuinYhCvUPKyVWLtsgg7SHK6JA3N41iMv+sBovv7czoBkRxPCCvnU8RhU7Hpt2TCso+xGgXIJB1KKCTtWORddznu3d/sY4aEUF89lYLGYnVl6WHa9EuFXMDhUVdI7oxhpKZ/HE+nbc+I+NAc2IIIYX9K3jMcyNVQpB4AZ/2kUUfw7XRQUt6uzwDAz568ZibrmnNuyAoihlUUGZpb/zeFdB2fw6g81IG57wYufiY6fgomOmAAB2+CziCWK4Qt86HpNWLTvBjO9PUcFSxOzk/mYdBihbpZ4D/hcTFPnbGx/i0bXbVTdWkMG5b7f3GDcG0huLvnacwsd6nXLwWFx+wnQAQM9gGsm0v0KeIIYj9K3jMVmfYnbM8LuCsvtzqOlLjrAqKggA3z/zYADABUdPcTs1V/D35NG1H6sBykFmY2UkZjOvLH+WMTsUoOyY84+ajPEjqnHUtFGYM74RjdUx1UJ2wPVPYvaNT+GoW5fhvY7egGdKEOUJdT33mEzQbiw/emOVJPVc30vKLla9sQDgi/Mn4ZMHNGNcY1Ux03MMP529fSnVwhekC6e5PoGOniROPbgVT6xvB+AmRsoe4XAI4ZDmluQfU50d53zpyEn40pGTdNtG1sbRnndj9STT6Emm8dT6dkzPW30IgtCgbx2P8Ssby4xhk42V/+u+XYQ5+zVV+yL6ePjx9vYPaRWUo8FZNe67aD6uPnF//Pjzc7WNHk6H/8zzQclW8TyEfcbUx9XHTTW5zMN2iuEhCClk2fGYbD4mtJJjdkoRF6NmY7mO2Skv1wg/m66BlGbVCNCyM6OlHjNa6gEAN3x6FnqTaTTXe2fxikZCqkUrFglhKN+oO0jBV0mMqa8C0A0AOGb6aDy6djuJHYIwgcSOx6Tzaic4y44f2Vilq6DsPBvLvF1EkAxyQaM9g2kk8inn5RKce8Ex3scw8a7b3HXn7glZdkoDHxd17Iyc2NlJlZUJQgqJHY9RU88DSgsKOhvJNi6LCqovL7PrHExpad4DqQwG8hWc96WCenztJ17kWdXgIezDl1OYMCJXLXxXL4kdgpCx73zzBkSGubF8WI1/9O9zcO78iWiuT2gbh0k2Frs/W/f0480P9tp+XdapKcgn+pnPRmBfqjHDC3z+usvFujXc+a+FMxAOAV8+chKaG3L/5vf259pIZLIK9vQNYW/fkGdB6AQxnCDLjsdoAcrej/X5wybg84dNwHPvdKjbfHFjlTAbCwD+7c5X8P6tp9t6XbnG7AyY9OLalxb6qM6yE5JuJ9xz2OSReOM7J6KxOobuwZy47ktm0DWQwum/eBEf7h0AAJw2uxW/PHdekFMliMDZd755A0JtBOrjFzz/i9oPDaCzJLlEnKfTxp3ltnzyLoZPzxmrPt6XmmCGDTE7xsdEcTTVxBEKhdBQFVU/W/ev3KoKHQB4cn079lLjUGIfhyw7HsMsO37W2dGJHR/G++wh4/Dqv3ajbdoo1+cQteDM7zyJmz97EM5rm2z5unK10PPetelj6tTH+1LMDv855GvrUMxO6QmFQhhRE0dHTxJ3Ld8MIFdI8/lNHfjXrj7c/uy7mDdppHr8+BHVmDuhKaDZEoT/kNjxGFXs+GjZ4X9R+xErFI2E8SO+dosLZG6oGx7eYEPs2GsEGiTTmjWxsy9ZNaImMTuUjeUNE0ZUo6Mnia6BFADgnCMmoDYRwe3Pvoffvvw+fvvy+7rjn7nqk5jOfTYJopIJ9FvnhRdewBlnnIFx48YhFArhoYce0u3/yle+glAopPvvlFNO0R2zZ88enHvuuWhoaEBTUxMuvPBC9PaWT8n0IIoKhn12Y5UCt9NUY3bKzpGlceDYevXxviR2IiYxO/uSK89P/vs/DsF/LZiByz81HXd96ROY0VKPrx49BZ/cfwzmjG9U/6tL5H7jvr29O+AZE4R/BGrZ6evrw9y5c3HBBRfgc5/7nPSYU045Bffcc4/6PJHQx4ece+652L59O5YuXYpUKoWvfvWruOSSS3D//fd7One7ZAJIPde7sYbHwiITg3ZuGXMXlauoCwGY3lyPq07cH7WJaGAlCIIgYpJ6vi8JPj+ZPLoWV564v27byNo4fnfBEbptF//+dSzduAPv7+7zc3oEESiBip1TTz0Vp556quUxiUQCra2t0n1vv/02nnzySbz22ms47LDDAAC33347TjvtNPz4xz/GuHHjSj5np2R9zMZi6OKDhsnaKhM7k0bV2nhlmQbt5KmORwAA31gwI+CZ+A/vrqI6O+XDhBHVAICtu/sDnglB+EfZ/8R6/vnn0dzcjAMOOACXXXYZdu/ere5bsWIFmpqaVKEDAAsXLkQ4HMbKlStNz5lMJtHd3a37zysyAWRj8SER5WrxEJHN006KslLmlp3axL4bFsdbdnRp6BSzEyiT8z8iPtxLYofYdyjrb51TTjkFv//977Fs2TL88Ic/xPLly3Hqqacik8ml9ba3t6O5uVn3mmg0ipEjR6K9vd30vEuWLEFjY6P634QJEzy7hiAClHnLTplqAAMyscKaZ1qRVdtFlOeV1uQtO/sifGwO//6SZSdYJo7KVVv+uIv6aBH7DmX9s/Pss89WH8+ePRtz5szBtGnT8Pzzz2PBggWuz7t48WJcddVV6vPu7m7PBE8643/qud/ZWKWAn+fs/Rqx7qMupDKFXVTlbtmZMtqOK64yqYlrXy8hk5o7hP+Mz7uxOrqTUBSl7ApyEoQXDKtvnalTp2L06NF47733AACtra3o6OjQHZNOp7Fnzx7TOB8gFwfU0NCg+88r0kFYdoZjNhY3z/qq3CLJmqhaoVVQ9mBSRXDPVw7D0dNH47az5gQ9lcDgrVo6y84+FKRdjoxryomdgVQG3QPytiYEUWkMK7Hz4YcfYvfu3Rg7NleRtq2tDZ2dnVi9erV6zLPPPotsNov58+cHNU0dmQC6ng/3bKzG6hgAzSpmRbl2Pf/UzBbcd9F8NDdUBT2VwODjlfj3Z1/KSCtHauJR1OaF6E5qHErsIwTqxurt7VWtNACwZcsWrFmzBiNHjsTIkSNx00034ayzzkJrays2b96Mb37zm5g+fTpOPvlkAMCBBx6IU045BRdffDHuuusupFIpXH755Tj77LPLIhML4GN2/BtzOFp2+PWvsSYndjI2mnwOhzo7+yq1Cc2yo4sjGy4fygqmOh5B31DGtGEtQVQagVp2Xn/9dRx66KE49NBDAQBXXXUVDj30UNxwww2IRCJYu3YtPvOZz2D//ffHhRdeiHnz5uHFF1/U1dq57777MHPmTCxYsACnnXYajjnmGPzqV78K6pIMaG4s/271cInT0aPNuSlv2UlJ3FiDqQz+/c5XcPMjGwFw7SKG4yVXOHWJWNBTIEyoiuWEaF/SWQ86ghiuBGrZOf7441U3hIynnnqq4DlGjhxZNgUEZaQD7o01XISPzrJTHQcgd2P9dfWHeP2DvXj9g7244YxZ6udnuFznvkQdZ9nZl9155QiLp+pLkmWH2Dco62ysSkAtKkhuLEt4sdJQzQKUjWJnzba9uueFQ5iJoKir0r5eDp3YhNF1+9ssFEl4DcuUI7FD7CuQ2PGYYOrsaI+HidbRibKGKi1mR0yNfXt7j99TI1xSX6W5scKhEC4/Yd+rIl2usHiq7sFUwDMhCH8YVtlYw5FAup5zZqThEgyqt+xoi6RYa+dfO4V+PsOg6/m+Sj2XjUUZWOUFs+z0DpJlh9g3ILHjMUEEKEf5dhG+jVocesuOtkjyVZT7h9IYSGkBlZmsAopPLl9qOLFDWqe8YJ3Pe8iNRewjkNjxGFYYL+rjl73esuPfuMUQMrXsaGJnV8+Q7jWpTFbrek5yp+yo5YoKUgB5eUFih9jXILHjMZms/41AdZadYbLI8LPk3R9DnNjZ268XO+l8TA8wfETdvkQtubHKFhY83kduLGIfgcSOx7C12teYndDwe1v5NPPqeERtKbCrZwjPvrMDmayCzgF9MGUqzeVi0VpadtTGeTcWvUHlBPtB0UtFBYl9BMrG8hjWLsJPsRMdfloHg2ktFqc6FkEsEkY6m8H5v12Fnb1JXHfaTDTXJ3SvSWWyWiNQPydL2KKGq7NDb1B5wfrP9ZMbi9hHGIbL4vCCFQH2tTfWMOwqPWV0LVobqjC9uQ7RSFi17LDePf+34gPs6hVidvgAZbIclB28ZWcoTRWRygkmdqiCMrGvQJYdj2EtD2K+ZmMNv4U/FgnjxWs/pVaajkT019CbTGN3nyB20lkoWYrZKVeqYtpnfjBFi2o5wVp59A/R+0LsG5DY8Zis4n8F5eEaHxHjLFJR4Yb1D2WwR7TsZLKUel7G8NY2EjvlBXMxUiNQYl9h+Pk7hhmZAOrsVELmS0yw7CTTWewdEMWOoopJSj0vbwZT5MYqJ5iLcSCVxeoP9uKJddupdQRR0ZBlx2M0sePfmJUgdmSuuB1dg7rnOsvO8L/kiubAsQ1BT4HgYGUBdvUmcdadrwAAJoyoxvWfnqX+bBjXVI2D92sMaIYEUVpI7HhMNoB2EbFKEDsSdfhuR6/ueTqbBVM7JHbKk2ev/iQ27+zFEVNGBj0VgqOWz5TLs23vAP7z/1brtj3+jWMxaxwJVWL4Q2LHY9L5xViMQfGSirDsRIzXIAZTDqV5NxZRjkwdU4epY+qCngYhUJfQf/UfNnkEEtEw9vTlallt7xpAZ38Kz7zdTmKHqAhI7HgMq7PjZ4ZUJYgdq+y1WCSEVEbRtZIg0w5B2Ke+KobRdXG1nMPJB7Xi4mOnqvt/9cJm3PL4O3jlvd34xoKgZkkQpYMClD0miK7nFSF2JJYdIGfBaa6vAgB8uHeA641FEIQT5oxvUh/v31Kv23fUtNEAgDUfduLKP63BVX9ag5ff2+nn9AiipJBlx2OCETvDX8PyaeiRcEi9jxNG1qAuEcVHnQO47sF16jEkdgjCGYdNHoFn3+kAAEwdXavbd+DYBtRXRdEzmMaDb34EAFi1ZQ9e+tYJvs+TIErB8F8Vyxy1grKvYse3oTyDFzuHTmhSH88Z3yiN5yE3FkE44xDOsrNfU7VuXyQcwu++egQWfWoazm+bBADo6E2qjXcJYrhBlh2PyeS/HPzMkBquRQV5eEHziUkj8PoHewHkfoFuF1LQAaACPHcE4Stt00bhGwumY+LIGumPsU9MGoFPTBqB/qE0frfiAwyls+hNplFfFQtgtgRRHCR2PCaIRqCVEbOjWXamjq7F9OY6vNfRi88euh9WbdkT4MwIojIIhUK46sQDCh5XE4+iOhbBQCqDjp6kY7EzMJRRsyZ5IuEQqmLGFHiC8AISOx7DEob8FCCVYNmJc2JnbFM1/vKfbegaSGHy6FqdEGJUgOeOIMqWkbVxfNQ5gJ09SUxzUEpg8d/X4o+rtkn3hUPATZ85CF9um1yiWRKEObRGeAz7RUPtIpwRi2r3q7WhCiNq45icD6KUiR2K2SEI7xhVGwcA7OwxupDNSGeyeOjNj033ZxXgty+/X+zUCMIWZNnxGJZFRHV2HMKZvVsbqnS7olHj9VXAFRNE2TK6PgEA+LjTvthZ/3E3BlIZ1CYiWHXdQp3FuXswhbYly7BlVx8eeetjjKiJl3zO05vr0NpYVfhAYp+AxI7HsABlaQaRR0QqwMrROZBSHzdU6z+mMstOBVwyQZQtLQ05sSNLDgAARVHw6NrteH93n7rtza2dAIBDJoxQe3ExquMRHDpxBFZ/sBdf/+Obnsy5PhHFq9ctMIxN7JvQp8Bjgqiz42eau1fs7dfETkhQMnESOwThKy156+oHu/vwUeeAYf8zG3fgxn9skL72uBmjpduvOekA3PiPDRjKZKT7ASCEEGKRMIYyWaiN8GywvWsQPck0lm/aidPmjLX9OqJyIbHjMWojUB9X41MPbsWtT7yNeZNG+DZmqenqHzLdJ6uuHCJHFkF4xrh8HZ7nNu3E0bc+a3rcYZNGYEze5QXkApvPPXKS9Ngjp43CU1ceV9qJ5vn2g+tw38qtuPeV95HKZgu/wEPG1CfQNnWU4Ucb4S8kdjyGtTPw09pSm4hi5XULh3XsjtVvOHJjEYS/HDN9NFoaEtjTZ/4j5PDJI/H7C45AtAyqmn5m7jjct3IrVr2/B6veD75UxQOXHIkjp44Kehr7NCR2PCYTgGUHGP5Byj/7wiH42n1v4MYzZhn2JaISsUOWHYLwjHFN1Vh53cKgp2GbwyePxLnzJ2Lj9u5A5/HB7n7s6RvC6vf3ktgJGBI7HqOlntNi7ITDJo/Eqm/Lv1xlvxzJskMQBCMcDuEH/zY76Gng1ifexl3L/4VNO3qCnso+T/D2xgoniADlSkcWoEwQBFFuHNjaAADYvLM34JkQtGp4TIYsOyVHGqBMt5cgiDJjekuu2vTWPf0Bz4QgN5bHsGysSmjhUC7Eo8Z+OnR3CYIoN6aOrkMIQM9gGlc88OY+/aM3Hg3jqhMP0GXr+QmJHY9h2Vj78oe81MgtO3R/CYIoL6rjEezXVI0POwfw0Brz1hn7ChcfO5XETqWiBSgHPJEKQtZBmSAIohy568ufwGPr2qHs499bsUjYk7YgdiGx4zFagDKpnVKRzhq/NPqHzKuwEgRBBMXB+zXh4P2agp7GPg+twB6jWnbIzVIyshKx05tMBzATgiAIYjhAYsdjWKVyMuyUDpllp4/EDkEQBGECLcEeQ0UFS086YxQ7PYMkdgiCIAg5JHY8JkNurJIja+xHbiyCIAjCDBI7HnL1n9eoFgc/G4FWOp87dDwA4KBxDeq2nsFUUNMhCIIgyhwSOx7y2vt7AQDRcAijaoNLuas0Dmitx6pvL8BDi45Wt5FlhyAIgjCDUs895IZPz0LPYAr7t9ajKcD6ApVIc32V7jlVqCYIgiDMILHjIQtntQQ9hYrnt185HNf9fR1+9oVDgp4KQRAEUaaElH29rCOA7u5uNDY2oqurCw0NDYVfQBAEQRBE4NhdvylmhyAIgiCIiobEDkEQBEEQFQ2JHYIgCIIgKhoSOwRBEARBVDQkdgiCIAiCqGgCFTsvvPACzjjjDIwbNw6hUAgPPfSQbr+iKLjhhhswduxYVFdXY+HChXj33Xd1x+zZswfnnnsuGhoa0NTUhAsvvBC9vb0+XgVBEARBEOVMoGKnr68Pc+fOxR133CHdf9ttt+EXv/gF7rrrLqxcuRK1tbU4+eSTMTg4qB5z7rnnYsOGDVi6dCkeffRRvPDCC7jkkkv8ugSCIAiCIMqcsqmzEwqF8OCDD+LMM88EkLPqjBs3DldffTX+3//7fwCArq4utLS04N5778XZZ5+Nt99+G7NmzcJrr72Gww47DADw5JNP4rTTTsOHH36IcePG2Rqb6uwQBEEQxPBj2NfZ2bJlC9rb27Fw4UJ1W2NjI+bPn48VK1YAAFasWIGmpiZV6ADAwoULEQ6HsXLlStNzJ5NJdHd36/4jCIIgCKIyKVux097eDgBoadG3XGhpaVH3tbe3o7m5Wbc/Go1i5MiR6jEylixZgsbGRvW/CRMmlHj2BEEQBEGUC2Urdrxk8eLF6OrqUv/btm1b0FMiCIIgCMIjylbstLa2AgB27Nih275jxw51X2trKzo6OnT70+k09uzZox4jI5FIoKGhQfcfQRAEQRCVSdmKnSlTpqC1tRXLli1Tt3V3d2PlypVoa2sDALS1taGzsxOrV69Wj3n22WeRzWYxf/583+dMEARBEET5EQ1y8N7eXrz33nvq8y1btmDNmjUYOXIkJk6ciCuuuALf//73MWPGDEyZMgXf+c53MG7cODVj68ADD8Qpp5yCiy++GHfddRdSqRQuv/xynH322bYzsQiCIAiCqGwCFTuvv/46PvWpT6nPr7rqKgDA+eefj3vvvRff/OY30dfXh0suuQSdnZ045phj8OSTT6Kqqkp9zX333YfLL78cCxYsQDgcxllnnYVf/OIXjubBsu8pK4sgCIIghg9s3S5URads6uwEyYcffkgZWQRBEAQxTNm2bRvGjx9vup/EDoBsNouPP/4Y9fX1CIVCJTtvd3c3JkyYgG3btlEQtIfQffYPutf+QPfZH+g++4OX91lRFPT09GDcuHEIh83DkAN1Y5UL4XDYUhEWC2V8+QPdZ/+ge+0PdJ/9ge6zP3h1nxsbGwseU7bZWARBEARBEKWAxA5BEARBEBUNiR0PSSQSuPHGG5FIJIKeSkVD99k/6F77A91nf6D77A/lcJ8pQJkgCIIgiIqGLDsEQRAEQVQ0JHYIgiAIgqhoSOwQBEEQBFHRkNghCIIgCKKiIbHjIXfccQcmT56MqqoqzJ8/H6tWrQp6SsOKF154AWeccQbGjRuHUCiEhx56SLdfURTccMMNGDt2LKqrq7Fw4UK8++67umP27NmDc889Fw0NDWhqasKFF16I3t5eH6+ivFmyZAkOP/xw1NfXo7m5GWeeeSY2bdqkO2ZwcBCLFi3CqFGjUFdXh7POOgs7duzQHbN161acfvrpqKmpQXNzM6655hqk02k/L6XsufPOOzFnzhy1sFpbWxueeOIJdT/d59Jz6623IhQK4YorrlC30X0uDd/97ncRCoV0/82cOVPdX3b3WSE84YEHHlDi8bjy29/+VtmwYYNy8cUXK01NTcqOHTuCntqw4fHHH1e+/e1vK3//+98VAMqDDz6o23/rrbcqjY2NykMPPaS89dZbymc+8xllypQpysDAgHrMKaecosydO1d59dVXlRdffFGZPn26cs455/h8JeXLySefrNxzzz3K+vXrlTVr1iinnXaaMnHiRKW3t1c95tJLL1UmTJigLFu2THn99deVI488UjnqqKPU/el0Wjn44IOVhQsXKm+++aby+OOPK6NHj1YWL14cxCWVLf/4xz+Uxx57TPnnP/+pbNq0SbnuuuuUWCymrF+/XlEUus+lZtWqVcrkyZOVOXPmKP/1X/+lbqf7XBpuvPFG5aCDDlK2b9+u/rdz5051f7ndZxI7HnHEEUcoixYtUp9nMhll3LhxypIlSwKc1fBFFDvZbFZpbW1VfvSjH6nbOjs7lUQiofzxj39UFEVRNm7cqABQXnvtNfWYJ554QgmFQspHH33k29yHEx0dHQoAZfny5Yqi5O5pLBZT/vKXv6jHvP322woAZcWKFYqi5ERpOBxW2tvb1WPuvPNOpaGhQUkmk/5ewDBjxIgRym9+8xu6zyWmp6dHmTFjhrJ06VLlk5/8pCp26D6XjhtvvFGZO3eudF853mdyY3nA0NAQVq9ejYULF6rbwuEwFi5ciBUrVgQ4s8phy5YtaG9v193jxsZGzJ8/X73HK1asQFNTEw477DD1mIULFyIcDmPlypW+z3k40NXVBQAYOXIkAGD16tVIpVK6+zxz5kxMnDhRd59nz56NlpYW9ZiTTz4Z3d3d2LBhg4+zHz5kMhk88MAD6OvrQ1tbG93nErNo0SKcfvrpuvsJ0Oe51Lz77rsYN24cpk6dinPPPRdbt24FUJ73mRqBesCuXbuQyWR0byIAtLS04J133gloVpVFe3s7AEjvMdvX3t6O5uZm3f5oNIqRI0eqxxAa2WwWV1xxBY4++mgcfPDBAHL3MB6Po6mpSXeseJ9l7wPbR2isW7cObW1tGBwcRF1dHR588EHMmjULa9asoftcIh544AG88cYbeO211wz76PNcOubPn497770XBxxwALZv346bbroJxx57LNavX1+W95nEDkEQAHK/htevX4+XXnop6KlULAcccADWrFmDrq4u/PWvf8X555+P5cuXBz2timHbtm34r//6LyxduhRVVVVBT6eiOfXUU9XHc+bMwfz58zFp0iT8+c9/RnV1dYAzk0NuLA8YPXo0IpGIIfJ8x44daG1tDWhWlQW7j1b3uLW1FR0dHbr96XQae/bsofdB4PLLL8ejjz6K5557DuPHj1e3t7a2YmhoCJ2dnbrjxfssex/YPkIjHo9j+vTpmDdvHpYsWYK5c+fi5z//Od3nErF69Wp0dHTgE5/4BKLRKKLRKJYvX45f/OIXiEajaGlpofvsEU1NTdh///3x3nvvleXnmcSOB8TjccybNw/Lli1Tt2WzWSxbtgxtbW0BzqxymDJlClpbW3X3uLu7GytXrlTvcVtbGzo7O7F69Wr1mGeffRbZbBbz58/3fc7liKIouPzyy/Hggw/i2WefxZQpU3T7582bh1gsprvPmzZtwtatW3X3ed26dTphuXTpUjQ0NGDWrFn+XMgwJZvNIplM0n0uEQsWLMC6deuwZs0a9b/DDjsM5557rvqY7rM39Pb2YvPmzRg7dmx5fp5LHvJMKIqSSz1PJBLKvffeq2zcuFG55JJLlKamJl3kOWFNT0+P8uabbypvvvmmAkD5yU9+orz55pvKBx98oChKLvW8qalJefjhh5W1a9cqn/3sZ6Wp54ceeqiycuVK5aWXXlJmzJhBqeccl112mdLY2Kg8//zzuhTS/v5+9ZhLL71UmThxovLss88qr7/+utLW1qa0tbWp+1kK6UknnaSsWbNGefLJJ5UxY8ZQqq7At771LWX58uXKli1blLVr1yrf+ta3lFAopDz99NOKotB99go+G0tR6D6Xiquvvlp5/vnnlS1btigvv/yysnDhQmX06NFKR0eHoijld59J7HjI7bffrkycOFGJx+PKEUccobz66qtBT2lY8dxzzykADP+df/75iqLk0s+/853vKC0tLUoikVAWLFigbNq0SXeO3bt3K+ecc45SV1enNDQ0KF/96leVnp6eAK6mPJHdXwDKPffcox4zMDCgfO1rX1NGjBih1NTUKP/2b/+mbN++XXee999/Xzn11FOV6upqZfTo0crVV1+tpFIpn6+mvLnggguUSZMmKfF4XBkzZoyyYMECVegoCt1nrxDFDt3n0vCFL3xBGTt2rBKPx5X99ttP+cIXvqC899576v5yu88hRVGU0tuLCIIgCIIgygOK2SEIgiAIoqIhsUMQBEEQREVDYocgCIIgiIqGxA5BEARBEBUNiR2CIAiCICoaEjsEQRAEQVQ0JHYIgiAIgqhoSOwQBEEQBFHRkNghCKIiOf7443HFFVcEPQ2CIMoAEjsEQRAEQVQ0JHYIgiAIgqhoSOwQBLFP8Nhjj6GxsRH33Xdf0FMhCMJnokFPgCAIwmvuv/9+XHrppbj//vvx6U9/OujpEAThM2TZIQiiornjjjvwta99DY888ggJHYLYRyHLDkEQFctf//pXdHR04OWXX8bhhx8e9HQIgggIsuwQBFGxHHrooRgzZgx++9vfQlGUoKdDEERAkNghCKJimTZtGp577jk8/PDD+PrXvx70dAiCCAhyYxEEUdHsv//+eO6553D88ccjGo3iZz/7WdBTIgjCZ0jsEARR8RxwwAF49tlncfzxxyMSieC///u/g54SQRA+ElLIkU0QBEEQRAVDMTsEQRAEQVQ0JHYIgiAIgqhoSOwQBEEQBFHRkNghCIIgCKKiIbFDEARBEERFQ2KHIAiCIIiKhsQOQRAEQRAVDYkdgiAIgiAqGhI7BEEQBEFUNCR2CIIgCIKoaEjsEARBEARR0fx/Esqc0xbqQqkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Scatterplot with Seaborn\n",
    "sns.lineplot(data=adf.df, x=\"k\", y=\"current_state_cost\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOL, looking at the above, it seems that our objective in iteration 0 was competitive with the best prompt(s) discovered including the one we wound up on.\n",
    "\n",
    "This isn't all that atypical when we run with: \n",
    "1. A simple objective\n",
    "2. An unclear problem setup (i.e. the run above (a) asks for a json object but (b) says \"conversational\" and (c) doesn't guide the LLM to generate JSON in a formal way (or try to parse it or anything)\n",
    "3. A pretty powerful target model (currently `phi3:3.8b-instruct`)\n",
    "\n",
    "But in any case, this was a runtest for the revamped eval pipeline and the full-blown guidance pipelines, not well set-up as a real optimization problem.\n",
    "\n",
    "Nonetheless there is data to look at, and we can set up some of our more interesting analyses using what we have now! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dead Analysis Code Below!  Won't run with the refactored code above... but will be restored soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct data from end-state\n",
    "\n",
    "TODO: refactor all this into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert a is not None\n",
    "# assert a.temperature_functions is not None\n",
    "\n",
    "\n",
    "# #### X axis: Iterations\n",
    "# # both the \"completed\" and \"possible\" iterations are interesting / useful for different depictions\n",
    "# # e.g. even with a few iterations completed it could be \"interesting\" to see the state costs for the\n",
    "# # available range atop the cooling schedule for the full range of iterations -- giving a sense of\n",
    "# # \"where are we\" progress-wise in the optimization process.\n",
    "\n",
    "# # xlim bottom that includes Bootstrapping iteration k=0 (first \"real\" iteration being k=1)\n",
    "# x_iter_min = 0\n",
    "\n",
    "# # xlim top that shows all _possible_ iterations / full cooling schedule / etc.\n",
    "# x_iter_max = a.config.max_iteration_count\n",
    "# x_iter_vec = range(\n",
    "#     x_iter_min, x_iter_max\n",
    "# )  # vector of all possible iteration numbers [0..max_iteration_count]\n",
    "\n",
    "# # xlim top that shows only completed iterations w/ scores and costs\n",
    "# x_completediter_max = (\n",
    "#     most_recent_iter_completed  # X limit when showing completed iterations\n",
    "# )\n",
    "# x_completediter_vec = list(\n",
    "#     _iter_cost_dict.keys()\n",
    "# )  # vector of only completed iteration numbers x=[0..completediter_max] for which we have scores\n",
    "\n",
    "# ##### Y axis #1: Cost\n",
    "# # ylim bottom at the lowest possible cost (hypothetical bottom of range)\n",
    "# # could use 0 instead but this accounts for cases where the real-world min_cost is nonzero (positive or negative)\n",
    "# y_cost_min = AnnealerScores.get_min_cost()\n",
    "\n",
    "# # ylim top at the highest possible cost (hypothetical top of range)\n",
    "# y_cost_max = AnnealerScores.get_max_cost()\n",
    "\n",
    "\n",
    "# # \"candidatecost\" reflects the cost of the new candidate prompt generated at iteration k, prior to acceptance/rejection\n",
    "# # \"better\" refers to the subset of candidate costs that were better (lower) than the state cost\n",
    "# # \"worse\" refers to the subset of candidate costs that were worse than the state cost, regardless of whether they were accepted\n",
    "# # \"accepted\" refers to the subset of candidate costs that were accepted.  note that ALL better candidates\n",
    "# # and SOME worse candidates get accepted, the latter being dependent on the temperature/cooling schedule,\n",
    "# # and the magnitude of worse-ness\n",
    "\n",
    "# # note: when ever a candidatecost plot includes rejected candidates, doesn't really make sense to show as a line plot\n",
    "# #     however markers, \"error-bar-like\" etc. treatments could be interesting\n",
    "\n",
    "# # candidatecost vector for all completed iterations\n",
    "# y_candidatecost_vec = [\n",
    "#     a.iter_dict[_k].scores.final_cost for _k in x_completediter_vec\n",
    "# ]  # vector of all candidate cost values x=[0..completediter_max]. Makes more sense as markers without lines.  or could be shown as one-sided error bars deviating from the state cost\n",
    "\n",
    "# # candidatecost subset vectors including only accepted iterations/candidatecosts (some better, some worse)\n",
    "# x_acceptedcandidateiter_vec = [\n",
    "#     _k\n",
    "#     for _k in x_completediter_vec\n",
    "#     if a.iter_dict[_k].was_accepted and a.iter_dict[_k].scores is not None\n",
    "# ]\n",
    "# y_acceptedcandidatecost_vec = [\n",
    "#     a.iter_dict[_k].scores.final_cost\n",
    "#     for _k in x_acceptedcandidateiter_vec  # type: ignore\n",
    "# ]\n",
    "\n",
    "# # candidatecost subset vectors including only rejected iterations/candidatecosts (all will be worse, but not all worse candidates are rejected)\n",
    "# x_rejectedcandidateiter_vec = [\n",
    "#     _k\n",
    "#     for _k in x_completediter_vec\n",
    "#     if not a.iter_dict[_k].was_accepted and a.iter_dict[_k].scores is not None\n",
    "# ]\n",
    "# y_rejectedcandidatecost_vec = [\n",
    "#     a.iter_dict[_k].scores.final_cost\n",
    "#     for _k in x_rejectedcandidateiter_vec  # type: ignore\n",
    "# ]\n",
    "# # candidatecost subset vectors including all/only \"worse\" iterations/candidatecosts, indpendent of acceptance\n",
    "# # could be interesting to show this atop the temperature-depndent acceptance envelope, below\n",
    "# x_worsecostiter_vec = [\n",
    "#     _k\n",
    "#     for _k in x_completediter_vec\n",
    "#     if y_candidatecost_vec[_k - 1] < _iter_cost_dict[_k]  # type: ignore\n",
    "# ]\n",
    "# y_worsecost_vec = [\n",
    "#     y_candidatecost_vec[_k]\n",
    "#     for _k in x_worsecostiter_vec  # shows the actual costs of the \"worse\" candidates\n",
    "# ]\n",
    "# y_abs_worsecost_vec = [\n",
    "#     y_worsecost_vec[_k] - _iter_cost_dict[_k]  # type: ignore\n",
    "#     for _k in x_worsecostiter_vec  # shows the absolute magnitude of \"worseness\" accounting for the state cost # type: ignore\n",
    "# ]\n",
    "\n",
    "# #### Y axis #2: Temperature\n",
    "# # could definitely be `twinx`ed with costs to be shown together on single plots\n",
    "\n",
    "# # ylim top and bottom for temperature\n",
    "# y_temp_max = (\n",
    "#     AnnealerTemperatureFunctions.get_max_temperature()\n",
    "# )  # starting / highest temperature (k=1)\n",
    "# y_temp_min = a.temperature_functions.get_temperature(\n",
    "#     x_iter_max\n",
    "# )  # ending / lowest temperature\n",
    "\n",
    "# # cooling schedule at all iterations\n",
    "# y_temp_vec = [a.temperature_functions.get_temperature(_k) for _k in x_iter_vec]\n",
    "\n",
    "# # percentcooled is a transformation of cooling schedule\n",
    "# # created more as a \"helper\" to simplify code in other plots below than a plot unto itself\n",
    "# y_percentcooled_vec = [y_temp_vec[_k] / y_temp_max for _k in x_iter_vec]\n",
    "\n",
    "# # Because we use deterministic threshold acceptance and not a stochastic acceptance, we can compute and possibly\n",
    "# # overlay a kind of \"acceptance envelope\" that spreads (upward only) from the statecost_vec\n",
    "# # This visualizes the range of potentially acceptable \"worse\" costs at each iteration\n",
    "# # (acceptance is a function of the percent cooled, as well as abs_worsecost.\n",
    "\n",
    "# # This could make for an interesting filled area atop the state cost line, within which only the\n",
    "# # accepted (but none of the rejected) candidate costs would fall.  This would give a sense of\n",
    "# # how the \"dynamic range\" of our cost function is interacting with the acceptance function (and its \"envelope\")\n",
    "# # as the cooling schedule progresses.\n",
    "\n",
    "# # absolute magnitude of the acceptance envelope.  This is the \"height\" of the envelope above the state cost line\n",
    "# # without taking into account the actual \"height\" of the state cost line itself\n",
    "# y_abs_acceptance_eenvelope_vec = [\n",
    "#     (y_cost_max - y_cost_min) * y_percentcooled_vec[_k] for _k in x_iter_vec\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Hypothetical \"ceiling\" of the acceptance envelope as it sits atop the statecost.\n",
    "# # Note that this can/will exceed y_cost_max.\n",
    "# y_hypotheticalceiling_acceptance_envelope_vec = [\n",
    "#     a.iter_dict[_k].scores.final_cost + y_abs_acceptance_eenvelope_vec[_k]  # type: ignore\n",
    "#     for _k in x_completediter_vec\n",
    "# ]\n",
    "\n",
    "# # Real \"ceiling\" of the acceptance envelope as it sits atop the statecost.\n",
    "# # Accounts for the fact that no cost can actually exceed max_cost\n",
    "# y_realceiling_acceptance_envelope_vec = [\n",
    "#     max(\n",
    "#         a.iter_dict[_k].scores.final_cost + y_abs_acceptance_eenvelope_vec[_k],  # type: ignore\n",
    "#         y_cost_max,\n",
    "#     )  # type: ignore\n",
    "#     for _k in x_completediter_vec\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Scatterplot with Seaborn\n",
    "# sns.lineplot(data=adf.df, x=\"k\", y=\"current_state_cost\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_statecost_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# State Cost\u001b[39;00m\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m---> 12\u001b[0m     x_completediter_vec, \u001b[43my_statecost_vec\u001b[49m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState Cost\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Candidate Costs (Accepted and Rejected)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\n\u001b[1;32m     17\u001b[0m     x_acceptedcandidateiter_vec,\n\u001b[1;32m     18\u001b[0m     y_acceptedcandidatecost_vec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccepted Candidates\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_statecost_vec' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Figure setup for better readability\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot 1: State and Candidate Costs with Temperature Overlay\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "# State Cost\n",
    "plt.plot(\n",
    "    x_completediter_vec, y_statecost_vec, marker=\"o\", linestyle=\"-\", label=\"State Cost\"\n",
    ")\n",
    "\n",
    "# Candidate Costs (Accepted and Rejected)\n",
    "plt.scatter(\n",
    "    x_acceptedcandidateiter_vec,\n",
    "    y_acceptedcandidatecost_vec,\n",
    "    marker=\"o\",\n",
    "    color=\"green\",\n",
    "    label=\"Accepted Candidates\",\n",
    ")\n",
    "plt.scatter(c\n",
    "    x_rejectedcandidateiter_vec,\n",
    "    y_rejectedcandidatecost_vec,\n",
    "    marker=\"x\",\n",
    "    color=\"red\",\n",
    "    label=\"Rejected Candidates\",\n",
    ")\n",
    "\n",
    "# Acceptance Envelope\n",
    "plt.fill_between(\n",
    "    x_completediter_vec,\n",
    "    y_statecost_vec,\n",
    "    y_ceiling_acceptance_envelope_vec,\n",
    "    color=\"lightblue\",\n",
    "    alpha=0.3,\n",
    "    label=\"Acceptance Envelope\",\n",
    ")\n",
    "\n",
    "# Temperature (Secondary Y-Axis)\n",
    "ax2 = plt.twinx()\n",
    "ax2.plot(x_iter_vec, y_temp_vec, color=\"orange\", linestyle=\":\", label=\"Temperature\")\n",
    "ax2.set_ylabel(\"Temperature\", color=\"orange\")\n",
    "\n",
    "plt.title(\"Annealing Progress: Cost vs. Temperature\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\")  # Optional: Grid for better readability\n",
    "\n",
    "# Plot 2: Focus on Acceptance Behavior\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(\n",
    "    x_worsecostiter_vec,\n",
    "    y_abs_worsecost_vec,\n",
    "    marker=\"x\",\n",
    "    linestyle=\":\",\n",
    "    color=\"purple\",\n",
    "    label=\"Absolute Worse Cost\",\n",
    ")\n",
    "plt.plot(\n",
    "    x_completediter_vec,\n",
    "    y_abs_acceptance_eenvelope_vec[: len(x_completediter_vec)],\n",
    "    linestyle=\"-.\",\n",
    "    color=\"blue\",\n",
    "    label=\"Acceptance Envelope (Absolute)\",\n",
    ")\n",
    "plt.ylim(bottom=0)  # Set the Y-axis lower limit to 0\n",
    "plt.title(\"Acceptance Behavior: Worse Costs vs. Envelope\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Absolute Cost Difference\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\")  # Optional: Grid for better readability\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnealerPrompt(system_prompt_text='{{ \\n  \"entities\": [\\n    {{ \\n      \"name\": \"lights\",\\n      \"description\": \"smart lighting devices that can be controlled to turn on/off, adjust brightness, and change color\"\\n    {{,\\n    {{ \\n      \"name\": \"thermostats\",\\n      \"description\": \"devices that can monitor and control the temperature in a home\"\\n    {{,\\n    }}\\n      \"name\": \"security systems\",\\n      \"description\": \"components like door/window sensors, motion detectors, and cameras that can monitor a home\\'s security\"\\n    {{,\\n    }}\\n      \"name\": \"appliances\",\\n      \"description\": \"kitchen and household appliances like refrigerators, ovens, washers/dryers that can be queried for status and controlled remotely\"\\n     }},\\n    }}\\n      \"name\": \"media players\",\\n      \"description\": \"devices like smart TVs, speakers, and game consoles that can play audio/video content\"\\n     }}\\n  ]\\n }}', user_prompt_text='[input]\\n{input}')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.iter_dict[50].candidate_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_scores_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[43mscores_dict\u001b[49m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores_dict' is not defined"
     ]
    }
   ],
   "source": [
    "all_scores_df = pd.DataFrame.from_dict(scores_dict, orient=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.plotting import scatter_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# if c is None or not isinstance(c, SqliteDict):\n",
    "#     raise ValueError(\"Disk cache not initialized\")\n",
    "# scores_df = last_iter_scores_df\n",
    "\n",
    "# sm = scatter_matrix(\n",
    "#     scores_df,\n",
    "#     figsize=(5, 5),\n",
    "#     diagonal=\"hist\",\n",
    "#     hist_kwds={\"bins\": 10},\n",
    "#     range_padding=0.1,\n",
    "# )\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         sm[i, j].set_xlim(0, 10)\n",
    "#         # if i!=j:\n",
    "#         #     sm[i,j].set_ylim(0,None)\n",
    "# _ = plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c[\"test\"] = id.scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Visualize all the scores to get some intuition into the distributions\n",
    "# from pandas.plotting import scatter_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# try:\n",
    "#     sm = scatter_matrix(scores_df, figsize=(5, 5),diagonal=\"hist\",hist_kwds={'bins':10},range_padding=0.1)\n",
    "#     for i in range(4):\n",
    "#         for j in range(4):\n",
    "#             sm[i,j].set_xlim(0,10)\n",
    "#             # if i!=j:\n",
    "#             #     sm[i,j].set_ylim(0,None)\n",
    "#     _=plt.show()\n",
    "# except:\n",
    "#     print(\"skipping\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work jotted down at various points: \n",
    "\n",
    "Implement some \"summary\" evaluators:\n",
    "- length penatly / soft pressure toward conciseness in both prompts and also other intermediate outputs\n",
    "- LLM evaluations of the candidate prompt itself (not the outputs it generates)\n",
    "    - **Buttress harmlessness via direct prompt inspection**, so we aren't 100% leaning on eval output evaluators (and the base model's robustness)\n",
    "        - e.g. ```The prompt is not written in a way that seems likely to suppress or minimize safety prompts or disclaimers in sensitive / high risk cases```\n",
    "        - Why: Suppose the annealer was (without malicious intent) told to \"generate one-sentence actionable answers with zero commentary\", and run on a dataset that happens to frequently trigger lower-sensitivity discretionary disclaimers & suggestive nudges\n",
    "            - We want to guard against the optimizer inadvertently learning to suppress \"commentary\" in a way that generalizes so well it impacts even high-sensitivity / mandatory cases\n",
    "            - Output evaluators alone can't be expected to accomplish this as the pressures they create are 100% dependent on the datasets they see\n",
    "            - TL;DR the annealer is in some sense a \"prompt gaming engine\"; it's hard to forsee all the ways that could potentally lead to unexpected, nongood outcomes (but not all that hard to come up with examples); seems worth it to put a few high-level safety catch-alls in place     \n",
    "    - soft pressure toward token-effiency / conciseness in the prompt itself  \n",
    "    - buttress output assessments in various situations  \n",
    "        - output format conformance, e.g. suppose the objective says \"... using XML formatting\"  \n",
    "            - ... and the outputs do show up in XML... but just coincidentally, i.e. because of the base model's happenstantial behavior / preferences.  Or something hidden within the langchain templates I happened to use in here.  But not because it's actually reinforced by the prompt.\n",
    "            - ... or, instead, the outputs do show up in XML, but the prompt itself is clearly out of whack, e.g. actually asks for \"json\" but then includes few-shots that use XML, or vice versa, or who-knows-what incoherent / wrong thing that happens to work just well enough to fool the output evaluators during annealing / on that dataset. \n",
    "        - might be able to help improve outcomes when the eval dataset is unlabeled... TBD, need some real-world data\n",
    "    - Note, I don't think it's generally advisable to create a lot of evaluators sitting in judgment of the quality or styling of the prompts themselves, as the basic exercise here is \"find something that -- for whatever reason -- happens to work best\".  Specifically **without** leaning into some preconceived notion of what that ought to look like or be.  (If we knew that, why would we anneal the prompt in the first place?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
